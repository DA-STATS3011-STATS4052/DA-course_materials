[
  {
    "objectID": "notes_24_25.html",
    "href": "notes_24_25.html",
    "title": "Week 4: Regression modelling part 2",
    "section": "",
    "text": "Let’s expand upon what we learned last week by revisiting the instructor evaluation data set evals. In Week 3 you were tasked with examining the relationship between teaching score (score) and age (age). Now, let’s also introduce the additional (binary) categorical explanatory variable gender (gender). That is, we we will be examining:\n\nthe teaching score (score) as our outcome variable \\(y\\);\nage (age) as our numerical explanatory variable \\(x_1\\); and\ngender (gender) as our categorical explanatory variable \\(x_2\\).\n\n\nStart by subsetting the evals data set so that we only have the variables we are interested in, that is, score, age and gender. Note, it is best to give your new data set a different name than evals as to not overwrite the original evals data set. Your new data set should look like the one below.\n\n\n\n\n\n\n Question\n\n\n\nYou can also view your data set using the glimpse function, or by opening a spreadsheet view in RStudio using the View function.\nUse the skim function to obtain some summary statistics from our data:\n\n\nAnswer\n\n\nCodeeval.score |&gt;\n  skim()\n\n\nData summary\n\n\nName\neval.score\n\n\nNumber of rows\n463\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n7\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\ngender\n0\n1\nFALSE\n2\nmal: 268, fem: 195\n\n\nethnicity\n0\n1\nFALSE\n2\nnot: 399, min: 64\n\n\nlanguage\n0\n1\nFALSE\n2\neng: 435, non: 28\n\n\nrank\n0\n1\nFALSE\n3\nten: 253, ten: 108, tea: 102\n\n\npic_outfit\n0\n1\nFALSE\n2\nnot: 386, for: 77\n\n\npic_color\n0\n1\nFALSE\n2\ncol: 385, bla: 78\n\n\ncls_level\n0\n1\nFALSE\n2\nupp: 306, low: 157\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1\n232.00\n133.80\n1.00\n116.50\n232.00\n347.5\n463.00\n▇▇▇▇▇\n\n\nprof_ID\n0\n1\n45.15\n27.55\n1.00\n20.00\n43.00\n70.5\n94.00\n▇▇▆▆▆\n\n\nscore\n0\n1\n4.17\n0.54\n2.30\n3.80\n4.30\n4.6\n5.00\n▁▁▅▇▇\n\n\nage\n0\n1\n48.37\n9.80\n29.00\n42.00\n48.00\n57.0\n73.00\n▅▆▇▆▁\n\n\nbty_avg\n0\n1\n4.42\n1.53\n1.67\n3.17\n4.33\n5.5\n8.17\n▃▇▇▃▂\n\n\ncls_did_eval\n0\n1\n36.62\n45.02\n5.00\n15.00\n23.00\n40.0\n380.00\n▇▁▁▁▁\n\n\ncls_students\n0\n1\n55.18\n75.07\n8.00\n19.00\n29.00\n60.0\n581.00\n▇▁▁▁▁\n\n\n\n\n\n\n\n\nNow, let’s compute the correlation coefficient between our outcome variable score and our numerical explanatory variable age:\n\nCodeeval.score |&gt; \n  get_correlation(formula = score ~ age)\n\n# A tibble: 1 × 1\n     cor\n   &lt;dbl&gt;\n1 -0.107\n\n\n\n\n\n\n\n\n Question\n\n\n\nWhy do we not include the categorical variable gender when calculating the correlation?\n\n\nAnswer\n\nThe correlation coefficient only exists between numerical variables, which is why we do not include our categorical variable gender.\n\n\n\nWe can now visualise our data by producing a scatterplot, where seeing as we have the categorical variable gender, we shall plot the points using different colours for each gender:\n\nCodeggplot(eval.score, aes(x = age, y = score, color = gender)) +\n  geom_jitter() +\n  labs(x = \"Age\", y = \"Teaching Score\", color = \"Gender\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n Instructor evaluation scores by age and gender. The points have been jittered.\n\n\n\nNote: The above code has jittered the points, however, this is not necessary and geom_point would suffice. To plot separate points by gender we simply add the color argument to the aes function and pass to it gender.\nFrom the scatterplot we can see that:\n\nThere are very few women over the age of 60 in our data set.\nFrom the plotted regression lines we can see that the lines have different slopes for men and women. That is, the associated effect of increasing age appears to be more severe for women than it does for men, i.e. the teaching score of women drops faster with age.\n\nHere, we shall begin by fitting what is referred to as a parallel regression lines model. This model implies that the slope of relationship between teaching score (score) and age (age) is the same for both males and females, with only the intercept of the regression lines changing. Hence, our parallel regression lines model is given as:\n\\[\\begin{align}\ny_{i} &= \\alpha + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i \\nonumber \\\\\n&= \\alpha + \\beta_{\\mbox{age}} \\cdot \\mbox{age} + \\beta_{\\mbox{male}} \\cdot \\mathbb{I}_{\\mbox{male}}(x) + \\epsilon_i, \\nonumber\n\\end{align}\\]\nwhere\n\n\\(\\alpha\\) is the intercept of the regression line for females;\n\\(\\beta_{\\mbox{age}}\\) is the slope of the regression line for both males and females;\n\\(\\beta_{\\mbox{male}}\\) is the additional term added to \\(\\alpha\\) to get the intercept of the regression line for males; and\n\n\\(\\mathbb{I}_{\\mbox{male}}(x)\\) is an indicator function such that\n\\[\\mathbb{I}_{\\mbox{male}}(x)=\\left\\{\n            \\begin{array}{ll}\n              1 ~~~ \\mbox{if gender} ~ x ~ \\mbox{is male},\\\\\n              0 ~~~ \\mbox{Otherwise}.\\\\\n            \\end{array}\n          \\right.\\]\n\n\nWe can fit the parallel regression lines model as follows:\n\nCodelm_spec &lt;- linear_reg() |&gt; set_engine(\"lm\")\npar.model &lt;- lm_spec |&gt; fit(score ~ age + gender, data = eval.score)\npar.model &lt;- par.model |&gt; extract_fit_engine()\nget_regression_table(par.model)\n\n# A tibble: 3 × 7\n  term         estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept       4.48      0.125     35.8    0        4.24     4.73 \n2 age            -0.009     0.003     -3.28   0.001   -0.014   -0.003\n3 gender: male    0.191     0.052      3.63   0        0.087    0.294\n\n\nHence, the regression line for females is given by:\n\\[\\widehat{\\mbox{score}} = 4.48 - 0.009 \\cdot \\mbox{age},\\]\nwhile the regression line for males is given by:\n\\[\\widehat{\\mbox{score}} = 4.48 - 0.009 \\cdot \\mbox{age} + 0.191 = 4.671 - 0.009 \\cdot \\mbox{age}.\\] Now, let’s superimpose our parallel regression lines onto the scatterplot of teaching score against age:\n\nCodeggplot(eval.score, aes(x = age, y = score, color = gender)) +\n  geom_jitter() +\n  labs(x = \"Age\", y = \"Teaching Score\", color = \"Gender\") +\n  geom_parallel_slopes(se = FALSE)\n\n\n\n Instructor evaluation scores by age and gender with parallel regression lines superimposed.\n\n\n\nNote: go through the code used to create coeff and slopes and make sure you understand it.\nFrom the parallel regression lines model both males and females have the same slope, that is, the associated effect of age on teaching score is the same for both men and women. Hence, for every one year increase in age, there is an associated decrease in teaching score of 0.009. However, male instructors have a higher intercept term, that is, there is a vertical bump in the regression line for males in teaching scores. This is linked to the average difference in teaching scores that males obtain relative to females.\n\n\n\n\n\n\n Question\n\n\n\nWhat is different between our previous scatterplot of teaching score against age (Figure 6) and the one we just created with our parallel lines superimposed (Figure 7)?\n\n\nAnswer\n\nIn the original plot we have what is referred to as an interaction effect between age and gender. Hence, gender interacts in different ways for both males and females by age, and as such we should have different intercepts and slopes.\n\n\n\n\nThere is an interaction effect if the associated effect of one variable depends on the value of another variable. For example, the effect of age here will depend on whether the instructor is male or female, that is, the effect of age on teaching scores will differ by gender. The interaction model can be written as:\n\\[\\begin{align}\ny_{i} &= \\alpha + \\beta_1  x_{1i} + \\beta_2  x_{2i} + \\beta_3  x_{1i}  x_{2i} + \\epsilon_i \\nonumber \\\\\n&= \\alpha + \\beta_{\\mbox{age}} \\cdot \\mbox{age} + \\beta_{\\mbox{male}} \\cdot \\mathbb{I}_{\\mbox{male}}(x) + \\beta_{\\mbox{age, male}} \\cdot \\mbox{age} \\cdot \\mathbb{I}_{\\mbox{male}}(x) + \\epsilon_i, \\nonumber\n\\end{align}\\]\nwhere \\(\\beta_{\\mbox{age, male}} \\cdot \\mbox{age} \\cdot \\mathbb{I}_{\\mbox{male}}(x)\\) corresponds to the interaction term.\nIn order to fit an interaction term within our regression model we replace the + sign with the * sign as follows:\n\nCodelm_spec &lt;- linear_reg()\nint.model &lt;- lm_spec |&gt; fit(score ~ age * gender, data = eval.score)\nint.model &lt;- int.model |&gt; extract_fit_engine()\nget_regression_table(int.model)\n\n# A tibble: 4 × 7\n  term           estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept         4.88      0.205     23.8    0        4.48     5.29 \n2 age              -0.018     0.004     -3.92   0       -0.026   -0.009\n3 gender: male     -0.446     0.265     -1.68   0.094   -0.968    0.076\n4 age:gendermale    0.014     0.006      2.45   0.015    0.003    0.024\n\n\nHence, the regression line for females is given by:\n\\[\\widehat{\\mbox{score}} = 4.88 - 0.018 \\cdot \\mbox{age},\\] while the regression line for males is given by:\n\\[\\widehat{\\mbox{score}} = 4.88 - 0.018 \\cdot \\mbox{age} - 0.446 + 0.014 \\cdot \\mbox{age} = 4.434 - 0.004 \\cdot \\mbox{age}.\\]\n\n\n\n\n\n\n Question\n\n\n\nHow do they compare with the teaching score values from the parallel regression lines model?\n\n\nAnswer\n\nHere, we can see that, although the intercept for male instructors may be lower, the associated average decrease in teaching score with age (0.004) is not as severe as it is for female instructors (0.018).\n\n\n\n\nNow we have to assess the fit of the model by looking at plots of the residuals. We shall do this for the interaction model. First, we need to obtain the fitted values and residuals from the interaction model as follows:\n\nCoderegression.points &lt;- get_regression_points(int.model)\n\n# A tibble: 463 × 6\n      ID score   age gender score_hat residual\n   &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1     1   4.7    36 female      4.25    0.448\n 2     2   4.1    36 female      4.25   -0.152\n 3     3   3.9    36 female      4.25   -0.352\n 4     4   4.8    36 female      4.25    0.548\n 5     5   4.6    59 male        4.20    0.399\n 6     6   4.3    59 male        4.20    0.099\n 7     7   2.8    59 male        4.20   -1.40 \n 8     8   4.1    51 male        4.23   -0.133\n 9     9   3.4    51 male        4.23   -0.833\n10    10   4.5    40 female      4.18    0.318\n# ℹ 453 more rows\n\n\nLet’s start by looking at a scatterplot of the residuals against the explanatory variable by gender:\n\nCodeggplot(regression.points, aes(x = age, y = residual)) +\n  geom_point() +\n  labs(x = \"age\", y = \"Residual\") +\n  geom_hline(yintercept = 0, col = \"blue\", size = 1) +\n  facet_wrap(~ gender)\n\n\n\nResiduals vs the explanatory variable age by gender.\n\n\n\nNow, we can plot the residuals against the fitted values:\n\nCodeggplot(regression.points, aes(x = score_hat, y = residual)) +\n  geom_point() +\n  labs(x = \"Fitted values\", y = \"Residual\") +\n  geom_hline(yintercept = 0, col = \"blue\", size = 1) +\n  facet_wrap(~ gender)\n\n\n\nResiduals vs the fitted values.\n\n\n\n\nFinally, let’s plot histograms of the residuals to assess whether they are normally distributed with mean zero:\n\nCodeggplot(regression.points, aes(x = residual)) +\n  geom_histogram(binwidth = 0.25, color = \"white\") +\n  labs(x = \"Residual\") +\n  facet_wrap(~gender)\n\n\n\nHistograms of the residuals by gender."
  },
  {
    "objectID": "notes_24_25.html#exploratory-data-analysis",
    "href": "notes_24_25.html#exploratory-data-analysis",
    "title": "Week 4: Regression modelling part 2",
    "section": "",
    "text": "Start by subsetting the evals data set so that we only have the variables we are interested in, that is, score, age and gender. Note, it is best to give your new data set a different name than evals as to not overwrite the original evals data set. Your new data set should look like the one below.\n\n\n\n\n\n\n Question\n\n\n\nYou can also view your data set using the glimpse function, or by opening a spreadsheet view in RStudio using the View function.\nUse the skim function to obtain some summary statistics from our data:\n\n\nAnswer\n\n\nCodeeval.score |&gt;\n  skim()\n\n\nData summary\n\n\nName\neval.score\n\n\nNumber of rows\n463\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n7\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\ngender\n0\n1\nFALSE\n2\nmal: 268, fem: 195\n\n\nethnicity\n0\n1\nFALSE\n2\nnot: 399, min: 64\n\n\nlanguage\n0\n1\nFALSE\n2\neng: 435, non: 28\n\n\nrank\n0\n1\nFALSE\n3\nten: 253, ten: 108, tea: 102\n\n\npic_outfit\n0\n1\nFALSE\n2\nnot: 386, for: 77\n\n\npic_color\n0\n1\nFALSE\n2\ncol: 385, bla: 78\n\n\ncls_level\n0\n1\nFALSE\n2\nupp: 306, low: 157\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1\n232.00\n133.80\n1.00\n116.50\n232.00\n347.5\n463.00\n▇▇▇▇▇\n\n\nprof_ID\n0\n1\n45.15\n27.55\n1.00\n20.00\n43.00\n70.5\n94.00\n▇▇▆▆▆\n\n\nscore\n0\n1\n4.17\n0.54\n2.30\n3.80\n4.30\n4.6\n5.00\n▁▁▅▇▇\n\n\nage\n0\n1\n48.37\n9.80\n29.00\n42.00\n48.00\n57.0\n73.00\n▅▆▇▆▁\n\n\nbty_avg\n0\n1\n4.42\n1.53\n1.67\n3.17\n4.33\n5.5\n8.17\n▃▇▇▃▂\n\n\ncls_did_eval\n0\n1\n36.62\n45.02\n5.00\n15.00\n23.00\n40.0\n380.00\n▇▁▁▁▁\n\n\ncls_students\n0\n1\n55.18\n75.07\n8.00\n19.00\n29.00\n60.0\n581.00\n▇▁▁▁▁\n\n\n\n\n\n\n\n\nNow, let’s compute the correlation coefficient between our outcome variable score and our numerical explanatory variable age:\n\nCodeeval.score |&gt; \n  get_correlation(formula = score ~ age)\n\n# A tibble: 1 × 1\n     cor\n   &lt;dbl&gt;\n1 -0.107\n\n\n\n\n\n\n\n\n Question\n\n\n\nWhy do we not include the categorical variable gender when calculating the correlation?\n\n\nAnswer\n\nThe correlation coefficient only exists between numerical variables, which is why we do not include our categorical variable gender.\n\n\n\nWe can now visualise our data by producing a scatterplot, where seeing as we have the categorical variable gender, we shall plot the points using different colours for each gender:\n\nCodeggplot(eval.score, aes(x = age, y = score, color = gender)) +\n  geom_jitter() +\n  labs(x = \"Age\", y = \"Teaching Score\", color = \"Gender\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n Instructor evaluation scores by age and gender. The points have been jittered.\n\n\n\nNote: The above code has jittered the points, however, this is not necessary and geom_point would suffice. To plot separate points by gender we simply add the color argument to the aes function and pass to it gender.\nFrom the scatterplot we can see that:\n\nThere are very few women over the age of 60 in our data set.\nFrom the plotted regression lines we can see that the lines have different slopes for men and women. That is, the associated effect of increasing age appears to be more severe for women than it does for men, i.e. the teaching score of women drops faster with age."
  },
  {
    "objectID": "notes_24_25.html#multiple-regression-parallel-slopes-model",
    "href": "notes_24_25.html#multiple-regression-parallel-slopes-model",
    "title": "Week 4: Regression modelling part 2",
    "section": "",
    "text": "Here, we shall begin by fitting what is referred to as a parallel regression lines model. This model implies that the slope of relationship between teaching score (score) and age (age) is the same for both males and females, with only the intercept of the regression lines changing. Hence, our parallel regression lines model is given as:\n\\[\\begin{align}\ny_{i} &= \\alpha + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i \\nonumber \\\\\n&= \\alpha + \\beta_{\\mbox{age}} \\cdot \\mbox{age} + \\beta_{\\mbox{male}} \\cdot \\mathbb{I}_{\\mbox{male}}(x) + \\epsilon_i, \\nonumber\n\\end{align}\\]\nwhere\n\n\\(\\alpha\\) is the intercept of the regression line for females;\n\\(\\beta_{\\mbox{age}}\\) is the slope of the regression line for both males and females;\n\\(\\beta_{\\mbox{male}}\\) is the additional term added to \\(\\alpha\\) to get the intercept of the regression line for males; and\n\n\\(\\mathbb{I}_{\\mbox{male}}(x)\\) is an indicator function such that\n\\[\\mathbb{I}_{\\mbox{male}}(x)=\\left\\{\n            \\begin{array}{ll}\n              1 ~~~ \\mbox{if gender} ~ x ~ \\mbox{is male},\\\\\n              0 ~~~ \\mbox{Otherwise}.\\\\\n            \\end{array}\n          \\right.\\]\n\n\nWe can fit the parallel regression lines model as follows:\n\nCodelm_spec &lt;- linear_reg() |&gt; set_engine(\"lm\")\npar.model &lt;- lm_spec |&gt; fit(score ~ age + gender, data = eval.score)\npar.model &lt;- par.model |&gt; extract_fit_engine()\nget_regression_table(par.model)\n\n# A tibble: 3 × 7\n  term         estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept       4.48      0.125     35.8    0        4.24     4.73 \n2 age            -0.009     0.003     -3.28   0.001   -0.014   -0.003\n3 gender: male    0.191     0.052      3.63   0        0.087    0.294\n\n\nHence, the regression line for females is given by:\n\\[\\widehat{\\mbox{score}} = 4.48 - 0.009 \\cdot \\mbox{age},\\]\nwhile the regression line for males is given by:\n\\[\\widehat{\\mbox{score}} = 4.48 - 0.009 \\cdot \\mbox{age} + 0.191 = 4.671 - 0.009 \\cdot \\mbox{age}.\\] Now, let’s superimpose our parallel regression lines onto the scatterplot of teaching score against age:\n\nCodeggplot(eval.score, aes(x = age, y = score, color = gender)) +\n  geom_jitter() +\n  labs(x = \"Age\", y = \"Teaching Score\", color = \"Gender\") +\n  geom_parallel_slopes(se = FALSE)\n\n\n\n Instructor evaluation scores by age and gender with parallel regression lines superimposed.\n\n\n\nNote: go through the code used to create coeff and slopes and make sure you understand it.\nFrom the parallel regression lines model both males and females have the same slope, that is, the associated effect of age on teaching score is the same for both men and women. Hence, for every one year increase in age, there is an associated decrease in teaching score of 0.009. However, male instructors have a higher intercept term, that is, there is a vertical bump in the regression line for males in teaching scores. This is linked to the average difference in teaching scores that males obtain relative to females.\n\n\n\n\n\n\n Question\n\n\n\nWhat is different between our previous scatterplot of teaching score against age (Figure 6) and the one we just created with our parallel lines superimposed (Figure 7)?\n\n\nAnswer\n\nIn the original plot we have what is referred to as an interaction effect between age and gender. Hence, gender interacts in different ways for both males and females by age, and as such we should have different intercepts and slopes."
  },
  {
    "objectID": "notes_24_25.html#multiple-regression-interaction-model",
    "href": "notes_24_25.html#multiple-regression-interaction-model",
    "title": "Week 4: Regression modelling part 2",
    "section": "",
    "text": "There is an interaction effect if the associated effect of one variable depends on the value of another variable. For example, the effect of age here will depend on whether the instructor is male or female, that is, the effect of age on teaching scores will differ by gender. The interaction model can be written as:\n\\[\\begin{align}\ny_{i} &= \\alpha + \\beta_1  x_{1i} + \\beta_2  x_{2i} + \\beta_3  x_{1i}  x_{2i} + \\epsilon_i \\nonumber \\\\\n&= \\alpha + \\beta_{\\mbox{age}} \\cdot \\mbox{age} + \\beta_{\\mbox{male}} \\cdot \\mathbb{I}_{\\mbox{male}}(x) + \\beta_{\\mbox{age, male}} \\cdot \\mbox{age} \\cdot \\mathbb{I}_{\\mbox{male}}(x) + \\epsilon_i, \\nonumber\n\\end{align}\\]\nwhere \\(\\beta_{\\mbox{age, male}} \\cdot \\mbox{age} \\cdot \\mathbb{I}_{\\mbox{male}}(x)\\) corresponds to the interaction term.\nIn order to fit an interaction term within our regression model we replace the + sign with the * sign as follows:\n\nCodelm_spec &lt;- linear_reg()\nint.model &lt;- lm_spec |&gt; fit(score ~ age * gender, data = eval.score)\nint.model &lt;- int.model |&gt; extract_fit_engine()\nget_regression_table(int.model)\n\n# A tibble: 4 × 7\n  term           estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept         4.88      0.205     23.8    0        4.48     5.29 \n2 age              -0.018     0.004     -3.92   0       -0.026   -0.009\n3 gender: male     -0.446     0.265     -1.68   0.094   -0.968    0.076\n4 age:gendermale    0.014     0.006      2.45   0.015    0.003    0.024\n\n\nHence, the regression line for females is given by:\n\\[\\widehat{\\mbox{score}} = 4.88 - 0.018 \\cdot \\mbox{age},\\] while the regression line for males is given by:\n\\[\\widehat{\\mbox{score}} = 4.88 - 0.018 \\cdot \\mbox{age} - 0.446 + 0.014 \\cdot \\mbox{age} = 4.434 - 0.004 \\cdot \\mbox{age}.\\]\n\n\n\n\n\n\n Question\n\n\n\nHow do they compare with the teaching score values from the parallel regression lines model?\n\n\nAnswer\n\nHere, we can see that, although the intercept for male instructors may be lower, the associated average decrease in teaching score with age (0.004) is not as severe as it is for female instructors (0.018)."
  },
  {
    "objectID": "notes_24_25.html#assessing-model-fit",
    "href": "notes_24_25.html#assessing-model-fit",
    "title": "Week 4: Regression modelling part 2",
    "section": "",
    "text": "Now we have to assess the fit of the model by looking at plots of the residuals. We shall do this for the interaction model. First, we need to obtain the fitted values and residuals from the interaction model as follows:\n\nCoderegression.points &lt;- get_regression_points(int.model)\n\n# A tibble: 463 × 6\n      ID score   age gender score_hat residual\n   &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1     1   4.7    36 female      4.25    0.448\n 2     2   4.1    36 female      4.25   -0.152\n 3     3   3.9    36 female      4.25   -0.352\n 4     4   4.8    36 female      4.25    0.548\n 5     5   4.6    59 male        4.20    0.399\n 6     6   4.3    59 male        4.20    0.099\n 7     7   2.8    59 male        4.20   -1.40 \n 8     8   4.1    51 male        4.23   -0.133\n 9     9   3.4    51 male        4.23   -0.833\n10    10   4.5    40 female      4.18    0.318\n# ℹ 453 more rows\n\n\nLet’s start by looking at a scatterplot of the residuals against the explanatory variable by gender:\n\nCodeggplot(regression.points, aes(x = age, y = residual)) +\n  geom_point() +\n  labs(x = \"age\", y = \"Residual\") +\n  geom_hline(yintercept = 0, col = \"blue\", size = 1) +\n  facet_wrap(~ gender)\n\n\n\nResiduals vs the explanatory variable age by gender.\n\n\n\nNow, we can plot the residuals against the fitted values:\n\nCodeggplot(regression.points, aes(x = score_hat, y = residual)) +\n  geom_point() +\n  labs(x = \"Fitted values\", y = \"Residual\") +\n  geom_hline(yintercept = 0, col = \"blue\", size = 1) +\n  facet_wrap(~ gender)\n\n\n\nResiduals vs the fitted values.\n\n\n\n\nFinally, let’s plot histograms of the residuals to assess whether they are normally distributed with mean zero:\n\nCodeggplot(regression.points, aes(x = residual)) +\n  geom_histogram(binwidth = 0.25, color = \"white\") +\n  labs(x = \"Residual\") +\n  facet_wrap(~gender)\n\n\n\nHistograms of the residuals by gender."
  },
  {
    "objectID": "notes_24_25.html#inference-using-sample-statistics",
    "href": "notes_24_25.html#inference-using-sample-statistics",
    "title": "Week 4: Regression modelling part 2",
    "section": "\n2.1 Inference using sample statistics",
    "text": "2.1 Inference using sample statistics\nThe table below lists a variety of contexts where sample statistics can be used to estimate population parameters. In all 6 cases, the point estimate/sample statistic estimates the unknown population parameter. It does so by computing summary statistics based on a sample of size \\(n\\). We’ll cover Scenarios 5 and 6, namely construct CIs for the parameters in simple and multiple linear regression models. We will consider CIs based on theoretical results when standard assumptions hold, although sampling procedures such as bootstrap also exist. We will also consider how to use CIs for variable selection and finish by considering a model selection strategy based on objective measures for model comparisons.\n\n\n\n\n\n\n\n\n\n\nTable 1: Scenarios of sample statistics for inference.\n\n\n\n\n\n\n\n\n\nScenario\nPopulation Parameter\nPopulation Notation\nSample Statistic\nSample Notation\n\n\n\n1\nPopulation proportion\n\\(p\\)\nSample proportion\n\\(\\widehat{p}\\)\n\n\n2\nPopulation mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\n\n3\nDiff.in pop. props\n\\(p_1 - p_2\\)\nDiff. in sample props\n\\(\\widehat{p}_1 - \\widehat{p}_2\\)\n\n\n4\nDiff. in pop. means\n\\(\\mu_1 - \\mu_2\\)\nDiff. in sample means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\n\n5\nPop. intercept\n\\(\\beta_0\\)\nSample intercept\n\n\\(\\widehat{\\beta}_0\\) or \\(b_0\\)\n\n\n\n6\nPop. slope\n\\(\\beta_1\\)\nSample slope\n\n\\(\\widehat{\\beta}_1\\) or \\(b_1\\)\n\n\n\n\nIn reality, we don’t have access to the population parameter values (if we did, why would we need to estimate them?) we only have a single sample of data from a larger population. We’d like to be able to make some reasonable guesses about population parameters using that single sample to create a range of plausible values for a population parameter. This range of plausible values is known as a confidence interval.\nThere are theoretical ways of defining confidence intervals for these different scenarios (such as you saw in ‘Statistical Inference’ in Semester 1). But we can also use a single sample to get some idea of how other samples might vary in terms of their sample statistics, i.e. to estimate the sampling distributions of sample statistics. One common way this is done is via a process known as bootstrapping.\nThe confidence intervals we will see this week are calculated using the theoretical results based on the standard assumptions that you will have seen in Regression Modelling in first semester. These values are not based on bootstrapping techniques since these become much harder to implement when working with multiple variables and its beyond the scope of this course."
  },
  {
    "objectID": "notes_24_25.html#confidence-intervals-for-regression-parameters",
    "href": "notes_24_25.html#confidence-intervals-for-regression-parameters",
    "title": "Week 4: Regression modelling part 2",
    "section": "\n3.1 Confidence Intervals for Regression Parameters",
    "text": "3.1 Confidence Intervals for Regression Parameters\nTo illustrate this, let’s have another look at teaching evaluations data evals in the moderndive package that we used in Week 3 and start with the SLR model with age as the the single explanatory variable and the instructors’ evaluation scores as the response variable. This data and the fitted model are shown here.\n\nCodeslr.model &lt;- linear_reg()\nslr.model &lt;- slr.model |&gt; fit(score ~ age, data = evals)\n\n (Intercept)          age \n 4.461932354 -0.005938225 \n\n\n\n\n\n\nFigure 1: SLR model applied to the teaching evaluation Data.\n\n\n\nThe point estimate of the slope parameter here is \\(\\widehat{\\beta}=\\) -0.006.\nLet’s continue with the teaching evaluations data by fitting the multiple regression model with one numerical and one categorical explanatory variable. In this model:\n\n\n\\(y\\): response variable of instructor evaluation score\n\nexplanatory variables\n\n\n\\(x_1\\): numerical explanatory variable of age\n\n\n\\(x_2\\): categorical explanatory variable of gender\n\n\n\n\n\nCodeevals_multiple &lt;- evals |&gt;\n                  select(score, gender, age)\n\n\nFirst, recall that we had two competing potential models to explain professors’ teaching evaluation scores:\n\nModel 1: Parallel lines model (no interaction term) - both male and female professors have the same slope describing the associated effect of age on teaching score\nModel 2: Interaction model - allowing for male and female professors to have different slopes describing the associated effect of age on teaching score\n\nRefresher: Visualisations\nRecall the plots we made for both these models:\n\n\n\n\nModel 1: Parallel regression lines.\n\n\n\n\n\n\n\nModel 2: Separate regression lines.\n\n\n\nRefresher: Regression tables\nLet’s also recall the regression models. First, the regression model with no interaction effect: note the use of + in the formula.\n\nCodepar.model &lt;- linear_reg()\npar.model &lt;- par.model |&gt; \n  fit(score ~ age + gender, data = evals_multiple) |&gt; \n  extract_fit_engine()\n\nget_regression_table(par.model) |&gt; \n  knitr::kable(\n               digits = 3,\n               caption = \"Model 1: Regression model with no interaction effect included.\", \n               booktabs = TRUE\n        )\n\n\nModel 1: Regression model with no interaction effect included.\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\nintercept\n4.484\n0.125\n35.792\n0.000\n4.238\n4.730\n\n\nage\n-0.009\n0.003\n-3.280\n0.001\n-0.014\n-0.003\n\n\ngender: male\n0.191\n0.052\n3.632\n0.000\n0.087\n0.294\n\n\n\n\n\nSecond, the regression model with an interaction effect: note the use of * in the formula.\n\nCodeint.model &lt;- linear_reg()\nint.model &lt;- int.model  |&gt;\n  fit(score ~ age * gender, data = evals_multiple) |&gt;\n  extract_fit_engine()\n\nget_regression_table(int.model) |&gt; \n  knitr::kable(\n                digits = 3,\n                caption = \"Model 2: Regression model with interaction effect included.\", \n                booktabs = TRUE\n      )\n\n\nModel 2: Regression model with interaction effect included.\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\nintercept\n4.883\n0.205\n23.795\n0.000\n4.480\n5.286\n\n\nage\n-0.018\n0.004\n-3.919\n0.000\n-0.026\n-0.009\n\n\ngender: male\n-0.446\n0.265\n-1.681\n0.094\n-0.968\n0.076\n\n\nage:gendermale\n0.014\n0.006\n2.446\n0.015\n0.003\n0.024\n\n\n\n\n\nNotice that, together with the estimated parameter values, the tables include other information about each estimated parameter in the model, namely:\n\n\nstd_error: the standard error of each parameter estimate;\n\nstatistic: the test statistic value used to test the null hypothesis that the population parameter is zero;\n\np_value: the \\(p\\) value associated with the test statistic under the null hypothesis; and\n\nlower_ci and upper_ci: the lower and upper bounds of the 95% confidence interval for the population parameter\n\nThese values are calculated using the theoretical results based on the standard assumptions that you will have seen in Regression Modelling in first semester. Theses values are not based on bootstrapping techniques but theoretical results since these become much harder to implement when working with multiple variables and its beyond the scope of this course."
  }
]