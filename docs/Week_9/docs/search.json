[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Week 8: Generalised Linear Models part 2",
    "section": "",
    "text": "Last week we introduced Generalised Linear Models (GLMs). Particularly, we looked at logistic regression to model outcomes of interest that take one of two categorical values (e.g.yes/no, success/failure, alive/dead). This week we will continue reviewing logistic regression to model grouped binary outcomes (e.g. number of successes out of a fixed number of trials) and then we we will generalise this to situations where the response variable is categorical with more than two categories. First lets look at the framework for modelling categorical data with only two categories, i.e.\n\nbinary, taking the value 1 (say success, with probability \\(p\\)) or 0 (failure, with probability \\(1-p\\)) or\nbinomial, where \\(y_i\\) is the number of events (successes) in a given number of trials \\(n_i\\), with the probability of success being \\(p_i\\) and the probability of failure being \\(1-p_i\\).\n\nIn both cases the distribution of \\(y_i\\) is assumed to be binomial, but in the first case it is \\(\\mathrm{Bin}(1,p_i)\\) and in the second case it is \\(\\mathrm{Bin}(n_i,p_i)\\). The first case was covered last week, so now lets focus on the second case.\nBefore we proceed, we will load all the packages needed for this week:\n\nCodelibrary(tidyr)\nlibrary(ggplot2)\nlibrary(moderndive)\nlibrary(sjPlot)\nlibrary(tidymodels)\nlibrary(broom)\nlibrary(performance)\nlibrary(faraway)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "index.html#modelling-grouped-binary-data-with-a-categorical-covariate",
    "href": "index.html#modelling-grouped-binary-data-with-a-categorical-covariate",
    "title": "Week 8: Generalised Linear Models part 2",
    "section": "\n2.1 Modelling grouped binary data with a categorical covariate",
    "text": "2.1 Modelling grouped binary data with a categorical covariate\nIn the last section we reviewed the case when the explanatory variable was continuous., lets look now at the case when the explanatory variable is categorical.\nTo illustrate how the previous model works with categorical predictors we can discretized the temperature values into arbitrary categories as follows:\n\\[\n\\mathrm{temperature~ category} =\\begin{cases} \\mathrm{temperature} &gt; 29^\\circ C  &  \\text{high} \\\\ \\mathrm{temperature} &gt; 28^\\circ C& \\text{medium} \\\\ \\text{else} & \\text{low}\\end{cases}\n\\]\nIn R we can use the case_when() function to accomplish this:\n\nCodeturtles = turtles  %&gt;% mutate(\n  temp_fct = case_when(\n    temp &gt; 29 ~ \"high\",\n    temp &gt; 28 ~ \"medium\",\n    .default = \"low\"\n  ) %&gt;% as.factor()\n) \n\n\nNow, recall that as usual, R will set the baseline category for our explanatory variable in alphabetical order, i.e. the high temperature level will be treated as reference for the dicretized variable. However, we already know that the chances of a male being hatched increases with higher incubation temperatures.\nThus, it makes sense to assess how the chances of a male being hatched are affected by comparing higher temperature categories against lower ones. This implies that we will set low to be our reference category. Luckily, we have seen in previous tasks how to do this using the relevel() function:\n\nturtles = turtles %&gt;%\n  mutate(temp_fct = relevel(temp_fct,ref = \"low\")) \n\nWe can now fit a logistic regression using the low temperature level as the reference category for our dicretized temperature covariate. The model is then given by:\n\\[\\begin{align}\ny_i &\\sim \\mathrm{Binomial}(n_ip_i)\\\\\n\\mathrm{logit}(p_i) &= \\alpha +  \\beta_{1}  \\times \\mathbb{I}_{\\mathrm{temperature}}(\\mathrm{high}) + \\beta_{2}  \\times \\mathbb{I}_{\\mathrm{temperature}}(\\mathrm{medium}).\n\\end{align}\\]\n\n\\(\\alpha\\) represent the log-odds of a male turtle being hatched in low incubation temperature.\n\\(\\beta_1\\) are the change int the log-odds of a male turtle being hatched given it was incubated in a high temperature condition compared to a low one.\n\\(\\mathbb{I}_{\\mathrm{temperature}}(\\mathrm{high})\\) is an indicator variable that takes the value of 1 if the \\(i\\)th experiment replicate was conducted on a high temperature.\n\\(\\beta_2\\) are the change int the log-odds of a male turtle being hatched given it was incubated in a medium condition compared to a low one.\n\\(\\mathbb{I}_{\\mathrm{temperature}}(\\mathrm{medium})\\) is an indicator variable that takes the value of 1 if the \\(i\\)th experiment replicate was conducted on a medium temperature.\n\nIn R, the model can be fitted as follows:\n\nmodel_turtles_2 &lt;- glm(cbind(male,female) ~ temp_fct,\n                     data = turtles,\n                     family = binomial)\n\nLets print the model estimates odds scale and 95% confidence intervals (remember we can achieve this by setting conf.int=TRUE and exponentiate=TRUE within the tidy function):\n\nmodel_turtles_2 %&gt;% broom::tidy(conf.int = T,exponentiate = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n0.59\n0.29\n-1.80\n0.07\n0.33\n1.04\n\n\ntemp_fcthigh\n45.47\n1.06\n3.61\n0.00\n8.59\n843.97\n\n\ntemp_fctmedium\n6.32\n0.44\n4.23\n0.00\n2.76\n15.31\n\n\n\n\n\nWe can see that the odds of a male being hacthed if it was incubated on a low temperature condition are 0.59 the odds of a female being hatched if it was incubated on the same condition.\nAlternatively, we could interpret this as the odds of female being hatched in a low temperature incubation settings being \\(\\mathrm{exp}(\\alpha)^{-1} =\\) 1.68 higher than the odds of a male being hatched under the same setting. However, there is not enough evidence to support that the change in the odds is statistically significant since the confidence interval ( 0.33 , 1.04) contains 1 (remember we are in the odds scale).\nOn the other hand, the odds of a male being hatched are 45.47! significantly higher in a high temperature setting compared to a low temperature. Likewise, the odds of a male being hatched are 6.32 higher in a medium temperature condition compared to a low one.\nWhat if we want to compare the odds of a male being hatched if the egg was incubate on a high temperture conditon against a medium one?\nIn that case, we we will be looking at the following odds ratio:\n\\[\\begin{align}\n\\dfrac{\\mathrm{Odds}(\\mathrm{male}=1|\\mathrm{temperature}=high)}{\\mathrm{Odds}(\\mathrm{male}=1|\\mathrm{temperature}=medium)} &= \\dfrac{\\mathrm{exp}(\\alpha+\\beta_1)}{\\mathrm{exp}(\\alpha+\\beta_2)} \\\\\n&= \\mathrm{exp}(\\alpha+\\beta_1 - \\alpha - \\beta_2) \\\\\n&= \\mathrm{exp}(\\beta_1 - \\beta_2) = \\frac{\\mathrm{exp}(\\beta_1)}{ \\mathrm{exp}(\\beta_2)}\n\\end{align}\\]\nWhere \\(\\beta_1\\) and \\(\\beta_2\\) are the coefficients in the log-odd scale. However since we already have \\(\\mathrm{exp}(\\beta_1)=\\) 45.47 and \\(\\mathrm{exp}(\\beta_2)=\\) 6.32, then the odds of male being hatched from an egg that was incubated on a high temperature condition are \\(\\frac{45.47}{6.32}=\\) 7.2 greater than the one that was incubate on a medium temperature condition.\nFinally, we can calculate the probabilities of a male being hatched in each temperature condition as follows:\n\n\n\\(P(\\mathrm{male}=1|\\mathrm{temperature}=low) = \\dfrac{\\mathrm{\\mathrm{exp}(\\alpha})}{1 + \\mathrm{exp}(\\alpha)}.\\) In R this is:\n\nplogis(coef(model_turtles_2)[1])\n\n(Intercept) \n   0.372549 \n\n\n\n\n\\(P(\\mathrm{male}=1|\\mathrm{temperature}=medium) = \\dfrac{\\mathrm{\\mathrm{exp}(\\alpha + \\beta_1})}{1 + \\mathrm{exp}(\\alpha + \\beta_1)}.\\) In R this is equivalent to:\n\nplogis(coef(model_turtles_2)[1] + coef(model_turtles_2)[3])\n\n(Intercept) \n  0.7894737 \n\n\n\n\n\\(P(\\mathrm{male}=1|\\mathrm{temperature}=high) = \\dfrac{\\mathrm{\\mathrm{exp}(\\alpha + \\beta_2})}{1 + \\mathrm{exp}(\\alpha + \\beta_2)}\\). In R this is computed as:\n\nplogis(coef(model_turtles_2)[1] + coef(model_turtles_2)[2])\n\n(Intercept) \n  0.9642857 \n\n\n\n\nWe can visualize this probabilities using the plot_model() function as follows:\n\nCodeplot_model(type = \"pred\",\n           model_turtles_2,\n           terms = \"temp_fct\",\n           axis.title = c(\"Temperature Category\",\n                          \"Prob. of a hatchling being male\"),\n           title = \" \")"
  },
  {
    "objectID": "index.html#nominal-logistic-regression",
    "href": "index.html#nominal-logistic-regression",
    "title": "Week 8: Generalised Linear Models part 2",
    "section": "\n3.1 Nominal logistic regression",
    "text": "3.1 Nominal logistic regression\nNominal logistic regression, also known as multinomial logistic regression is used when there is no natural order among the response categories, for example:\n\nEye colour: Blue, Green, Brown, Hazel\nHouse types: Bungalow, Duplex, Terrace\nType of pet: Dog, Cat, Rodent, Fish, Bird\nGenotype: AA, Aa, aa\n\nThe goal is to estimate the probabilities for each class \\(j\\) (where \\(j \\in \\{1,2,\\ldots,J\\}\\)) based on the independent variables \\(\\mathbf{x}\\). The probability of the \\(j\\)th class is then given by:\n\\[\nP(Y=j|\\mathbf{x})= \\dfrac{\\mathrm{exp}(\\mathbf{x}^\\intercal \\boldsymbol{\\beta}_j)}{\\sum_{k=1}^J\\mathrm{exp}(\\mathbf{x}^\\intercal \\boldsymbol{\\beta}_k)}\n\\]\nTypically, one category is arbitrarily chosen as the reference category, and all other categories are compared with it. Suppose category \\(J\\) is chosen as the reference category. The log-odds for the other categories relative to the reference are:\n\\[\n\\mathrm{log}\\left(\\dfrac{P(Y=j|\\mathbf{x})}{P(Y=J|\\mathbf{x})}\\right) = \\mathrm{log}\\left(\\dfrac{p_j}{p_J}\\right) =\\mathbf{x}^\\intercal\\boldsymbol{\\beta}_j, ~~\\text{for } j=1,\\dots,J-1.\n\\tag{2}\\]"
  },
  {
    "objectID": "index.html#parameter-estimation-and-fitted-values",
    "href": "index.html#parameter-estimation-and-fitted-values",
    "title": "Week 8: Generalised Linear Models part 2",
    "section": "\n3.2 Parameter estimation and fitted values",
    "text": "3.2 Parameter estimation and fitted values\nThe \\(J-1\\) log-odds in Equation 2 are solved simultaneously to estimate the parameters \\(\\boldsymbol{\\beta}_j\\).\nGiven parameter estimates \\(\\hat{\\boldsymbol{\\beta}}_j\\), the linear predictors \\(\\mathbf{x}^\\intercal\\hat{\\boldsymbol{\\beta}}_j\\) can be calculated.\nFrom Equation 2, we derive:\n\\[\n\\hat{p}_j=\\hat{p}_J\\mathrm{exp}(\\mathbf{x}^\\intercal\\hat{\\boldsymbol{\\beta}}_j)\n\\tag{3}\\]\nNow we can express \\(\\hat{p}_J\\) in terms of the other probabilities by using the fact that \\(\\sum_{j=1}^{J-1} \\hat{p}_j + \\hat{p}_J = \\hat{p}_1+\\hat{p}_2+\\dots+\\hat{p}_J= 1.\\) For instance, substituting Equation 3 for \\(j = 1,\\ldots,J-1\\) in the sumation above yields to:\n\\[\n\\sum_{j=1}^{J-1} \\hat{p}_J\\mathrm{exp}(\\mathbf{x}^\\intercal\\hat{\\boldsymbol{\\beta}}_j) + \\hat{p}_J = 1.\n\\]\nSolving for \\(\\hat{p}_J\\) yields to\n\\[\n\\hat{p}_J\\left(\\sum_{j=1}^{J-1} \\mathrm{exp}(\\mathbf{x}^\\intercal\\hat{\\boldsymbol{\\beta}}_j) + 1\\right) = 1 \\\\ \\Rightarrow\\hat{p}_J = \\dfrac{1}{1+\\sum_{j=1}^{J-1} \\mathrm{exp}(\\mathbf{x}^\\intercal\\hat{\\boldsymbol{\\beta}}_j)}\n\\tag{4}\\]\nHence, the probabilities for each class are:\n\nFor the reference class \\(J:\\)\\[\n    \\hat{p}_J = \\dfrac{1}{1+\\sum_{j=1}^{J-1} \\mathrm{exp}(\\mathbf{x}^\\intercal\\hat{\\boldsymbol{\\beta}}_j)}\n    \\]\nBy substituting \\(\\hat{p}_J\\) in Equation 3 we find \\(\\hat{p}_j\\) for class \\(j = 1,2,\\ldots,J-1:\\) \\[\\hat{p}_j=\\dfrac{\\exp(\\mathbf{x}^\\intercal\\hat{\\boldsymbol{\\beta}}_j)}{1+\\sum_{j=1}^{J-1}\\exp(\\mathbf{x}^\\intercal\\hat{\\boldsymbol{\\beta}}_j)}. \\tag{5}\\]\n\nFitted values (expected frequencies) can be calculated for each covariate pattern by multiplying the estimated probabilities \\(\\hat{p}_j\\) by the total frequency of the covariate pattern. Parameter estimates \\(\\hat{\\boldsymbol{\\beta}}_j\\) depend on the choice of reference category, but estimated probabilities and hence, fitted values (predicted counts), don’t."
  },
  {
    "objectID": "index.html#example-fitting-a-nominal-logistic-regression-in-r",
    "href": "index.html#example-fitting-a-nominal-logistic-regression-in-r",
    "title": "Week 8: Generalised Linear Models part 2",
    "section": "\n3.3 Example: Fitting a nominal logistic regression in R",
    "text": "3.3 Example: Fitting a nominal logistic regression in R\nIn this example we look at data on subjects that were interviewed about the importance of various features when buying a car(McFadden et al. 2000).\nWe focus in particular on the importance of power steering and air conditioning. The variables available in this dataset are:\n\n\nsex: woman/man\n\nage: 18-23, 24-40, &gt;40\n\nresponse: no/little, important, very important\n\nfrequency: number of interviewed people on each group\n\nThe data set is available on the dobson R package. Lets begin loading it and produce some exploratory plots.\n\nCodedcars = dobson::Cars\n\ndcars$response &lt;- factor(dcars$response, \n                         levels = c(\"no/little\", \"important\", \"very important\")) \ndcars$age &lt;- factor(dcars$age, \n                         levels = c(\"18-23\", \"24-40\", \"&gt; 40\")) \n\n\n\n\n\n\nsex\nage\nresponse\nfrequency\n\n\n\nwomen\n18-23\nno/little\n26\n\n\nwomen\n18-23\nimportant\n12\n\n\nwomen\n18-23\nvery important\n7\n\n\nwomen\n24-40\nno/little\n9\n\n\nwomen\n24-40\nimportant\n21\n\n\nwomen\n24-40\nvery important\n15\n\n\nwomen\n&gt; 40\nno/little\n5\n\n\nwomen\n&gt; 40\nimportant\n14\n\n\nwomen\n&gt; 40\nvery important\n41\n\n\nmen\n18-23\nno/little\n40\n\n\nmen\n18-23\nimportant\n17\n\n\nmen\n18-23\nvery important\n8\n\n\nmen\n24-40\nno/little\n17\n\n\nmen\n24-40\nimportant\n15\n\n\nmen\n24-40\nvery important\n12\n\n\nmen\n&gt; 40\nno/little\n8\n\n\nmen\n&gt; 40\nimportant\n15\n\n\nmen\n&gt; 40\nvery important\n18\n\n\n\n\n\nFrom the plots of the data below, we can see that quite a large proportion of people – a little over 58% in the over 40 category considered the features very important and, similarly 60% of young people (18-23 years old) considered these features as having no or little importance. Sex also seems to have an impact on car feature preferences, with over 40% of men considering the features of no or little importance and over 40% of women considering them very important.\n\n\nR Plot\nR Code\n\n\n\n\n\n\n\nPreferences for air conditioning and power steering in cars by gender and age.\n\n\n\n\n\n\n ggplot(dcars, aes(x = age,\n                   y = frequency, \n                   fill = response)) +\n      geom_bar(stat = \"identity\",\n               position = \"dodge\" )+\n   labs(x = \"Age groups\",y =\"Frequency\")+\n   scale_fill_manual(name = \"Response category\",\n                     values = c(\"darkorange\",\"purple\",\"cyan4\")) \n \n ggplot(dcars, aes(x = sex,\n                   y = frequency, \n                   fill = response)) +\n      geom_bar(stat = \"identity\",\n               position = \"dodge\" )+\n   labs(x = \"Sex\",y =\"Frequency\")+\n      scale_fill_manual(name = \"Response category\",\n                        values = c(\"darkorange\",\"purple\",\"cyan4\")) \n\n\n\n\nAlthough the response is really an ordinal variable, we will treat it as nominal with “no/little importance” as the reference category (also occasionally referred to as “unimportant” in the rest for brevity.). Similarly we will initially regard age as nominal.\nWe can fit the following nominal logistic regression model using the multinom() function from library(nnet):\n\\[\n\\mathrm{log}\\left(\\frac{p_j}{p_1} \\right) = \\beta_{0j}+\\beta_{1j}x_1+\\beta_{2j}x_2+\\beta_{3j}x_3, ~~ j=2,3\n\\]\nwhere\n\n\n\\(j=1\\) for “no/little importance” (the reference category)\n\n\\(j=2\\) for “important”\n\n\\(j=3\\) for “very important”\n\n\\(x_1=1\\) for women and 0 for men,\n\n\\(x_2=1\\) for age 24-40 years and 0 otherwise\n\n\\(x_3=1\\) for age \\(&gt;\\) 40 years and 0 otherwise.\n\n\nlibrary(nnet)\nmodel_cars &lt;- multinom(response ~ age + sex, weight = frequency, data = dcars)\n\n# weights:  15 (8 variable)\ninitial  value 329.583687 \niter  10 value 290.566455\nfinal  value 290.351098 \nconverged\n\n\nNotice that model converged after 10 iterations, the default number of iteration is set to 100 but you can modify this by setting maxit=X where X is the number of iterations after which the algorithm will stop.\nLet look at model summaries and interpret the coefficients.\n\ntidy(model_cars,conf.int=T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny.level\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\nimportant\n(Intercept)\n-0.98\n0.26\n-3.82\n0.00\n-1.48\n-0.48\n\n\nimportant\nage24-40\n1.13\n0.34\n3.30\n0.00\n0.46\n1.80\n\n\nimportant\nage&gt; 40\n1.59\n0.40\n3.94\n0.00\n0.80\n2.38\n\n\nimportant\nsexwomen\n0.39\n0.30\n1.29\n0.20\n-0.20\n0.98\n\n\nvery important\n(Intercept)\n-1.85\n0.33\n-5.60\n0.00\n-2.50\n-1.20\n\n\nvery important\nage24-40\n1.48\n0.40\n3.69\n0.00\n0.69\n2.26\n\n\nvery important\nage&gt; 40\n2.92\n0.42\n6.90\n0.00\n2.09\n3.75\n\n\nvery important\nsexwomen\n0.81\n0.32\n2.53\n0.01\n0.18\n1.44\n\n\n\n\n\nNotice the two sets of coefficients, for the categories important and very important that correspond to the two log-odds equations comparing these to the baseline, which is no/little importance.\nWe can interpret these coefficients in terms of odds for each logit equation. For example:\n\ntidy(model_cars,conf.int=T,exponentiate = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny.level\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\nimportant\n(Intercept)\n0.38\n0.26\n-3.82\n0.00\n0.23\n0.62\n\n\nimportant\nage24-40\n3.09\n0.34\n3.30\n0.00\n1.58\n6.04\n\n\nimportant\nage&gt; 40\n4.89\n0.40\n3.94\n0.00\n2.22\n10.78\n\n\nimportant\nsexwomen\n1.47\n0.30\n1.29\n0.20\n0.82\n2.66\n\n\nvery important\n(Intercept)\n0.16\n0.33\n-5.60\n0.00\n0.08\n0.30\n\n\nvery important\nage24-40\n4.38\n0.40\n3.69\n0.00\n2.00\n9.62\n\n\nvery important\nage&gt; 40\n18.48\n0.42\n6.90\n0.00\n8.07\n42.34\n\n\nvery important\nsexwomen\n2.25\n0.32\n2.53\n0.01\n1.20\n4.23\n\n\n\n\n\nFrom this table, we can see that the odds of considering the features important (versus no/little importance) for over 40 year-olds are 4.89 times the odds for 18-23 year olds (which is the baseline category for the age group). In general, a positive coefficient (or greater than 1 odds multiplier) tells us that older people are more likely to consider the features important than young people, which is consistent with what we observed in the exploratory plots. Similarly, we can see that women are more likely than men to consider the features important, specifically the chances a woman finds these features important are 47% greater than men and more than twice consider these features as very important compared to men. However, in the first case, the difference between women and men is not statistically significant since the 95% CI contains 1.\nIn the table below we have a summary of the coefficients, odds ratios and confidence intervals for the log-odds corresponding to important versus no/little importance.\n\n\n\n\n\n\n\nParameter, \\(\\beta\\)\n\nEstimate, \\(\\hat{\\beta}\\)\n\n\\(OR=e^{\\hat{\\beta}}\\)\n\n\n\n\n\\(\\beta_{02}\\): constant\n-0.979 (0.256)\n0.37 [0.23, 0.62]\n\n\n\n\\(\\beta_{12}\\): women\n0.388 (0.301)\n1.47 [0.82,2.66]\n\n\n\n\\(\\beta_{22}\\): 24-40\n1.128 (0.342)\n3.09 [1.58, 6.04]\n\n\n\n\\(\\beta_{32}\\): &gt;40\n1.590 (0.403)\n4.90 [2.22,10.8]"
  },
  {
    "objectID": "index.html#model-checking-and-model-comparisons",
    "href": "index.html#model-checking-and-model-comparisons",
    "title": "Week 8: Generalised Linear Models part 2",
    "section": "\n3.4 Model checking and model comparisons",
    "text": "3.4 Model checking and model comparisons\nSummary statistics can be used to assess the adequacy of a model and also to compare models. Some of the statistics we can consider are:\n\nthe deviance \\(D=2[l(\\hat{\\boldsymbol{\\beta}}_{\\max})-l(\\hat{\\boldsymbol{\\beta}})]\\) (also referred to as residual deviance), where \\(l(\\hat{\\boldsymbol{\\beta}}_{\\max})\\) is the maximised log-likelihood for the saturated (full) model and \\(l(\\hat{\\boldsymbol{\\beta}})\\) is the maximised log-likelihood for the model of interest;\nthe likelihood ratio statistic, which is equal to the difference between the null deviance (deviance of the model with no predictors included) and the residual deviance for the model of interest;\nthe Akaike information criterion \\(AIC=-2l(\\hat{\\boldsymbol{\\beta}};\\mathbf{y})+2p\\), which equals the maximised log-likelihood of the model of interest plus a penalty term equal to twice the number of parameters in the model. The reason for this is that we can keep adding predictors to the model to improve the log-likelihood, but the cost is increased model complexity. The penalty term attempts to strike a balance between model complexity and how well the model fits.\n\nIf the model fits well, the deviance will be asymptotically \\(\\chi^2(N-p)\\), where \\(N\\) is \\(J-1\\) times the number of distinct covariate patterns in the data, and \\(p\\) is the number of parameters estimated.\nThe likelihood ratio statistic will be asymptotically \\(\\chi^2[p-(J-1)]\\) because the null (minimal) model will have one parameter for each logit defined in Equation 2.\nThe AIC can be used for model selection: calculate the criterion for each model and choose the one with the smallest value of the AIC.\nWe can compare the nominal logistic regression model with additive terms for age and sex with the null model by taking the difference in deviances (likelihood ratio test).\nThe null model can be fit as follows:\n\nmodel_null&lt;- multinom(response ~ 1, data=dcars, weights=frequency)\n\n# weights:  6 (2 variable)\ninitial  value 329.583687 \nfinal  value 329.272024 \nconverged\n\n\nTo see some model comparisson metrics we can use the glance() function from the broom package:\n\nglance(model_null)\n\n# A tibble: 1 × 4\n    edf deviance   AIC  nobs\n  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1     2     659.  663.    18\n\nglance(model_cars)\n\n# A tibble: 1 × 4\n    edf deviance   AIC  nobs\n  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1     8     581.  597.    18\n\n\nThe difference in deviance is \\(658.54-580.70=77.84\\) which is significant when compared with a \\(\\chi^2(8-2)\\):\n\nqchisq(df=6, p=0.95)\n\n[1] 12.59159\n\n# pval\npchisq(77.844,df=6,lower.tail = F)\n\n[1] 9.955365e-15\n\n\nAlternatively, we can run the likelihood ratio test using the anova function as follows:\n\nanova(model_null,model_cars,test = \"Chisq\")\n\nLikelihood ratio tests of Multinomial Models\n\nResponse: response\n      Model Resid. df Resid. Dev   Test    Df LR stat.      Pr(Chi)\n1         1        34   658.5440                                   \n2 age + sex        28   580.7022 1 vs 2     6 77.84185 9.992007e-15\n\n\nOverall, the explanatory variables are descriptive of car preferences.\nWe can also compare this model with the saturated (full) model, which includes an interaction between age and sex:\n\nmodel_full &lt;- multinom(response ~ age * sex, weight = frequency, data = dcars)\n\n# weights:  21 (12 variable)\ninitial  value 329.583687 \niter  10 value 288.541004\nfinal  value 288.381742 \nconverged\n\nglance(model_full)\n\n# A tibble: 1 × 4\n    edf deviance   AIC  nobs\n  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1    12     577.  601.    18\n\n\nThe difference in deviance between the additive and the saturated model is \\(580.70-576.76=3.94\\). This is not significant when compared with a \\(\\chi^2(12-8)\\), so the additive model appears to fit the data well.\n\nqchisq(df=4, p=0.95)\n\n[1] 9.487729\n\n# pval\npchisq(3.94,df=4,lower.tail = F)\n\n[1] 0.4141869\n\n\nAgain, we can use the anova function to compare the three nested models:\n\nanova(model_null,model_cars,model_full,test = \"Chisq\") \n\nLikelihood ratio tests of Multinomial Models\n\nResponse: response\n      Model Resid. df Resid. Dev   Test    Df  LR stat.      Pr(Chi)\n1         1        34   658.5440                                    \n2 age + sex        28   580.7022 1 vs 2     6 77.841851 9.992007e-15\n3 age * sex        24   576.7635 2 vs 3     4  3.938713 4.143637e-01\n\n\nThe same conclusion is supported when comparing the AIC for these models:\n\nlibrary(performance)\ncompare_performance(model_full,model_cars,model_null,metrics = \"AIC\")\n\n\n\n\n\nName\nModel\nAIC\nAIC_wt\n\n\n\nmodel_full\nmultinom\n600.76\n0.12\n\n\nmodel_cars\nmultinom\n596.70\n0.88\n\n\nmodel_null\nmultinom\n662.54\n0.00\n\n\n\n\n\nthe additive model has a smaller AIC of 596.7 compared to the interaction model which has AIC of 600.7.\n\n\n\n\n\n\n Task\n\n\n\nThe iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of three species of iris: setosa, versicolor, and virginica. Suppose that we have these measurements from an iris and we wish to classify it into one of the three species.\nFit a nominal logistic regression model to the data and predict the probability of each species from the fitted model using sepal length as the only predictor.\nNote that you can use the predicted probabilities from a nominal logistic regression model to classify an observation to the category with the highest predicted probability.\n\n\nTake hint\n\nWe can get a list which assigns each observation to the category with the highest predicted probability using predict(your_model).\n\n\n\nSee solution\n\n\nm.iris &lt;- multinom(Species ~ Sepal.Length, data=iris)\n\n# weights:  9 (4 variable)\ninitial  value 164.791843 \niter  10 value 91.337114\niter  20 value 91.035008\nfinal  value 91.033971 \nconverged\n\ncbind(iris,predict(m.iris)) %&gt;% head()\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species predict(m.iris)\n1          5.1         3.5          1.4         0.2  setosa          setosa\n2          4.9         3.0          1.4         0.2  setosa          setosa\n3          4.7         3.2          1.3         0.2  setosa          setosa\n4          4.6         3.1          1.5         0.2  setosa          setosa\n5          5.0         3.6          1.4         0.2  setosa          setosa\n6          5.4         3.9          1.7         0.4  setosa          setosa\n\n\n\n\n\nIn the car preferences example there was a natural ordering among the response categories that we have not accounted for. This ordering can be taken into account in the model specification. Unfortunately, we don’t have time to cover all of this in a single session. But please have a look at the extra material where we introduce the basis of ordinal logistic regression for ordered categorical responses."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Week 8: Extra material",
    "section": "",
    "text": "In the car preferences example there was a natural ordering among the response categories for the importance of power steering and air conditioning when buying a car: “no/little importance”, “important”, “very important”. This ordering can be taken into account in the model specification. Such ordering often arises in market research, opinion polls and questionnaires (e.g. student feedback at the University of Glasgow).\n\nSometimes an ordinal response could arise if there is a continuous variable \\(Z\\), such as severity of disease, which is hard to measure. \\(Z\\) is a latent variable, because it cannot be observed directly. Instead, cutpoints \\(C_j\\) are identified so that, for instance, patients have “no disease”, “mild disease”, “moderate disease” or “severe disease” corresponding to values of \\(Z\\) from low to high. \\(C_1, \\dots, C_{J-1}\\) identify \\(J\\) ordered categories with associated probabilities \\(p_1,p_2,\\dots, p_J\\). An example of the continuous distribution of \\(Z\\) with cutpoints for four categories is shown below. Here, four discrete responses can occur depending on the position of \\(Z\\) relative to the cutpoints \\(C_j\\).\n\n\n\n\n\n\n\n\n\nThere are several ways in which to model logits involving the probabilities \\(p_j\\). The most commonly used model is the proportional odds logistic regression model. If the linear predictor \\(\\boldsymbol{x}^\\intercal \\boldsymbol{\\beta}_j\\) has an intercept term \\(\\beta_{0j}\\) which depends on category \\(j\\), but the other explanatory variables do not depend on \\(j\\), then the model is\n\\[\n\\log \\left(\\frac{p_1+p_2+\\dots+p_j}{p_{j+1}+\\dots+p_J}\\right)=\\beta_{0j}+\\beta_1 x_1 + \\dots + \\beta_{p-1}x_{p-1}.\n\\]\nThis is called the proportional odds model and is based on the assumption that the effects of the covariates \\(x_1,\\dots, x_{p-1}\\) are the same for all categories on the logarithmic scale, as illustrated in the figure below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSome alternatives to the proportional odds model for ordinal responses are given below.\n\nCumulative logit model\n\nThe cumulative odds for the \\(j\\)th category are \\[\\frac{\\Pr(Z\\leq C_j)}{\\Pr(Z&gt;C_j)}=\\frac{p_1+p_2+\\dots+p_j}{p_{j+1}+\\dots+p_J}\\] The cumulative logit model is \\[\\log \\left(\\frac{p_1+p_2+\\dots+p_j}{p_{j+1}+\\dots+p_J}\\right)=\\boldsymbol{x}^\\intercal \\boldsymbol{\\beta}_j.\\]\n\nAdjacent categories logit model\n\nIf we consider ratios of probabilities, e.g. \\(\\frac{p_1}{p_2}, \\frac{p_2}{p_3},\\dots, \\frac{p_{J-1}}{p_J}\\) we can define the adjacent category logit model as\n\\[\n\\log \\left(\\frac{p_j}{p_{j+1}}\\right)=\\boldsymbol{x}^\\intercal \\boldsymbol{\\beta}_j, \\hspace{1cm} \\text{for } j=1,\\dots,J-1.\n\\]\nIf this is simplified to\n\\[\n\\log \\left(\\frac{p_j}{p_{j+1}}\\right)=\\beta_{0j}+\\beta_1 x_1 + \\dots + \\beta_{p-1}x_{p-1},\n\\]\nthe effect of each explanatory variable is assumed to be the same for all adjacent pairs of categories.\n\nContinuation ratio logit model\n\nAnother alternative is to consider the ratios of probabilities \\(\\frac{p_1}{p_2}, \\frac{p_1+p_2}{p_3},\\dots, \\frac{p_1+\\dots+p_{J-1}}{p_J}\\) or \\(\\frac{p_1}{p_2+\\dots+p_J}, \\frac{p_2}{p_3+\\dots+p_J},\\dots, \\frac{p_{J-1}}{p_J}\\).\nThe equation\n\\[\n   \\log \\left(\\frac{p_j}{p_{j+1}+\\dots+p_J}\\right)=\\boldsymbol{x}^\\intercal \\boldsymbol{\\beta}_j\n\\]\nmodels the odds of the response being in category \\(j\\), i.e. \\(C_{j-1}&lt;Z\\leq C_j\\) conditional upon \\(Z&gt;C_{j-1}\\).\nFor instance, in the car preferences data example we could estimate the odds of respondents regarding air conditioning and power steering as “unimportant” vs. “important” or “very important” using \\[\\log \\left(\\frac{p_1}{p_2+p_3}\\right).\\]\nSimilarly, the odds of these features being “very important” given that they are “important” or “very important” can be estimated by \\[\\log \\left(\\frac{p_2}{p_3}\\right).\\]"
  },
  {
    "objectID": "about.html#ordinal-logistic-regression",
    "href": "about.html#ordinal-logistic-regression",
    "title": "Week 8: Extra material",
    "section": "",
    "text": "In the car preferences example there was a natural ordering among the response categories for the importance of power steering and air conditioning when buying a car: “no/little importance”, “important”, “very important”. This ordering can be taken into account in the model specification. Such ordering often arises in market research, opinion polls and questionnaires (e.g. student feedback at the University of Glasgow).\n\nSometimes an ordinal response could arise if there is a continuous variable \\(Z\\), such as severity of disease, which is hard to measure. \\(Z\\) is a latent variable, because it cannot be observed directly. Instead, cutpoints \\(C_j\\) are identified so that, for instance, patients have “no disease”, “mild disease”, “moderate disease” or “severe disease” corresponding to values of \\(Z\\) from low to high. \\(C_1, \\dots, C_{J-1}\\) identify \\(J\\) ordered categories with associated probabilities \\(p_1,p_2,\\dots, p_J\\). An example of the continuous distribution of \\(Z\\) with cutpoints for four categories is shown below. Here, four discrete responses can occur depending on the position of \\(Z\\) relative to the cutpoints \\(C_j\\).\n\n\n\n\n\n\n\n\n\nThere are several ways in which to model logits involving the probabilities \\(p_j\\). The most commonly used model is the proportional odds logistic regression model. If the linear predictor \\(\\boldsymbol{x}^\\intercal \\boldsymbol{\\beta}_j\\) has an intercept term \\(\\beta_{0j}\\) which depends on category \\(j\\), but the other explanatory variables do not depend on \\(j\\), then the model is\n\\[\n\\log \\left(\\frac{p_1+p_2+\\dots+p_j}{p_{j+1}+\\dots+p_J}\\right)=\\beta_{0j}+\\beta_1 x_1 + \\dots + \\beta_{p-1}x_{p-1}.\n\\]\nThis is called the proportional odds model and is based on the assumption that the effects of the covariates \\(x_1,\\dots, x_{p-1}\\) are the same for all categories on the logarithmic scale, as illustrated in the figure below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSome alternatives to the proportional odds model for ordinal responses are given below.\n\nCumulative logit model\n\nThe cumulative odds for the \\(j\\)th category are \\[\\frac{\\Pr(Z\\leq C_j)}{\\Pr(Z&gt;C_j)}=\\frac{p_1+p_2+\\dots+p_j}{p_{j+1}+\\dots+p_J}\\] The cumulative logit model is \\[\\log \\left(\\frac{p_1+p_2+\\dots+p_j}{p_{j+1}+\\dots+p_J}\\right)=\\boldsymbol{x}^\\intercal \\boldsymbol{\\beta}_j.\\]\n\nAdjacent categories logit model\n\nIf we consider ratios of probabilities, e.g. \\(\\frac{p_1}{p_2}, \\frac{p_2}{p_3},\\dots, \\frac{p_{J-1}}{p_J}\\) we can define the adjacent category logit model as\n\\[\n\\log \\left(\\frac{p_j}{p_{j+1}}\\right)=\\boldsymbol{x}^\\intercal \\boldsymbol{\\beta}_j, \\hspace{1cm} \\text{for } j=1,\\dots,J-1.\n\\]\nIf this is simplified to\n\\[\n\\log \\left(\\frac{p_j}{p_{j+1}}\\right)=\\beta_{0j}+\\beta_1 x_1 + \\dots + \\beta_{p-1}x_{p-1},\n\\]\nthe effect of each explanatory variable is assumed to be the same for all adjacent pairs of categories.\n\nContinuation ratio logit model\n\nAnother alternative is to consider the ratios of probabilities \\(\\frac{p_1}{p_2}, \\frac{p_1+p_2}{p_3},\\dots, \\frac{p_1+\\dots+p_{J-1}}{p_J}\\) or \\(\\frac{p_1}{p_2+\\dots+p_J}, \\frac{p_2}{p_3+\\dots+p_J},\\dots, \\frac{p_{J-1}}{p_J}\\).\nThe equation\n\\[\n   \\log \\left(\\frac{p_j}{p_{j+1}+\\dots+p_J}\\right)=\\boldsymbol{x}^\\intercal \\boldsymbol{\\beta}_j\n\\]\nmodels the odds of the response being in category \\(j\\), i.e. \\(C_{j-1}&lt;Z\\leq C_j\\) conditional upon \\(Z&gt;C_{j-1}\\).\nFor instance, in the car preferences data example we could estimate the odds of respondents regarding air conditioning and power steering as “unimportant” vs. “important” or “very important” using \\[\\log \\left(\\frac{p_1}{p_2+p_3}\\right).\\]\nSimilarly, the odds of these features being “very important” given that they are “important” or “very important” can be estimated by \\[\\log \\left(\\frac{p_2}{p_3}\\right).\\]"
  },
  {
    "objectID": "about.html#proportional-odds-logistic-regression-model-for-the-car-preference-data",
    "href": "about.html#proportional-odds-logistic-regression-model-for-the-car-preference-data",
    "title": "Week 8: Extra material",
    "section": "\n2 Proportional odds logistic regression model for the car preference data",
    "text": "2 Proportional odds logistic regression model for the car preference data\nLooking at the car preference example again, we can fit the response as an ordinal variable using a proportional odds model of the form:\n\\[\n\\log \\left(\\frac {p_1} {p_2+p_3} \\right)= \\beta_{01} + \\beta_{1}x_1 + \\beta_{2}x_2 +\\beta_{3}x_3\n\\tag{1}\\]\n\\[\n\\log \\left(\\frac {p_1+p_2} {p_3}\\right) = \\beta_{02} + \\beta_{1}x_1 + \\beta_{2}x_2 +\\beta_{3}x_3\n\\tag{2}\\]\nwhere \\(j=1\\) for no/little importance (also referred to as “unimportant”), \\(j=2\\) for important and \\(j=3\\) for very important, \\(x_1 =1\\) for women and 0 for men, \\(x_2 = 1\\) for age 24-40 years and 0 otherwise and \\(x_3 = 1\\) for age \\(&gt; 40\\) and 0 otherwise.\nWe fit this model using the polr() function in library(MASS), which, incidentally, uses the parameterisation\n\\[\n\\log \\left(\\frac {p_1} {p_2+p_3} \\right)= \\beta_{01} - \\beta_{1}x_1 - \\beta_{2}x_2 -\\beta_{3}x_3\n\\tag{3}\\]\n\\[\n\\log \\left(\\frac {p_1+p_2} {p_3}\\right) = \\beta_{02} - \\beta_{1}x_1 - \\beta_{2}x_2 -\\beta_{3}x_3\n\\tag{4}\\]\ninstead of Equation 1 and Equation 2\n\nlibrary(MASS)\nm4 &lt;- polr(response ~ sex + age, data= dcars, weight = frequency, Hess=TRUE)\nsummary(m4)\n\nCall:\npolr(formula = response ~ sex + age, data = dcars, weights = frequency, \n    Hess = TRUE)\n\nCoefficients:\n          Value Std. Error t value\nsexwomen 0.5762     0.2262   2.548\nage24-40 1.1471     0.2776   4.132\nage&gt; 40  2.2325     0.2915   7.659\n\nIntercepts:\n                         Value  Std. Error t value\nno/little|important      0.6198 0.2168     2.8588 \nimportant|very important 2.2312 0.2546     8.7625 \n\nResidual Deviance: 581.2956 \nAIC: 591.2956 \n\n\nThe intercepts correspond to the \\(j\\)th category, so the log odds of considering the features “unimportant” is 0.620 corresponding to a probability of 0.65 for men age 18-23. The log odds of considering the features “unimportant” or “important” is 2.231 corresponding to a probability of 0.903, giving a probability of 0.253 of considering the features “important”. This leaves a probability of 0.097 of considering the features “very important” for men age 18-23. These probabilities are calculated using equations (Equation 3) and (Equation 4) together with \\(p_1+p_2+p_3=1\\). For instance for men age 18-23 (baseline) we get \\[\\hat{p}_1=\\frac{\\exp(\\hat{\\beta}_{01})}{1+\\exp(\\hat{\\beta}_{01})}=\\frac{\\exp(0.6198)}{1+\\exp(0.6198)}=0.650\\] and \\[\\hat{p}_3=\\frac{1}{1+\\exp(\\hat{\\beta}_{02})}=\\frac{1}{1+\\exp(2.2312)}=0.097.\\]\nThe likelihood ratio chi-squared statistic for the proportional odds model is 77.25, and the AIC is 591.3, both very similar to those obtained from the corresponding nominal logistic regression model (77.84 and 596.70 respectively). There is little difference in how well the proportional odds and nominal logistic regression models describe the data.\nMore examples and details on GLMs for nominal and ordinal data can be found in\n\nChapter 5 from Extending linear models with R: generalized linear, mixed effects and nonparametric regression models by Julian Faraway and in\nChapter 6 of Regression: models, methods and applications by Fahrmeir et al.\n\nR examples are also available from UCLA’s Institute for Digital Research and Education:\n\nNominal logistic regression example\nOrdinal logistic regression example"
  }
]