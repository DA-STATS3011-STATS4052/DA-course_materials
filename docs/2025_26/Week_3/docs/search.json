[
  {
    "objectID": "slides/slides.html#overview",
    "href": "slides/slides.html#overview",
    "title": "Data analysis",
    "section": "Overview",
    "text": "Overview\nNow that we are comfortable with visualising and manipulating data in R, we can now proceed onto modelling data.\nThere are many different modelling techniques. However, we will begin with one of the easier to understand and commonly-used approaches,linear regression.\nILO’s for today:\n\n\nConduct appropriate exploratory analysis to explore the relationship between a response and a relevant covariates/explanatory variables.\nFit a simple linear regression model with one numerical or categorical covariate and interpret its output.\nFit a multiple linear regression model with two numerical covariates.\nAssess linear regression model fit using diagnostic plots."
  },
  {
    "objectID": "slides/slides.html#load-libraries",
    "href": "slides/slides.html#load-libraries",
    "title": "Data analysis",
    "section": "Load libraries",
    "text": "Load libraries\nWe shall now load into R all of the libraries we will need for this session. This can be done by typing the following into your R script:\n\nlibrary(ggplot2)     # Data visualization\nlibrary(tidyverse)   # Data wrangling\nlibrary(performance) # Model assessment\nlibrary(skimr)       # Exploratory analysis\nlibrary(sjPlot)      # Plot and tables for linear models\n\n\n\nThe first two libraries, ggplot2 and tidyverse, are used for data wrangling and visualization.\nThe third library (performance) can be used to assess the model fit of linear regression models.\nThe fourth library skimr will be used to produce summary statistics for our data.\nThe sjPlot package will be used to summarise the output of linear models as well as performing some diagnostic plots."
  },
  {
    "objectID": "slides/slides.html#data-sets",
    "href": "slides/slides.html#data-sets",
    "title": "Data analysis",
    "section": "Data sets",
    "text": "Data sets\nTeaching evaluations at the UT Austin\nProfessor evaluation scores feedback obtained from end of semester student evaluations for a sample of 463 courses taught by 94 professors from the University of Texas at Austin.\n\nWe will examine the evaluation scores of the instructors based purely on one numerical variable their beauty score.\nThe goal: Explore the relationship between the average professor evaluation score and the average beauty rating of professor."
  },
  {
    "objectID": "slides/slides.html#data-sets-1",
    "href": "slides/slides.html#data-sets-1",
    "title": "Data analysis",
    "section": "Data sets",
    "text": "Data sets\nGapminder data\nThe Gapminder dataset is a comprehensive collection of global statistics on health, wealth, and development over time, spanning countries and decades.\n\nWe will examine life expectancy data from 2007 of every country on each continent.\nThe goal: Explore whether there are differences in the mean life expectancy across continents"
  },
  {
    "objectID": "slides/slides.html#data-sets-2",
    "href": "slides/slides.html#data-sets-2",
    "title": "Data analysis",
    "section": "Data sets",
    "text": "Data sets\nCredit Card Balance Data\nA simulated data set containing information on credit card balances, demographics, and financial attributes for a sample of individuals.\n\nWe will examine the impact that individuals credit limit and income have on their credit card balance.\nThe goal: Describe the relationship between credit balance and these two explanatory variables."
  },
  {
    "objectID": "slides/slides.html#exploring-our-data",
    "href": "slides/slides.html#exploring-our-data",
    "title": "Data analysis",
    "section": "Exploring our data",
    "text": "Exploring our data\nQuick univariate summaries via the skimr package.\n\n\n\n\nData summary\n\n\nName\nevals.scores\n\n\nNumber of rows\n463\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nscore\n0\n1\n4.17\n0.54\n2.30\n3.80\n4.30\n4.6\n5.00\n▁▁▅▇▇\n\n\nbty_avg\n0\n1\n4.42\n1.53\n1.67\n3.17\n4.33\n5.5\n8.17\n▃▇▇▃▂"
  },
  {
    "objectID": "slides/slides.html#exploring-our-data-1",
    "href": "slides/slides.html#exploring-our-data-1",
    "title": "Data analysis",
    "section": "Exploring our data",
    "text": "Exploring our data\nWhen our response and explanatory variable(s) are numerical we can:\n\nCompute the correlation between them:\n\n\\[\n\\rho (x,y) = \\dfrac{\\sum_{i=1}^n (x_i -\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^n (y_i-\\bar{y})^2}}\n\\]"
  },
  {
    "objectID": "slides/slides.html#exploring-our-data-2",
    "href": "slides/slides.html#exploring-our-data-2",
    "title": "Data analysis",
    "section": "Exploring our data",
    "text": "Exploring our data\nWhen our response and explanatory variable(s) are numerical we can:\n\nCompute the correlation between them:\n\n\\[\n\\rho (x,y) = \\dfrac{\\sum_{i=1}^n (x_i -\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^n (y_i-\\bar{y})^2}}\n\\]\n\nProduce scatterplots of our data"
  },
  {
    "objectID": "slides/slides.html#exploring-our-data-3",
    "href": "slides/slides.html#exploring-our-data-3",
    "title": "Data analysis",
    "section": "Exploring our data",
    "text": "Exploring our data\nWhen our response and explanatory variable(s) are categorical we can:\n\nSummarise the variable of interest based on our categorical variable.\n\n\n\n\n\n\ncontinent\nmedian\nmean\n\n\n\n\nAsia\n72.3960\n70.72848\n\n\nEurope\n78.6085\n77.64860\n\n\nAfrica\n52.9265\n54.80604\n\n\nAmericas\n72.8990\n73.60812\n\n\nOceania\n80.7195\n80.71950"
  },
  {
    "objectID": "slides/slides.html#exploring-our-data-4",
    "href": "slides/slides.html#exploring-our-data-4",
    "title": "Data analysis",
    "section": "Exploring our data",
    "text": "Exploring our data\nWhen our response and explanatory variable(s) are categorical we can:\n\nSummarise the variable of interest based on our categorical variable.\nProduce boxplots to examine the distribution of a numerical outcome variable across different levels of a categorical variable"
  },
  {
    "objectID": "slides/slides.html#simple-linear-regression",
    "href": "slides/slides.html#simple-linear-regression",
    "title": "Data analysis",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nLet the general form of the simple linear model:\n\\[y_i = \\beta_0 + \\beta_1 x_{i,1} + \\epsilon_i,  \\ i = 1,..n \\\\\n\\mathrm{such \\  that } \\ \\epsilon \\sim \\mathrm{Normal}(0,\\sigma^2)\\]\nused to describe the relationship between a response y and a set of variables or predictors x\n\n\\(y_i\\) is the \\(i^{th}\\) observation of the response variable;\n\\(\\alpha\\) is the intercept of the regression line;\n\\(\\beta\\) is the slope of the regression line;\n\\(x_i\\) is the \\(i^{th}\\) observation of the explanatory variable; and\n\\(\\epsilon_i\\) is the \\(i^{th}\\) random component."
  },
  {
    "objectID": "slides/slides.html#fitting-a-simple-linear-regression-in-r",
    "href": "slides/slides.html#fitting-a-simple-linear-regression-in-r",
    "title": "Data analysis",
    "section": "Fitting a Simple Linear Regression in R",
    "text": "Fitting a Simple Linear Regression in R\nModelling the relationship between the professor’s evaluation score and their beauty rating\n\nStatistical modelR-CodePlot\n\n\n\\[\\text{score} = \\alpha + \\beta \\  \\text{beauty} +  \\epsilon ~~\\text{such that}~\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)\\]\n\n\n\nmodel &lt;- lm(score ~ bty_avg, data = evals.scores)\n\n\n\n\nggplot(evals.scores,\n       aes(x = bty_avg, y = score)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "slides/slides.html#interpreting-the-output-of-a-linear-regression-model",
    "href": "slides/slides.html#interpreting-the-output-of-a-linear-regression-model",
    "title": "Data analysis",
    "section": "Interpreting the output of a linear regression model",
    "text": "Interpreting the output of a linear regression model\n\n\n\n\n\n\n\n\n\n \nscore\n\n\nPredictors\nEstimates\np\n\n\n(Intercept)\n3.88\n&lt;0.001\n\n\nbty avg\n0.07\n&lt;0.001\n\n\nObservations\n463\n\n\nR2 / R2 adjusted\n0.035 / 0.033\n\n\n\n\n\n\n\n\n\n\n\\[\\widehat{\\text{score}} = \\widehat{\\alpha} + \\widehat{\\beta} x_i = 3.88 + 0.07 \\cdot \\mathrm{bty\\_avg},\\]\n\n\\(\\widehat{\\alpha} = 3.88\\) is the intercept coefficient and means that, for any instructor with a bty_avg = 0, their average teaching score would be 3.88.\n\\(\\widehat{\\beta} = 0.07\\) is the slope coefficient associated with the exploratory variable bty_avg.\n\nFor every 1 unit increase in bty_avg, there is an associated increase of, on average, 0.06664 units of score."
  },
  {
    "objectID": "slides/slides.html#model-diagnostics",
    "href": "slides/slides.html#model-diagnostics",
    "title": "Data analysis",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\n\nThe deterministic part of the model captures all the non-random structure in the data, i.e. the residuals have mean zero.\nThe scale of the variability of the residuals is constant at all values of the explanatory variables (homoscedasticity).\n\n\nResiduals against fitted values."
  },
  {
    "objectID": "slides/slides.html#model-diagnostics-1",
    "href": "slides/slides.html#model-diagnostics-1",
    "title": "Data analysis",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\n\nThe deterministic part of the model captures all the non-random structure in the data, i.e. the residuals have mean zero.\nThe scale of the variability of the residuals is constant at all values of the explanatory variables (homoscedasticity).\nThe residuals are normally distributed."
  },
  {
    "objectID": "slides/slides.html#model-diagnostics-2",
    "href": "slides/slides.html#model-diagnostics-2",
    "title": "Data analysis",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\n\nThe deterministic part of the model captures all the non-random structure in the data, i.e. the residuals have mean zero.\nThe scale of the variability of the residuals is constant at all values of the explanatory variables (homoscedasticity).\nThe residuals are normally distributed.\nThe residuals are independent.\nThe values of the explanatory variables are recorded without error.\n\nJustified on the basis of the experimental context and are not formally examined."
  },
  {
    "objectID": "slides/slides.html#linear-regression-with-a-categorical-variable",
    "href": "slides/slides.html#linear-regression-with-a-categorical-variable",
    "title": "Data analysis",
    "section": "Linear Regression with a Categorical variable",
    "text": "Linear Regression with a Categorical variable\nHere, we will fit a simple linear regression model where the explanatory variable is categorical, i.e. a factor with two or more levels\n\nStatistical ModelR-Code\n\n\n\\[{\\text{life exp}} = {\\alpha} + {\\beta}_{\\text{Amer}} \\cdot \\mathbb{I}_{\\text{Amer}}(x) + {\\beta}_{\\text{Asia}} \\cdot \\mathbb{I}_{\\text{Asia}}(x) + {\\beta}_{\\text{Euro}} \\cdot \\mathbb{I}_{\\text{Euro}}(x) + {\\beta}_{\\text{Ocean}} \\cdot \\mathbb{I}_{\\text{Ocean}}(x),\\]\n\nthe intercept \\({\\alpha}\\) is the mean life expectancy for our baseline category Africa;\n\\({\\beta}_{\\text{continent}}\\) is the difference in the mean life expectancy of a given continent relative to the baseline category ,i.e., Africa.\n\\(\\mathbb{I}_{\\text{continent}}(x)\\) is an indicator function such that\n\\[\\mathbb{I}_{\\text{continent}}(x)=\\left\\{\n            \\begin{array}{ll}\n              1 ~~~ \\text{if country} ~ x ~ \\text{is in the continent},\\\\\n              0 ~~~ \\text{Otherwise}.\\\\\n            \\end{array}\n          \\right.\\]\n\n\n\n\nglimpse(gapminder2007)\n\nRows: 142\nColumns: 3\n$ country   &lt;fct&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Angola\", \"Argentina\", …\n$ continent &lt;fct&gt; Asia, Europe, Africa, Africa, Americas, Oceania, Europe, Asi…\n$ lifeExp   &lt;dbl&gt; 43.828, 76.423, 72.301, 42.731, 75.320, 81.235, 79.829, 75.6…\n\nlifeExp.model &lt;- lm(lifeExp ~ continent, data = gapminder2007)"
  },
  {
    "objectID": "slides/slides.html#interpretation-of-a-lm-with-a-categorical-variable",
    "href": "slides/slides.html#interpretation-of-a-lm-with-a-categorical-variable",
    "title": "Data analysis",
    "section": "Interpretation of a LM with a Categorical variable",
    "text": "Interpretation of a LM with a Categorical variable\n\n\n\n\n\n\n\n\n\n \nlife Exp\n\n\nPredictors\nEstimates\np\n\n\n(Intercept)\n54.81\n&lt;0.001\n\n\ncontinent [Americas]\n18.80\n&lt;0.001\n\n\ncontinent [Asia]\n15.92\n&lt;0.001\n\n\ncontinent [Europe]\n22.84\n&lt;0.001\n\n\ncontinent [Oceania]\n25.91\n&lt;0.001\n\n\nObservations\n142\n\n\nR2 / R2 adjusted\n0.635 / 0.625\n\n\n\n\n\n\n\n\n\n\nOur regression equation is given as:\n\\[\\widehat{\\text{life exp}} = \\widehat{\\alpha} + \\sum_j^4 \\widehat{\\beta}_{j}  \\mathbb{I}_{\\text{continent}_j}(x)\\]\n\nThe mean life expectancy for Africa is equal to the intercept term \\(\\widehat{\\alpha} = 54.8\\). \\(\\mathbb{I}_{\\text{continent}_j} = 0 ~\\forall~j.\\)"
  },
  {
    "objectID": "slides/slides.html#interpretation-of-linear-regression-with-a-categorical-variable",
    "href": "slides/slides.html#interpretation-of-linear-regression-with-a-categorical-variable",
    "title": "Data analysis",
    "section": "Interpretation of Linear Regression with a Categorical variable",
    "text": "Interpretation of Linear Regression with a Categorical variable\n\n\n\n\n\n\n\n\n\n \nlife Exp\n\n\nPredictors\nEstimates\np\n\n\n(Intercept)\n54.81\n&lt;0.001\n\n\ncontinent [Americas]\n18.80\n&lt;0.001\n\n\ncontinent [Asia]\n15.92\n&lt;0.001\n\n\ncontinent [Europe]\n22.84\n&lt;0.001\n\n\ncontinent [Oceania]\n25.91\n&lt;0.001\n\n\nObservations\n142\n\n\nR2 / R2 adjusted\n0.635 / 0.625\n\n\n\n\n\n\n\n\n\n\nOur regression equation is given as:\n\\[\\widehat{\\text{life exp}} = \\widehat{\\alpha} + \\sum_j^4 \\widehat{\\beta}_{j}  \\mathbb{I}_{\\text{continent}_j}(x)\\]\n\nThe mean life expectancy for the j\\(th\\) continent is given by \\(\\widehat{\\alpha} + \\widehat{\\beta}_j \\cdot \\mathbb{I}_{\\text{continent}_j}(x)\\). E.g.,\n\nthe mean life expectancy for Asia is:\n\n\n\\[\\widehat{\\alpha} + \\widehat{\\beta}_{\\text{Asia}} \\cdot \\mathbb{I}_{\\text{Asia}}(x) = 54.8 + 15.9 \\cdot 1 = 70.7 \\]"
  },
  {
    "objectID": "slides/slides.html#interpretation-of-linear-regression-with-a-categorical-variable-1",
    "href": "slides/slides.html#interpretation-of-linear-regression-with-a-categorical-variable-1",
    "title": "Data analysis",
    "section": "Interpretation of Linear Regression with a Categorical variable",
    "text": "Interpretation of Linear Regression with a Categorical variable\n\n\n\n\n\n\n\n\n\n \nlife Exp\n\n\nPredictors\nEstimates\np\n\n\n(Intercept)\n54.81\n&lt;0.001\n\n\ncontinent [Americas]\n18.80\n&lt;0.001\n\n\ncontinent [Asia]\n15.92\n&lt;0.001\n\n\ncontinent [Europe]\n22.84\n&lt;0.001\n\n\ncontinent [Oceania]\n25.91\n&lt;0.001\n\n\nObservations\n142\n\n\nR2 / R2 adjusted\n0.635 / 0.625\n\n\n\n\n\n\n\n\n\n\nOur regression equation is given as:\n\\[\\widehat{\\text{life exp}} = \\widehat{\\alpha} + \\sum_j^4 \\widehat{\\beta}_{j}  \\mathbb{I}_{\\text{continent}_j}(x)\\]\n\nLooking at \\(\\widehat{\\beta}_{\\text{Asia}}=15.9\\) on its own, we would say that the life expectancy in Asia is on average 15.9 years greater than in Africa."
  },
  {
    "objectID": "slides/slides.html#multiple-regression-with-two-numerical-explanatory-variables",
    "href": "slides/slides.html#multiple-regression-with-two-numerical-explanatory-variables",
    "title": "Data analysis",
    "section": "Multiple regression with two numerical explanatory variables",
    "text": "Multiple regression with two numerical explanatory variables\nLet the general form of the linear model:\n\\[y_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + ... +  \\beta_p x_{i,p} + \\epsilon_i,  \\ i = 1,..n\\]\n\n\\(y_i\\) is our response of the \\(i^{th}\\) observation;\n\\(\\beta_0\\) is the intercept;\n\\(\\beta_1\\) is the coefficient for the first explanatory variable \\(x_1\\);\n\\(\\beta_2\\) is the coefficient for the second explanatory variable \\(x_2\\);\n\\(\\beta_p\\) is the coefficient for the \\(p^{th}\\) explanatory variable \\(x_p\\);\n\\(\\epsilon_i\\) is the \\(i^{th}\\) random error component."
  },
  {
    "objectID": "slides/slides.html#fitting-a-multiple-regression",
    "href": "slides/slides.html#fitting-a-multiple-regression",
    "title": "Data analysis",
    "section": "Fitting a multiple regression",
    "text": "Fitting a multiple regression\nThe multiple regression model we will be fitting to the credit balance data is given as:\n\\[y_i = \\alpha + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i, ~~~~ \\epsilon \\sim N(0, \\sigma^2),\\]\n\nStatistical modelR-Code\n\n\n\\[\\text{score} = \\alpha + \\beta \\  \\text{beauty} +  \\epsilon ~~\\text{such that}~\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)\\]\n\n\\(y_i\\) is the balance of the \\(i^{th}\\) individual;\n\\(\\alpha\\) is the intercept and positions the best-fitting plane in 3D space;\n\\(\\beta_1\\) is the coefficient for the first explanatory variable \\(x_1\\);\n\\(\\beta_2\\) is the coefficient for the second explanatory variable \\(x_2\\); and\n\\(\\epsilon_i\\) is the \\(i^{th}\\) random error component.\n\n\n\n\nBalance.model &lt;- lm(Balance ~ Limit + Income, data = Cred)"
  },
  {
    "objectID": "slides/slides.html#interpreting-multiple-regression-model-output",
    "href": "slides/slides.html#interpreting-multiple-regression-model-output",
    "title": "Data analysis",
    "section": "Interpreting multiple regression model output",
    "text": "Interpreting multiple regression model output\n\n\n\n\n\n\n\n\n\n \nBalance\n\n\nPredictors\nEstimates\np\n\n\n(Intercept)\n-385.18\n&lt;0.001\n\n\nLimit\n0.26\n&lt;0.001\n\n\nIncome\n-7.66\n&lt;0.001\n\n\nObservations\n400\n\n\nR2 / R2 adjusted\n0.871 / 0.870\n\n\n\n\n\n\n\n\n\n\nOur regression equation is given as:\n\nThe intercept represents the credit card balance (Balance) of an individual who has $0 for both credit limit (Limit) and income (Income).\nThe coefficient for credit limit (Limit) tells us that, taking all other variables in the model into account, that there is an associated increase, on average, in credit card balance of $0.26.\nThe coefficient for income (Income) tells us that, taking all other variables in the model into account, that there is an associated decrease, on average, in credit card balance of $7.66."
  },
  {
    "objectID": "slides/slides.html#multiple-regression-model-assessment",
    "href": "slides/slides.html#multiple-regression-model-assessment",
    "title": "Data analysis",
    "section": "Multiple regression model assessment",
    "text": "Multiple regression model assessment\nBesides our usual diagnostic plots to check for: homoscedasticity, linearity,normality, etc."
  },
  {
    "objectID": "slides/slides.html#multiple-regression-model-assessment-1",
    "href": "slides/slides.html#multiple-regression-model-assessment-1",
    "title": "Data analysis",
    "section": "Multiple regression model assessment",
    "text": "Multiple regression model assessment\nBesides our usual diagnostic plots to check for: homoscedasticity, linearity,normality, etc.\nWe need to consider collinearity among our predictors\n\ncor(Cred)\n\n          Balance     Limit    Income\nBalance 1.0000000 0.8616973 0.4636565\nLimit   0.8616973 1.0000000 0.7920883\nIncome  0.4636565 0.7920883 1.0000000\n\n\nA high collinearity value may inflate the parameter uncertainty"
  },
  {
    "objectID": "slides/slides.html#multiple-regression-model-assessment-2",
    "href": "slides/slides.html#multiple-regression-model-assessment-2",
    "title": "Data analysis",
    "section": "Multiple regression model assessment",
    "text": "Multiple regression model assessment\nTo check for colliniearity we can compute the VIF (Variance Inflation Factor)\n\nVIF measures how much the variance of a regression coefficient is inflated due to multicollinearity among predictors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVIF = 1: No collinearity (predictor is uncorrelated with others).\n1 \\(&lt;\\) VIF \\(&lt;\\) 5: Moderate correlation (usually acceptable).\nVIF \\(\\geq\\) 5: High collinearity (may inflate coefficient variance, reducing reliability)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Week 3 Tasks",
    "section": "",
    "text": "Task 1\n\n\n\nExamine the relationship between teaching score and age in the evals data set. What is the value of the correlation coefficient? How would you interpret this verbally? Finally, produce a scatterplot of teaching score and age.\n\n\n\nClick here to see the solution\n\nCodeevals.age &lt;- evals %&gt;%\n  dplyr::select(score, age)\n\ncor(evals.age$score, evals.age$age)\n\n[1] -0.107032\n\nCodeggplot(evals.age, aes(x = age, y = score)) +\n  geom_point() +\n  labs(x = \"Age\", y = \"Teaching Score\",\n       title = \"Relationship between Teaching Score and Age\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task 2\n\n\n\nPerform a formal analysis of the relationship between teaching score and age by fitting a simple linear regression model. Superimpose your best-fitting line onto your scatterplot from the previous Task .\n\n\n\nClick here to see the solution\n\nCodemodel &lt;- lm(score ~ age, data = evals.age)\nmodel\n\n\nCall:\nlm(formula = score ~ age, data = evals.age)\n\nCoefficients:\n(Intercept)          age  \n   4.461932    -0.005938  \n\nCodeggplot(evals.age, aes(x = age, y = score)) +\n  geom_point() +\n  labs(x = \"Age\", y = \"Teaching Score\",\n       title = \"Relationship between Teaching Score and Age\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task 3\n\n\n\nAssess the model assumptions from Task 2 by plotting the residuals diagnostic plots.\n\n\n\nClick here to see the solution\n\nCodecheck_model(model,check=c(\"linearity\",\"homogeneity\",\"qq\",\"normality\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task 4\n\n\n\nPerform the same analysis we did on life expectancy from the gapminder data set in 2007. However, subset the data for the year 1997. Are there any differences in the results across this 10 year period?\n\n\n\nClick here to see the solution\n\nCodegapminder1997 &lt;- gapminder %&gt;%\n  filter(year == 1997) %&gt;%\n  dplyr::select(country, continent, lifeExp)\n\nlifeExp.continent &lt;- gapminder1997 %&gt;%\n  summarize(median = median(lifeExp), mean = mean(lifeExp),.by=continent)\nlifeExp.continent\n\n# A tibble: 5 × 3\n  continent median  mean\n  &lt;fct&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 Asia        70.3  68.0\n2 Europe      76.1  75.5\n3 Africa      52.8  53.6\n4 Americas    72.1  71.2\n5 Oceania     78.2  78.2\n\nCodelifeExp.model &lt;- lm(lifeExp ~ continent, data = gapminder1997)\nlifeExp.model\n\n\nCall:\nlm(formula = lifeExp ~ continent, data = gapminder1997)\n\nCoefficients:\n      (Intercept)  continentAmericas      continentAsia    continentEurope  \n            53.60              17.55              14.42              21.91  \n continentOceania  \n            24.59  \n\n\n\n\n\n\n\n\n\n\n\n Task 5\n\n\n\nReturn to the Credit data set and fit a multiple regression model with Balance as the outcome variable, and Income and Age as the explanatory variables, respectively. Assess the assumptions of the multiple regression model.\n\n\n\nClick here to see the solution\n\nCode# Select variables of interest\nCred &lt;- Credit %&gt;%\n  select(Balance, Income, Age)\n# Explore the data\nCred %&gt;%\n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n400\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nBalance\n0\n1\n520.02\n459.76\n0.00\n68.75\n459.50\n863.00\n1999.00\n▇▅▃▂▁\n\n\nIncome\n0\n1\n45.22\n35.24\n10.35\n21.01\n33.12\n57.47\n186.63\n▇▂▁▁▁\n\n\nAge\n0\n1\n55.67\n17.25\n23.00\n41.75\n56.00\n70.00\n98.00\n▆▇▇▇▁\n\n\n\n\nCode# Correlation between covariates\nCred %&gt;%\n  cor()\n\n            Balance    Income         Age\nBalance 1.000000000 0.4636565 0.001835119\nIncome  0.463656457 1.0000000 0.175338403\nAge     0.001835119 0.1753384 1.000000000\n\nCode# Scatterplot\nggplot(Cred, aes(x = Age, y = Balance)) +\n  geom_point() +\n  labs(x = \"Age (in years)\", y = \"Credit card balance (in $)\",\n       title = \"Relationship between balance and age\") \n\n\n\n\n\n\nCode# Fit the model\nBalance.model &lt;- lm(Balance ~ Age + Income, data = Cred)\n# Model output\ntab_model(Balance.model,show.ci = F)\n\n\n\n\n \nBalance\n\n\nPredictors\nEstimates\np\n\n\n(Intercept)\n359.67\n&lt;0.001\n\n\nAge\n-2.19\n0.069\n\n\nIncome\n6.24\n&lt;0.001\n\n\nObservations\n400\n\n\nR2 / R2 adjusted\n0.221 / 0.218\n\n\n\n\nCode# Check assumptions\ncheck_model(Balance.model,check=c(\"linearity\",\"homogeneity\",\"qq\",\"normality\"))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression modelling part 1",
    "section": "",
    "text": "Now that we are comfortable with visualising and manipulating data in R, we can now proceed onto modelling data. The key idea behind modelling data is to infer the relationship between an:\n\n\noutcome (or response) variable \\(y\\) and\nan explanatory (or predictor) variable \\(x\\), which can also be referred to as an independent variable or covariate.\n\nModelling can be used for two purposes:\n\nExplanation: For describing the relationship between an outcome variable \\(y\\) and an explanatory variable x, and determining the potential significance of such relationships using quantifiable measures.\nPrediction: for predicting the outcome variable \\(y\\) given information from one or more explanatory variables.\n\nThere are many different modelling techniques. However, we will begin with one of the easier to understand and commonly-used approaches, linear regression. In particular, we will start by looking at simple linear regression, where we only have one explanatory variable.\nNote: Additional information and examples can be found in Chapter 5 of An Introduction to Statistical and Data Science via R.\nYou can download today’s session R script below:\n Download Week 3 R script \n\n\n\n\n\n\nNote\n\n\n\nI recommend opening a new Quarto file and using it to create your own notes as you work through today’s exercises. This will help you practice coding in a structured way, reinforce your learning, and keep a reusable record of your progress."
  },
  {
    "objectID": "index.html#exploratory-data-analysis",
    "href": "index.html#exploratory-data-analysis",
    "title": "Regression modelling part 1",
    "section": "\n3.1 Exploratory data analysis",
    "text": "3.1 Exploratory data analysis\nBefore you ever do any statistical modelling of data, you should always perform an exploratory data analysis of the data. Performing an exploratory data analysis can give us an idea of the distribution of the data, and whether it contains any strange values, such as outliers or missing values. However, more importantly, it is used to inform which statistical model we should fit to the data. An exploratory data analysis may involve:\n\nLooking at the raw values of the data, either by looking at the spreadsheet directly, or using R.\nBy computing various summary statistics, such as the five-number summary, means, and standard deviations.\nPlotting the data using various data visualisation techniques.\n\nLet’s examine the data evals. First we read the data (stored in .csv format) and then we can look at the raw values from evals using the RStudio pop-up spreadsheet viewer using:\n\nCodeevals &lt;- read.csv(\"evals.csv\")\nView(evals)\n\n\nAt the moment we are only really interested in the instructors teaching (score) and beauty (bty_avg) scores, and so we can look at a subset of the data as follows:\n\nCodeevals.scores &lt;- evals %&gt;%\n  dplyr::select(score, bty_avg)\n\n\nThe outcome variable score is a numerical average of the average teaching score based on students’ evaluations between 1 and 5. The explanatory variable bty_avg is the numerical variable of the average beauty score from a panel of six students’ scores between 1 and 10. As both variables are numerical, we can compute summary statistics for them using the skim function from the skimr package as follows:\n\nCodeevals.scores %&gt;%\n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n463\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nscore\n0\n1\n4.17\n0.54\n2.30\n3.80\n4.30\n4.6\n5.00\n▁▁▅▇▇\n\n\nbty_avg\n0\n1\n4.42\n1.53\n1.67\n3.17\n4.33\n5.5\n8.17\n▃▇▇▃▂\n\n\n\n\n\nThis provides us with the following information:\n\n\nmissing: the number of missing values.\n\ncomplete: the number of non-missing values.\n\nn: the total number of observations.\n\nmean: the mean or average.\n\nsd: the standard deviation.\n\np0: the \\(0^{th}\\) percentile: the value at which 0% of values are smaller than it (i.e. the minimum).\n\np25: the \\(25^{th}\\) percentile: the value at which 25% of values are smaller than it (i.e. the 1st quartile).\n\np50: the \\(50^{th}\\) percentile: the value at which 50% of values are smaller than it (i.e. the median).\n\np75: the \\(75^{th}\\) percentile: the value at which 75% of values are smaller than it (i.e. the 3rd quartile).\n\np100: the \\(100^{th}\\) percentile: the value at which 100% of values are smaller than it (i.e. the maximum).\n\nhist: provides a snapshot of a histogram of the variable.\n\nThese summary statistics give us an idea of how both variables are distributed. For example, the mean teaching score (score) is 4.17 out 5, while the mean beauty score (bty_avg) is 4.42 out of 10. Also, the middle 50% of the data for score lies between 3.8 and 4.6, while the middle 50% of bty_avg lies between 3.17 and 5.5."
  },
  {
    "objectID": "index.html#correlation",
    "href": "index.html#correlation",
    "title": "Regression modelling part 1",
    "section": "\n3.2 Correlation",
    "text": "3.2 Correlation\nThe above summary statistics provide information about each variable separately. However, we are interested in a potential relationship between the two variables and as such it would be of interest to evaluate some statistic that considers both variables simultaneously. One such statistic is the correlation, which ranges between -1 and 1 and describes the strength of the linear relationship between two numerical variables, such that\n\n-1 indicates a perfect negative relationship. That is, as the values of one variable increase, the values of the other decrease.\n0 indicates no relationship. The values of both variables increase/decrease independently of one another.\n1 indicates a perfect positive relationship. That is, the values of both variables increase simultaneously.\n\nThe correlation coefficient \\(\\rho(\\cdot)\\) between two variables \\(x\\) and \\(y\\) can be computed as:\n\\[\n\\rho (x,y) = \\dfrac{\\sum_{i=1}^n (x_i -\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^n (y_i-\\bar{y})^2}}\n\\]\nHere, \\(\\bar{x}\\) and \\(\\bar{y}\\) denotes the mean of \\(x_i\\) and \\(y_i\\) respectively across \\(i= 1,\\ldots,n\\) observations.The plot below displays scatterplots for hypothetical numerical variables \\(x\\) and \\(y\\) simulated to have different levels of correlation.\n\n\n\n\nDiffering levels of correlation between variables.\n\n\n\nThe correlation coefficient can be computed in R using the cor function as follows:\n\nCodecor(evals.scores$score,evals.scores$bty_avg)\n\n[1] 0.1871424\n\n\nHere, we are given a correlation coefficient of 0.187 for the relationship between teaching (score) and beauty (bty_avg) scores. This suggests a rather weakly positive linear relationship between the two variables. There is some subjective interpretation surrounding correlation coefficients not very close to -1, 0, 1. The table below provides a rough guide as to the verbal interpretation of a correlation coefficient.\n\n\n\n\n\n\nCorrelation coefficient\nVerbal interpretation\n\n\n\n0.90 to 1.00 (-0.90 to -1.00)\nVery strong positive (negative) correlation\n\n\n0.70 to 0.90 (-0.70 to -0.90)\nStrong positive (negative) correlation\n\n\n0.50 to 0.70 (-0.50 to -0.70)\nModerate positive (negative) correlation\n\n\n0.30 to 0.50 (-0.30 to -0.50)\nWeak positive (negative) correlation\n\n\n0.00 to 0.30 (0.00 to -0.30)\nVery weak positive (negative) correlation\n\n\n\nThe next step in our exploratory data analysis is to visualise the data using appropriate plotting techniques. Here, a scatterplot is appropriate since both score and bty_avg are numerical variables:\n\nCodeggplot(evals.scores, aes(x = bty_avg, y = score)) +\n  geom_point() +\n  labs(x = \"Beauty Score\", y = \"Teaching Score\", title = \"Relationship of teaching and beauty scores\")\n\n\n\nRelationship between teaching and beauty scores.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe outcome variable should always be plotted on the y-axis.\n\n\nWhat can we observe from the scatterplot? Well, here it can be hard to see the weakly positive linear relationship suggested by the correlation coefficient (0.187), which is why our correlation coefficient is considered very weak in the verbal interpretation.\nAdditionally, as our numerical variables are averages of integers (or whole numbers), a lot of the values will be plotted on top of one another. This is often referred to as over-plotting, and can be alleviated by slightly nudging (jittering) the points in a random direction (This can be done by adding ageom_jitter() layer in our ggplot object). For example, let’s look at the three points in the top-right of the scatterplot that have a beauty score slightly less than 8. Are there really only three values plotted there, or are there more that we cannot see due to over-plotting? Let’s find out by adding some jitter to the plot:\n\n\n\n\nComparing regular and jittered scatterplots.\n\n\n\nFrom the jittered scatterplot we can see that:\n\nThere are actually more than just three points plotted in the top-right; and\nThere are more instructors with a beauty score between 3 and 4.5 than originally appears due to over-plotting.\n\n\n\n\n\n\n\nNote\n\n\n\nJittering does not actually change the values within a data set, it is merely a tool for visualisation purposes. Hence, we shall continue on with plotting the original data."
  },
  {
    "objectID": "index.html#formal-analysis",
    "href": "index.html#formal-analysis",
    "title": "Regression modelling part 1",
    "section": "\n3.3 Formal analysis",
    "text": "3.3 Formal analysis\nAfter completing an exploratory data analysis the next step is to perform a formal analysis on the data. This involves constructing an appropriate statistical model from the information gathered during the exploratory data analysis step. Here, we shall be fitting a simple linear regression model to the data on teaching and beauty scores, where our objective is to acquire the best fitting regression line. This is done by finding estimates of the intercept (\\(\\alpha\\)) and slope (\\(\\beta\\)) which give us the best-fitting line to the data. This can be done in R using the lm function:\n\nCodemodel &lt;- lm(score ~ bty_avg, data = evals.scores)\ntab_model(model,show.ci = F)\n\n\n\n\n \nscore\n\n\nPredictors\nEstimates\np\n\n\n(Intercept)\n3.88\n&lt;0.001\n\n\nbty avg\n0.07\n&lt;0.001\n\n\nObservations\n463\n\n\nR2 / R2 adjusted\n0.035 / 0.033\n\n\n\n\n\nWe have summarised the fitted model using the tab_model function from the sjPlot library (we have omitted confidence intervals for now). This tells us that our best-fitting line to the data is:\n\\[\\widehat{\\text{score}} = \\widehat{\\alpha} + \\widehat{\\beta} x_i = 3.88 + 0.07 \\cdot \\mathrm{bty\\_avg},\\] where\n\n\n\\(\\widehat{\\alpha} = 3.88\\) is the intercept coefficient and means that, for any instructor with a bty_avg = 0, their average teaching score would be 3.88. Note that bty_avg = 0 is not actually possible as bty_avg is an average of beauty scores ranging between 1 and 10.\n\n\\(\\widehat{\\beta} = 0.07\\) is the slope coefficient associated with the exploratory variable bty_avg, and summarises the relationship between score and bty_avg. That is, as bty_avg increases, so does score, such that\n\nFor every 1 unit increase in bty_avg, there is an associated increase of, on average, 0.06664 units of score.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe broom library is another alternative to sjPlot that allow us to visualize linear model output in a tidy way. For example, we can ask for a model summary using the tidy function that will return the output in a data.frame format:\n\nCodelibrary(broom)\ntidy(model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   3.88      0.0761     51.0  1.56e-191\n2 bty_avg       0.0666    0.0163      4.09 5.08e-  5\n\n\n\n\nFinally, we can superimpose our best-fitting line onto our scatterplot to see how it fits through the points using the geom_smooth function as follows:\n\nCodeggplot(evals.scores, aes(x = bty_avg, y = score)) +\n  geom_point() +\n  labs(x = \"Beauty Score\", y = \"Teaching Score\", \n       title = \"Relationship of teaching and beauty scores\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\nRelationship between teaching and beauty scores with regression line superimposed.\n\n\n\nNow that we have fitted our simple linear regression model to the data, how do we use it to obtain information on individual data points? This can be done by looking at the fitted values. For example, let’s say we are interested in looking at the 21st instructor who has the following teaching and beauty scores:\n\n\nscore\nbty_avg\n\n\n4.9\n7.33\n\n\nWhat would the score be on our best-fitting line for this instructor with a bty_avg of 7.33? We simply plug the instructor’s bty_avg into our regression model:\n\\[\\widehat{\\mbox{score}} = 3.88034 + 0.06664 \\cdot \\mbox{bty\\_avg} = 3.88034 + 0.06664 \\cdot 7.33 = 4.369,\\] The regression model gives our instructor a score of 4.369. However, we know the score of the instructor is 4.9 meaning that our model was out by 0.531. This is known as the residual (\\(\\epsilon\\)) and can be thought of as the error or lack of fit of the regression line. In this case, the residual is given by:\n\\[ \\widehat{\\epsilon} = y - \\widehat{y} = 4.9 - 4.369 = 0.531.\\]This is essentially the distance between the fitted regression line and the observed (true) value. This can be seen on the following scatterplot:\n\n\n\n\nExample of observed value, fitted value, and residual.\n\n\n\nwhere\n\nthe red circle is the observed (true) score (\\(y=4.9\\)) of the instructor;\nthe red square is the fitted value (\\(\\widehat{y} = 4.369\\)) from the regression line; and\nthe blue arrow is the distance between the observed and fitted values, that is, the residual.\n\nResiduals and fitted values can also be obtained directly from the fitted model by typing model$fitted.values and model$residuals respectively. We can appended these values to our original data using the mutate function as follows:\n\nCode model_output &lt;- evals.scores %&gt;% \n   mutate(score_hat  = model$fitted.values,\n          residuals  = model$residuals)\n model_output %&gt;% slice(1:6)\n\n  score bty_avg score_hat  residuals\n1   4.7       5  4.213523  0.4864769\n2   4.1       5  4.213523 -0.1135231\n3   3.9       5  4.213523 -0.3135231\n4   4.8       5  4.213523  0.5864769\n5   4.6       3  4.080249  0.5197509\n6   4.3       3  4.080249  0.2197509"
  },
  {
    "objectID": "index.html#assessing-model-fit",
    "href": "index.html#assessing-model-fit",
    "title": "Regression modelling part 1",
    "section": "\n3.4 Assessing model fit",
    "text": "3.4 Assessing model fit\nWhen we fit a simple linear regression model there are five main assumptions that we need to hold true in order for the model to be an appropriate fit to the data. These assumptions are:\n\nThe deterministic part of the model captures all the non-random structure in the data, i.e. the residuals have mean zero.\nThe scale of the variability of the residuals is constant at all values of the explanatory variables (homoscedasticity).\nThe residuals are normally distributed.\nThe residuals are independent.\nThe values of the explanatory variables are recorded without error.\n\nOne way we can check our first assumption is to plot the residuals (residuals) against the explanatory variable (bty_avg). From this we should be able to check that the explanatory variable has a linear relationship with the outcome variable (score). We can plot the residuals against our explanatory variable using:\n\nCodeggplot(model_output, aes(x = bty_avg, y = residuals)) +\n  geom_point() +\n  labs(x = \"Beauty Score\", y = \"Residual\") +\n  geom_hline(yintercept = 0, col = \"blue\", linewidth = 1)\n\n\n\nResiduals against beauty score.\n\n\n\nIdeally, for the first assumption to hold we should observe the following:\n\nThere should be no systematic pattern, i.e. the residuals should appear randomly scattered.\nThe residuals should have mean zero. That is, they should be evenly scattered above and below the zero line. This is because the regression model will overestimate some of the fitted values, but it will also underestimate some, and hence, on average, they should even out to have mean zero.\n\nAnother way in which we can examine our first two assumptions is by plotting the residuals against the fitted values.\nThe performance package allows us to do so via the check_model() function as follows:\n\nCodecheck_model(model,check = c(\"homogeneity\",\"linearity\"))\n\n\n\nResiduals against fitted values.\n\n\n\nHere we have asked to check_model function to check the homoscedasticityand linearity assumptions by setting check =  c(\"homogeneity\",\"linearity\") . From the plot of the residuals against the fitted values we want to examine whether:\n\nThe residuals have mean zero (left plot).\nIf the residuals have constant variance across all levels of the fitted values. That is, the range (or spread) of the residuals should be similar across all levels of the fitted values and display no obvious changes in variability (right plot).\n\nThese two assumptions seems to hold for our model. To assess our third assumption that the residuals are normally distributed we can simply plot a histogram of the residuals and compare the theoretical quantiles of the normal distribution against the sample quantiles.\n\nCodecheck_model(model,check = c(\"normality\",\"qq\"))\n\n\n\nQQ-plot and Histogram of residuals.\n\n\n\nIdeally, for the assumption of normally distributed residuals, the histogram should be bell-shaped and centred at zero, i.e. the residuals have mean zero. However, in practice this will almost never be the case, and as such, like the plots of the residuals, there is some subjectivity in whether you believe the assumptions hold. For instance, here we can see that the histogram is slightly skewed to the left in that the distribution has a longer tail to the left. We can also assess these assumption by comparing the theoretical quantiles of the normal distribution against the sample quantiles. If the normality assumption holds, then we expect that most of the dots should fall along the line. However this seems not to be the case and while the histogram appears to be relatively symmetrical and bell-shaped, the assumption of normally distributed random errors seems dubious.\nNote that we can get similar diagnostic plots using the plot_model function from the sjPlot package by adding the type='diag' option. This will produce a list of plots that can be accessed as follows:\n\nCodediag_plots = plot_model(model,type='diag')\n\n# QQ plot\ndiag_plots[[1]]\n\n\n\n\n\n\nCode# Histrogram/density plot\ndiag_plots[[2]]\n\n\n\n\n\n\nCode# Residuals vs fitted values\ndiag_plots[[3]]\n\n\n\n\n\n\n\nFinally, assumptions 4. and 5. are often justified on the basis of the experimental context and are not formally examined."
  },
  {
    "objectID": "index.html#exploratory-data-analysis-1",
    "href": "index.html#exploratory-data-analysis-1",
    "title": "Regression modelling part 1",
    "section": "\n4.1 Exploratory data analysis",
    "text": "4.1 Exploratory data analysis\nLet’s examine a subset of the gapminder data set relating to the year 2007. That is, we use the filter function to choose only the observations pertaining to 2007, and then select the variables we are interested in (remember to solve any conflicting libraries using the conflicts package or refer to which package each function is obtained from). Lastly we declare the country and continent continent variables as factors using the mutate function:\n\nCodegapminder &lt;- read.csv(\"gapminder.csv\")\n\ngapminder2007 &lt;- gapminder %&gt;%\n  dplyr::filter(year == 2007) %&gt;% \n  dplyr::select(country, continent, lifeExp) %&gt;%\n  mutate(country  = as.factor(country),\n         continent = as.factor(continent))\n\n\nThe new data set can be examined using either the View or glimpse functions, i.e.\n\nCodeglimpse(gapminder2007)\n\nRows: 142\nColumns: 3\n$ country   &lt;fct&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Angola\", \"Argentina\", …\n$ continent &lt;fct&gt; Asia, Europe, Africa, Africa, Americas, Oceania, Europe, Asi…\n$ lifeExp   &lt;dbl&gt; 43.828, 76.423, 72.301, 42.731, 75.320, 81.235, 79.829, 75.6…\n\n\nHere, we can see that both country and continent are factors (fct), which is a way in how R stores categorical variables. Similarly to our previous exploratory data analysis, we can obtain summary statistics using the skim function. First, let’s take a look at the life expectancy (lifeExp) and continent variables:\n\nCodegapminder2007 %&gt;% \n  select(continent, lifeExp) %&gt;% \n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n142\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\ncontinent\n0\n1\nFALSE\n5\nAfr: 52, Asi: 33, Eur: 30, Ame: 25\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\nlifeExp\n0\n1\n67.01\n12.07\n39.61\n57.16\n71.94\n76.41\n82.6\n▂▃▃▆▇\n\n\n\n\nThe summary output for the numerical outcome variable lifeExp is the same as we have seen previously. However, for the categorical variable continent we obtain:\n\n\nn_unique: the number of levels (or categories) of the variable, i.e. the number of continents.\n\ntop_counts: the top counts from the top categories.\n\nordered: whether the variable is ordinal or not. That is, whether or not the ordering of the categories matter.\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is important to always check your data after performing any kind of manipulation. This helps ensure that you have not accidentally overwritten a variable or introduced errors\n\n\nWe can summarise any differences in life expectancy by continent by taking a look at the median and mean life expectancies of each continent using the summarize functions as follows:\n\nCodelifeExp.continent &lt;- gapminder2007 %&gt;%\n  summarize(median = median(lifeExp), mean = mean(lifeExp),.by = continent)\nlifeExp.continent\n\n  continent  median     mean\n1      Asia 72.3960 70.72848\n2    Europe 78.6085 77.64860\n3    Africa 52.9265 54.80604\n4  Americas 72.8990 73.60812\n5   Oceania 80.7195 80.71950\n\n\nBoxplots are often used when examining the distribution of a numerical outcome variable across different levels of a categorical variable:\n\nCodeggplot(gapminder2007, aes(x = continent, y = lifeExp)) +\n  geom_boxplot() +\n  labs(x = \"Continent\", y = \"Life expectancy (years)\", \n       title = \"Life expectancy by continent\")\n\n\n\nLife expectancy by continent in 2007.\n\n\n\nHere, we can see that the middle 50% of the life expectancy distribution of Africa is much smaller than, and does not overlap with, the middle 50% of the remaining four continents, while the country with the highest life expectancy in Africa is less than all countries in Oceania. Speaking of Oceania, there is almost no variability (or spread) in life expectancy in this continent, however that may well be because it consists of only two countries (Australia and New Zealand). There is more variability in life expectancy in the continents of Africa and Asia."
  },
  {
    "objectID": "index.html#formal-analysis-1",
    "href": "index.html#formal-analysis-1",
    "title": "Regression modelling part 1",
    "section": "\n4.2 Formal analysis",
    "text": "4.2 Formal analysis\nWhen examining the relationship between a numerical outcome variable \\(y\\) and a categorical explanatory variable \\(x\\), we are not just looking to find the best-fitting line to the data as before, but are examining relative differences to a baseline category. For example, the table below displays the mean life expectancy of each continent, as well as the differences between the means of each continent and Africa. Now, in comparison with Africa we can see that the mean life expectancy of the other continents is around 18-26 years greater than that of Africa.\n\n\n\n\ncontinent\nmean\nmean vs Africa\n\n\n\nAfrica\n54.81\n0.00\n\n\nAmericas\n73.61\n18.80\n\n\nAsia\n70.73\n15.92\n\n\nEurope\n77.65\n22.84\n\n\nOceania\n80.72\n25.91\n\n\n\n\n\nNow let us fit our regression model to the data, where lifeExp is our outcome variable \\(y\\) and continent is our categorical explanatory variable \\(x\\):\n\nCodelifeExp.model &lt;- lm(lifeExp ~ continent, data = gapminder2007)\ntab_model(lifeExp.model,show.ci = F)\n\n\n\n\n \nlife Exp\n\n\nPredictors\nEstimates\np\n\n\n(Intercept)\n54.81\n&lt;0.001\n\n\ncontinent [Americas]\n18.80\n&lt;0.001\n\n\ncontinent [Asia]\n15.92\n&lt;0.001\n\n\ncontinent [Europe]\n22.84\n&lt;0.001\n\n\ncontinent [Oceania]\n25.91\n&lt;0.001\n\n\nObservations\n142\n\n\nR2 / R2 adjusted\n0.635 / 0.625\n\n\n\n\n\nWe obtain five estimates: the intercept term and four others relating to the continents (continent [Americas], continent [Asia], continent [Europe] and continent [Oceania]), such that our regression equation is given as:\n\\[\\widehat{\\text{life exp}} = \\widehat{\\alpha} + \\widehat{\\beta}_{\\text{Amer}} \\cdot \\mathbb{I}_{\\text{Amer}}(x) + \\widehat{\\beta}_{\\text{Asia}} \\cdot \\mathbb{I}_{\\text{Asia}}(x) + \\widehat{\\beta}_{\\text{Euro}} \\cdot \\mathbb{I}_{\\text{Euro}}(x) + \\widehat{\\beta}_{\\text{Ocean}} \\cdot \\mathbb{I}_{\\text{Ocean}}(x),\\] where\n\nthe intercept \\(\\widehat{\\alpha}\\) is the mean life expectancy for our baseline category Africa;\n\\(\\widehat{\\beta}_{\\text{continent}}\\) is the difference in the mean life expectancy of a given continent relative to the baseline category Africa; and\n\n\\(\\mathbb{I}_{\\text{continent}}(x)\\) is an indicator function such that\n\\[\\mathbb{I}_{\\text{continent}}(x)=\\left\\{\n            \\begin{array}{ll}\n              1 ~~~ \\text{if country} ~ x ~ \\text{is in the continent},\\\\\n              0 ~~~ \\text{Otherwise}.\\\\\n            \\end{array}\n          \\right.\\]\n\n\nEssentially, the estimates for each continent are known as offsets relative to the baseline category (Africa in this case). For example, the mean life expectancy for Africa is simply equal to the intercept term \\(\\widehat{\\alpha} = 54.8\\). However, the mean life expectancy for Asia is:\n\\[\\widehat{\\alpha} + \\widehat{\\beta}_{\\text{Asia}} \\cdot \\mathbb{I}_{\\text{Asia}}(x) = 54.8 + 15.9 \\cdot 1 = 70.7 \\]\nIf we just look at a \\(\\widehat{\\beta}_{\\text{continent}}\\) on their own, then we would interpret these coefficients in relative terms with respect to the baseline category. E.g., looking at \\(\\widehat{\\beta}_{\\text{Asia}}=15.9\\) , we would say that the life expectancy in Asia is on average 15.9 years greater than in Africa."
  },
  {
    "objectID": "index.html#assessing-model-fit-1",
    "href": "index.html#assessing-model-fit-1",
    "title": "Regression modelling part 1",
    "section": "\n4.3 Assessing model fit",
    "text": "4.3 Assessing model fit\nWhat do the fitted values \\(\\widehat{y}\\) and the residuals \\(y - \\widehat{y}\\) correspond to when we are dealing with a categorical explanatory variable? Let’s explore the gapminder2007 data set in order to understand how they work.\n\nCodegapminder2007 %&gt;% slice(1:8)\n\n      country continent lifeExp\n1 Afghanistan      Asia  43.828\n2     Albania    Europe  76.423\n3     Algeria    Africa  72.301\n4      Angola    Africa  42.731\n5   Argentina  Americas  75.320\n6   Australia   Oceania  81.235\n7     Austria    Europe  79.829\n8     Bahrain      Asia  75.635\n\n\nHere, we see the life expectancy of each country and the continent they are from. For example, let’s remember the life expectancies of Afghanistan (43.8) and Bahrain (75.6). Now, we can obtain the fitted values and residuals in the same way we did previously:\n\nCodelifeExp.model_output &lt;- gapminder2007 %&gt;% \n  mutate(lifeExp_hat  = lifeExp.model$fitted.values,\n         residual = lifeExp.model$residuals)\n\nlifeExp.model_output %&gt;% slice(1:10)\n\n\n\n\n\nTable 1: Observed life expectancy values, fitted values and residuals of a linear model fitted to the gapminder2007 data.\n\n\n\n\n\n\n\n\n\nThe first row of the regression table corresponds to the observed life expectancy (lifeExp), fitted value (lifeExp_hat) and the residual error (residual) for Afghanistan. Here, we see that the fitted value (lifeExp_hat = 70.7) is much greater than the life expectancy of Afghanistan (lifeExp = 43.8) with a residual = -26.9. Now, for Bahrain (ID = 8) we also have the same fitted value (lifeExp_hat = 70.7) . This is because the fitted values for each country correspond to the mean life expectancy for that continent (you can use the search box in Table 1 above and type Asia to show only the observation in the Asia continent). Hence, all countries in Africa have the fitted value lifeExp_hat = 70.7, while all countries in Europe have the fitted value lifeExp_hat = 77.6. The residual error in this case is then how much a country deviates from the mean life expectancy of its respective continent.\nFor assessing the assumptions surrounding the residuals for a categorical explanatory variable, we can plot the residuals for each continent:\n\nCodeggplot(lifeExp.model_output, aes(x = continent, y = residual)) +\n  geom_jitter(width = 0.1) + \n  labs(x = \"Continent\", y = \"Residual\") +\n  geom_hline(yintercept = 0, col = \"blue\")\n\n\n\nResiduals over continent.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe have jittered the points for each continent in order to see the residuals for each country more clearly.\n\n\nHere, we see that there is an even spread of the residuals above and below the zero line for each continent, and hence our assumption that the residuals have mean zero appears valid. There is an outlier observed for Asia with a large negative residual (relating to Afghanistan). We could also check this by plotting the residuals agianst the fitted values as follows:\n\nCodecheck_model(lifeExp.model,check=c(\"homogeneity\",\"linearity\"))\n\n\n\nResiduals vs fitted values.\n\n\n\nAgain, our assumption that the residuals have mean zero seems to hold.\n\n\n\n\n\n\n Question\n\n\n\nWhat about the homoscedasticity assumption, is it valid?\n\nyes, residuals are not heteroscedasticno, there is an unbalanced number of residuals per countryyes, residuals show an even spread across countriesno, the spread of the residuals is not even across countries\n\n\n\nTo check that the residual errors are normally distributed, we plot a histogram and a QQplot of them:\n\nCodecheck_model(lifeExp.model,check=c(\"qq\",\"normality\"))\n\n\n\nQQ-plot and Histogram of residuals.\n\n\n\nWhile some of the dots deviate from the flat line in the QQ plot, these deviation occur at the tail where we have less data. Furthermore, the histogram of the residuals are close to a bell-shaped and thus, the assumption of normality seems valid."
  },
  {
    "objectID": "index.html#exploratory-data-analysis-2",
    "href": "index.html#exploratory-data-analysis-2",
    "title": "Regression modelling part 1",
    "section": "\n5.1 Exploratory data analysis",
    "text": "5.1 Exploratory data analysis\n\n\n\n\n\n\n Task\n\n\n\nStart by subsetting the Credit data set so that we only have the variables we are interested in, that is, Balance, Limit and Income. Note, it is best to give your new data set a different name than Credit as to not overwrite the original Credit data set. Define a new data set named Cred containing only the aforementioned variables.\n\n\nTake a hint\n\nYou can use the select function from the dplyr package to select different variables in a data frame.\n\n\n\n\nClick here to see the solution\n\nCodeCred = Credit %&gt;%\n  dplyr::select(c(Balance,Limit,Income))\n\n\n\n\n\nTake a look at summary statistics relating to our newly created data set using the skim function:\n\nCodeCred %&gt;%\n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n400\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nBalance\n0\n1\n520.02\n459.76\n0.00\n68.75\n459.50\n863.00\n1999.00\n▇▅▃▂▁\n\n\nLimit\n0\n1\n4735.60\n2308.20\n855.00\n3088.00\n4622.50\n5872.75\n13913.00\n▆▇▃▁▁\n\n\nIncome\n0\n1\n45.22\n35.24\n10.35\n21.01\n33.12\n57.47\n186.63\n▇▂▁▁▁\n\n\n\n\n\nNow that we are looking at the relationship between an outcome variable and multiple explanatory variables, we need to examine the correlation between each of them. We can examine the correlation between Balance, Limit and Income by creating a table of correlations as follows:\n\nCodeCred %&gt;%\n  cor()\n\n          Balance     Limit    Income\nBalance 1.0000000 0.8616973 0.4636565\nLimit   0.8616973 1.0000000 0.7920883\nIncome  0.4636565 0.7920883 1.0000000\n\n\n\n\n\n\n\n\n Question\n\n\n\nWhy are the diagonal components of our correlation table all equal to 1?\n\nbecause variables have been standardized to have an unit variancebecause they are the correlation of a column with itselfbecause we have a diagonal covariance-variance matrix\n\n\n\nFrom our correlation table we can see that the correlation between our two explanatory variables is 0.792, which is a strong positive linear relationship. Hence, we say there is a high degree of collinearity between our explanatory variables.\nCollinearity (or multicollinearity) occurs when an explanatory variable within a multiple regression model can be linearly predicted from the other explanatory variables with a high level of accuracy. For example, in this case, since Limit and Income are highly correlated, we could take a good guess as to an individual’s Income based on their Limit. That is, having one or more highly correlated explanatory variables within a multiple regression model essentially provides us with redundant information. Normally, we would remove one of the highly correlated explanatory variables, however, for the purpose of this example we shall ignore the potential issue of collinearity and carry on. You may want to use the pairs function or the ggpairs function from the GGally package to look at potential relationships between all of the variables within a data set.\n\n\n\n\n\n\nNote\n\n\n\nWhen we have several potential explanatory variables a model selection technique can help to identify which explanatory variables are significant predictors (in addition to the others) and which variables should be removed from the model. One procedure that can be used is stepwise regression, which implements an automatic procedure for choosing which explanatory variables should be included within the final model. A common stepwise procedure compares models using the model fit criterion Akaike Information Criterion (AIC) and can be implemented in R using the stepAIC function from the MASS library. This procedure allows for forward selection and backward selection (or both), where forward selection starts with the simplest model before iteratively including one explanatory variable at a time until the AIC reaches a minimum. The backward selection approach starts with the most complex model before removing one explanatory variable at a time until the minimium AIC is achieved. We will cover more of this in the next session.\n\n\nLet’s now produce scatterplots of the relationship between the outcome variable and the explanatory variables. First, we shall look at the scatterplot of Balance against Limit:\n\nCodeggplot(Cred, aes(x = Limit, y = Balance)) +\n  geom_point() +\n  labs(x = \"Credit limit (in $)\", y = \"Credit card balance (in $)\", \n       title = \"Relationship between balance and credit limit\") \n\n\n\nRelationship between balance and credit limit.\n\n\n\nNow, let’s look at a scatterplot of Balance and Income:\n\nCodeggplot(Cred, aes(x = Income, y = Balance)) +\n  geom_point() +\n  labs(x = \"Income (in $1000)\", y = \"Credit card balance (in $)\", \n       title = \"Relationship between balance and income\") \n\n\n\nRelationship between balance and income.\n\n\n\nThe two scatterplots above focus on the relationship between the outcome variable Balance and each of the explanatory variables independently. In order to get an idea of the relationship between all three variables we can use the plot_ly function within the plotly library to plot a 3-dimensional scatterplot as follows:\n\nCodelibrary(plotly)\nplot_ly(Cred, x = ~Income, y = ~Limit, z = ~Balance,\n        type = \"scatter3d\", mode = \"markers\")\n\n\n3D scatterplot of balance, credit limit, and income.\n\n\nWhen we fitting our regression model with just a single covariate we looked at the best-fitting line. However, now that we have more than one explanatory variable, we are looking at the best-fitting plane, which is a 3-dimensional generalisation of the best-fitting line."
  },
  {
    "objectID": "index.html#formal-analysis-2",
    "href": "index.html#formal-analysis-2",
    "title": "Regression modelling part 1",
    "section": "\n5.2 Formal analysis",
    "text": "5.2 Formal analysis\nThe multiple regression model we will be fitting to the credit balance data is given as:\n\\[y_i = \\alpha + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i, ~~~~ \\epsilon \\sim N(0, \\sigma^2),\\]\nwhere\n\n\n\\(y_i\\) is the balance of the \\(i^{th}\\) individual;\n\n\\(\\alpha\\) is the intercept and positions the best-fitting plane in 3D space;\n\n\\(\\beta_1\\) is the coefficient for the first explanatory variable \\(x_1\\);\n\n\\(\\beta_2\\) is the coefficient for the second explanatory variable \\(x_2\\); and\n\n\\(\\epsilon_i\\) is the \\(i^{th}\\) random error component.\n\nSimilarly to a simple linear regression, we use the lm function to fit the regression model and the tab_model function to view our parameter estimates:\n\nCodeBalance.model &lt;- lm(Balance ~ Limit + Income, data = Cred)\ntab_model(Balance.model,show.ci = F)\n\n\n\n\n \nBalance\n\n\nPredictors\nEstimates\np\n\n\n(Intercept)\n-385.18\n&lt;0.001\n\n\nLimit\n0.26\n&lt;0.001\n\n\nIncome\n-7.66\n&lt;0.001\n\n\nObservations\n400\n\n\nR2 / R2 adjusted\n0.871 / 0.870\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo include multiple explanatory variables within a regression model we simply use the + sign, that is Balance ~ Limit + Income.\n\n\nHow do we interpret our model estimates defining the regression plane? They can be interpreted as follows:\n\nThe intercept represents the credit card balance (Balance) of an individual who has $0 for both credit limit (Limit) and income (Income). However, the interpretation of the intercept in this case is somewhat limited as there are no individuals with $0 credit limit and income in the data set, with the smallest credit card balance being $0.\nThe coefficient for credit limit (Limit) tells us that, taking all other variables in the model into account, that there is an associated increase, on average, in credit card balance of $0.26.\nSimilarly, the coefficient for income (Income) tells us that, taking all other variables in the model into account, that there is an associated decrease, on average, in credit card balance of $7.66.\n\nWhat do you notice that is strange about our coefficient estimates given our exploratory data analysis? Well, from our scatterplots of credit card balance against both credit limit and income, we seen that there appeared to be a positive linear relationship. Then, why do we then get a negative coefficient for income (-7.66)? This is due to a phenomenon known as Simpson’s Paradox. This occurs when there are trends within different categories (or groups) of data, but that these trends disappear when the categories are grouped as a whole. For more details see Section 6.3.4 of An Introduction to Statistical and Data Sciences in R."
  },
  {
    "objectID": "index.html#assessing-model-fit-for-multiple-regression",
    "href": "index.html#assessing-model-fit-for-multiple-regression",
    "title": "Regression modelling part 1",
    "section": "\n5.3 Assessing model fit for multiple regression",
    "text": "5.3 Assessing model fit for multiple regression\nNow we need to assess our model assumptions. Similarly to simple regression, our model assumptions are:\n\nThe deterministic part of the model captures all the non-random structure in the data, i.e. the residuals have mean zero.\nThe scale of the variability of the residuals is constant at all values of the explanatory variables.\nThe residuals are normally distributed.\nThe residuals are independent.\nThe values of the explanatory variables are recorded without error.\n\nFirst, we need to obtain the fitted values and residuals from our regression model:\n\nCodeBalance.model_output &lt;-  Cred %&gt;% \n  mutate(Balance_hat  = Balance.model$fitted.values,\n         residual = Balance.model$residuals)\n\n\nWe can assess our first two model assumptions by producing scatterplots of our residuals against each of our explanatory variables. First, let’s begin with the scatterplot of the residuals against credit limit:\n\nCodeggplot(Balance.model_output, aes(x = Limit, y = residual)) +\n  geom_point() +\n  labs(x = \"Credit limit (in $)\", y = \"Residual\", title = \"Residuals vs credit limit\")  +\n  geom_hline(yintercept = 0, col = \"blue\", linewidth = 1)\n\n\n\nResiduals vs credit limit.\n\n\n\nNow, let’s plot a scatterplot of the residuals against income:\n\nCodeggplot(Balance.model_output, aes(x = Income, y = residual)) +\n  geom_point() +\n  labs(x = \"Income (in $1000)\", y = \"Residual\", title = \"Residuals vs income\") +\n  geom_hline(yintercept = 0, col = \"blue\", linewidth = 1)\n\n\n\nResiduals vs income.\n\n\n\nAlternatively, we can use the check_model function to produce the standard diagnostic plots for a fitted linear regression model:\n\nCodecheck_model(Balance.model, check= c(\"linearity\",\"homogeneity\",\"qq\",\"normality\"))\n\n\n\nBalance.model Residuals checks.\n\n\n\n\n\n\n\n\n\n Question\n\n\n\nWhich assumptions does each of the following plots address?\n\n\n\n\n\n\n\n\n Task\n\n\n\nUse the plot_model function from the sjPlot package to obtain the same diagnostic plots produced via check_model(Balance.model, check= c(homogeneity\",\"qq\",\"normality\")).\n\n\n\nClick here to see the solution\n\nCodediag_plots = plot_model(Balance.model,type='diag')\n\n# QQ plot\ndiag_plots[[2]]\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCode# Histrogram/density plot\ndiag_plots[[3]]\n\n\n\n\n\n\nCode# Residuals vs fitted values\ndiag_plots[[4]]\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nLastly, besides our usual diagnostic plots to checks we need to be carefull with the collinearity among our predictor since a high collinearity value may inflate the parameter uncertainty.\nTo check for colliniearity we can compute the VIF (Variance Inflation Factor) which measures how well a covariate \\(j\\) can be predicted by all other predictors in the model. It measures how much the variance of a regression coefficient is inflated due to collinearity.\nIt is computed by regressing a predicor \\(x_j\\) against all other predictors in the model and then obtaining the VIF as:\n\\[\nVIF_j = \\frac{1}{1-R^2_j}\n\\] where \\(R^2_j\\) is the coefficient of determination from this auxiliary regression. In general terms we can interpret VIF as follows:\n\nVIF = 1: No multicollinearity (predictor is uncorrelated with others).\n1 \\(&lt;\\) VIF &lt; 5: Moderate correlation (usually acceptable).\nVIF \\(\\geq\\) 5: High multicollinearity (may inflate coefficient variance, reducing reliability).\n\nWe can check for collinearity using the check_model function as follows:\n\nCodecheck_model(Balance.model,check=\"vif\")\n\n\n\n\n\n\n\nAlternatively, the aforementioned plot_model() function from sjPlot also compute this as part of the default diagnostic plots:\n\nCodediag_plots[[1]]\n\n\n\n\n\n\n\nYou can further enhance your data analysis skills by completing the additional tasks. Attempt these tasks first, then compare your solutions with the provided answers."
  }
]