[
  {
    "objectID": "Week4_Solutions.html",
    "href": "Week4_Solutions.html",
    "title": "Week 3 Tasks Solutions",
    "section": "",
    "text": "Tasks\n\nAssess the model assumptions for the parallel regression lines model. Do they appear valid?\n\n\nevals_multiple &lt;- evals %&gt;%\n                  select(score, gender, age)\npar.model &lt;- linear_reg()\npar.model &lt;- par.model |&gt;\n  fit(score ~ age + gender, data = evals_multiple) |&gt;\n  extract_fit_engine()\n\nSolution\n\nregression.points &lt;- get_regression_points(par.model) \n\nggplot(regression.points, aes(x = age, y = residual)) + \n  geom_point() +\n   labs(x = \"age\", y = \"Residual\") +\n   geom_hline(yintercept = 0, col = \"blue\", linewidth = 1) +\n   facet_wrap(~ gender)\n\n\n\n\n\n\n\nggplot(regression.points, aes(x = score_hat, y = residual)) +\n geom_point() +\n   labs(x = \"Fitted values\", y = \"Residual\") +\n   geom_hline(yintercept = 0, col = \"blue\", linewidth = 1) +\n   facet_wrap(~ gender)\n\n\n\n\n\n\n\nggplot(regression.points, aes(x = residual)) +\n   geom_histogram(binwidth = 0.25, color = \"white\") +\n   labs(x = \"Residual\") +\n   facet_wrap(~gender)\n\n\n\n\n\n\n\n\n\nReturn to the Credit data set and fit a multiple regression model with Balance as the outcome variable, and Income and Age as the explanatory variables, respectively. Assess the assumptions of the multiple regression model.\n\nSolution\n\nCred &lt;- Credit |&gt;\n  select(Balance, Income, Age)\n \nCred |&gt;\n   skim()\n\n\nData summary\n\n\nName\nCred\n\n\nNumber of rows\n400\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nBalance\n0\n1\n520.02\n459.76\n0.00\n68.75\n459.50\n863.00\n1999.00\n▇▅▃▂▁\n\n\nIncome\n0\n1\n45.22\n35.24\n10.35\n21.01\n33.12\n57.47\n186.63\n▇▂▁▁▁\n\n\nAge\n0\n1\n55.67\n17.25\n23.00\n41.75\n56.00\n70.00\n98.00\n▆▇▇▇▁\n\n\n\n\nCred |&gt;\n   cor()\n\n            Balance    Income         Age\nBalance 1.000000000 0.4636565 0.001835119\nIncome  0.463656457 1.0000000 0.175338403\nAge     0.001835119 0.1753384 1.000000000\n\nggplot(Cred, aes(x = Age, y = Balance)) +\n   geom_point() +\n   labs(x = \"Age (in years)\", y = \"Credit card balance (in $)\", \n        title = \"Relationship between balance and age\") +\n   geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# plot_ly(Cred, x = ~Income, y = ~Age, z = ~Balance,\n#          type = \"scatter3d\", mode = \"markers\")\n\nBalance.model &lt;- linear_reg()\nBalance.model &lt;- Balance.model |&gt; \n  fit(Balance ~ Age + Income, data = Cred) |&gt;\n  extract_fit_engine()\nget_regression_table(Balance.model)\n\n# A tibble: 3 × 7\n  term      estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept   360.      70.4        5.11   0       221.    498.   \n2 Age          -2.18     1.20      -1.82   0.069    -4.54    0.172\n3 Income        6.24     0.587     10.6    0         5.08    7.39 \n\nregression.points &lt;- get_regression_points(Balance.model)\n \nggplot(regression.points, aes(x = Income, y = residual)) +\n   geom_point() +\n   labs(x = \"Income (in $1000)\", y = \"Residual\", title = \"Residuals vs income\")  +\n   geom_hline(yintercept = 0, col = \"blue\", linewidth = 1)\n\n\n\n\n\n\n\nggplot(regression.points, aes(x = Age, y = residual)) +\n   geom_point() +\n   labs(x = \"Age (in years)\", y = \"Residual\", title = \"Residuals vs age\")  +\n   geom_hline(yintercept = 0, col = \"blue\", linewidth = 1)\n\n\n\n\n\n\n\nggplot(regression.points, aes(x = residual)) +\n   geom_histogram(color = \"white\") +\n   labs(x = \"Residual\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nReturn to the Credit data set and fit a parallel regression lines model with Balance as the outcome variable, and Income and Student as the explanatory variables, respectively. Assess the assumptions of the fitted model.\n\nSolution\n\nCred &lt;- Credit |&gt;\n   select(Balance, Income, Student)\n \nCred |&gt;\n   skim()\n\n\nData summary\n\n\nName\nCred\n\n\nNumber of rows\n400\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nStudent\n0\n1\nFALSE\n2\nNo: 360, Yes: 40\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nBalance\n0\n1\n520.02\n459.76\n0.00\n68.75\n459.50\n863.00\n1999.00\n▇▅▃▂▁\n\n\nIncome\n0\n1\n45.22\n35.24\n10.35\n21.01\n33.12\n57.47\n186.63\n▇▂▁▁▁\n\n\n\n\nggplot(Cred, aes(x = Income, y = Balance, color = Student)) +\n   geom_jitter() +\n   labs(x = \"Income (in $1000)\", y = \"Credit card balance (in $)\", color = \"Student\") +\n   geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\npar.model &lt;- linear_reg()\npar.model &lt;- par.model |&gt; \n   fit(Balance ~ Income + Student, data = Cred) |&gt;\n   extract_fit_engine()\nget_regression_table(par.model)\n\n# A tibble: 3 × 7\n  term         estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept      211.      32.5        6.50       0   147.     275.  \n2 Income           5.98     0.557     10.8        0     4.89     7.08\n3 Student: Yes   383.      65.3        5.86       0   254.     511.  \n\nregression.points &lt;- get_regression_points(par.model)\n \nggplot(regression.points, aes(x = Income, y = residual)) +\n   geom_point() +\n   labs(x = \"Income (in $1000)\", y = \"Residual\") +\n   geom_hline(yintercept = 0, col = \"blue\", linewidth = 1) +\n   facet_wrap(~ Student)\n\n\n\n\n\n\n\nggplot(regression.points, aes(x = Balance_hat, y = residual)) +\n   geom_point() +\n   labs(x = \"Fitted values\", y = \"Residual\") +\n   geom_hline(yintercept = 0, col = \"blue\", linewidth = 1) +\n   facet_wrap(~ Student)\n\n\n\n\n\n\n\nggplot(regression.points, aes(x = residual)) +\n   geom_histogram(color = \"white\") +\n   labs(x = \"Residual\") +\n   facet_wrap(~Student)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nTrickier\n\nLoad the library datasets and look at the iris data set of Edgar Anderson containing measurements (in centimeters) on 150 different flowers across three different species of iris. Fit an interaction model with Sepal.Width as the outcome variable, and Sepal.Length and Species as the explanatory variables. Assess the assumptions of the fitted model.\n\nSolution\n\nIrs &lt;- iris |&gt;\n   select(Sepal.Width, Sepal.Length, Species)\n \nIrs |&gt;\n   skim()\n\n\nData summary\n\n\nName\nIrs\n\n\nNumber of rows\n150\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSpecies\n0\n1\nFALSE\n3\nset: 50, ver: 50, vir: 50\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSepal.Width\n0\n1\n3.06\n0.44\n2.0\n2.8\n3.0\n3.3\n4.4\n▁▆▇▂▁\n\n\nSepal.Length\n0\n1\n5.84\n0.83\n4.3\n5.1\n5.8\n6.4\n7.9\n▆▇▇▅▂\n\n\n\n\nIrs |&gt; \n   get_correlation(formula = Sepal.Width ~ Sepal.Length)\n\n         cor\n1 -0.1175698\n\nggplot(Irs, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n   geom_point() +\n   labs(x = \"Sepal length (in centimetres)\", y = \"Sepal width (in centimetres)\", color = \"Species\") +\n   geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nint.model &lt;- linear_reg()\nint.model &lt;- int.model |&gt; \n  fit(Sepal.Width ~ Sepal.Length * Species, data = Irs) |&gt;\n  extract_fit_engine()\nget_regression_table(int.model)\n\n# A tibble: 6 × 7\n  term                    estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept                 -0.569     0.554     -1.03   0.306   -1.66     0.525\n2 Sepal.Length               0.799     0.11       7.24   0        0.58     1.02 \n3 Species: versicolor        1.44      0.713      2.02   0.045    0.032    2.85 \n4 Species: virginica         2.02      0.686      2.94   0.004    0.66     3.37 \n5 Sepal.Length:Speciesve…   -0.479     0.134     -3.58   0       -0.743   -0.215\n6 Sepal.Length:Speciesvi…   -0.567     0.126     -4.49   0       -0.816   -0.317\n\nregression.points &lt;- get_regression_points(int.model)\n \nggplot(regression.points, aes(x = Sepal.Length, y = residual)) +\n   geom_point() +\n   labs(x = \"Sepal length (in centimetres)\", y = \"Residual\") +\n   geom_hline(yintercept = 0, col = \"blue\", linewidth = 1) +\n   facet_wrap(~ Species)\n\n\n\n\n\n\n\nggplot(regression.points, aes(x = Sepal.Width_hat, y = residual)) +\n   geom_point() +\n   labs(x = \"Fitted values\", y = \"Residual\") +\n   geom_hline(yintercept = 0, col = \"blue\", linewidth = 1) +\n   facet_wrap(~ Species)\n\n\n\n\n\n\n\nggplot(regression.points, aes(x = residual)) +\n   geom_histogram(color = \"white\") +\n   labs(x = \"Residual\") +\n   facet_wrap(~ Species)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nFurther Tasks\nYou are encouraged to complete the following tasks by using Quarto to produce a single document which summarises all your work, i.e. the original questions, your R code, your comments and reflections, etc.\n\nData was collected on the characteristics of homes in the American city of Los Angeles (LA) in 2010 and can be found in the file LAhomes.csv on the Moodle page. The data contain the following variables:\n\n\ncity - the district of LA where the house was located\ntype - either SFR (Single Family Residences) or Condo/Twh (Condominium/Town House)\nbed - the number of bedrooms\nbath - the number of bathrooms\ngarage - the number of car spaces in the garage\nsqft - the floor area of the house (in square feet)\npool - Y if the house has a pool\nspa - TRUE if the house has a spa\nprice - the most recent sales price ($US)\nWe are interested in exploring the relationships between price and the other variables.\nRead the data into an object called LAhomes and answer the following questions.\n\n\nBy looking at the univariate and bivariate distributions on the price and sqft variables below, what would be a sensible way to proceed if we wanted to model this data? What care must be taken if you were to proceed this way?\n\n\nlibrary(gridExtra) # Package to display plots side by side\n\nhist1 &lt;- ggplot(LAhomes, aes(x = price)) +\n          geom_histogram()\n\nhist2 &lt;- ggplot(LAhomes, aes(x = sqft)) +\n          geom_histogram()\n\nhist1log &lt;- ggplot(LAhomes, aes(x = log(price))) +\n             geom_histogram()\n\nhist2log &lt;- ggplot(LAhomes, aes(x = log(sqft))) +\n             geom_histogram()\n\nplot1 &lt;- ggplot(LAhomes, aes(x = sqft, y = price)) +\n          geom_point()\n\nplot2 &lt;- ggplot(LAhomes, aes(x = log(sqft), y = log(price))) +\n          geom_point()\n\ngrid.arrange(hist1, hist2, hist1log, hist2log, plot1, plot2,\n             ncol = 2, nrow = 3)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nSolution Given the highly skewed nature of both price and sqft (as seen in both their histograms and their scatterplot) the use of the log transformation significantly reduces the skewness in the data and makes the scatterplot look more linear. Hence we should model the transformed variables log(price) and log(sqft) instead of the original variables.\nIf these transformations are employed, special care must be taken when interpreting the meaning of estimated model parameters since the data are no longer on the original units and the linear models of the logged variables will have a multiplicative effect, rather than an additive effect, in the original units.\n\n\nFit the simple linear model with log(price) as the response and log(sqft) as the predictor. Display the fitted model on a scatterplot of the data and a confidence interval for the slope parameter in the model and interpret its point and interval estimates.\n\n\nSolution\n\nLAhomes &lt;- mutate(LAhomes,log.price = log(price), log.sqft = log(sqft))\n\nslr.model1 &lt;- linear_reg()\nslr.model1 &lt;- slr.model1 |&gt;\n  fit(log(price) ~ log(sqft), data = LAhomes) |&gt;\n  extract_fit_engine()\n\nggplot(LAhomes,aes(x = log(sqft), y = log(price))) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\ncoeff &lt;- slr.model1 |&gt;\n          coef() \n\nslr.model1.est &lt;- get_regression_table(slr.model1)\nglimpse(slr.model1.est)\n\nRows: 2\nColumns: 7\n$ term      &lt;chr&gt; \"intercept\", \"log(sqft)\"\n$ estimate  &lt;dbl&gt; 2.703, 1.442\n$ std_error &lt;dbl&gt; 0.144, 0.020\n$ statistic &lt;dbl&gt; 18.809, 73.794\n$ p_value   &lt;dbl&gt; 0, 0\n$ lower_ci  &lt;dbl&gt; 2.421, 1.403\n$ upper_ci  &lt;dbl&gt; 2.985, 1.480\n\npercentile_beta_ci &lt;- c(slr.model1.est$lower_ci[2], slr.model1.est$upper_ci[2])\npercentile_beta_ci\n\n[1] 1.403 1.480\n\n\nThe above plot shows the fitted line log(price)=2.7 + 1.44 log(sqft). The point estimate of the slope parameter estimates that for every unit increase in log(sqft) the average log(price) of houses will increase by 1.44 US dollars. Another way of saying this is that each additional 1% of square footage produces an estimate of the average price which is 1.44% higher (i.e. there is a multiplicative effect in the original units).\nNote: you must be careful to avoid causative interpretations. Additional square footage does not necessarily cause the price of a specific house to go up.\nFurthermore, based on the confidence interval, we are 95% confident that the interval from 1.4 up to 1.48 contains the true rate of increase in the average of the logged prices as log(sqft) increase. Because this interval does not contain zero we conclude that the relationship between log(price) and log(sqft) is statistically significant.\n\n\nRepeat the analysis in part b. but with the log of the number of bathrooms (bath) as the single explanatory variable.\n\n\nSolution\n\nLAhomes &lt;- mutate(LAhomes, log.bath = log(bath))\n\nslr.model2 &lt;- linear_reg()\nslr.model2 &lt;- slr.model2 |&gt;\n  fit(log(price) ~ log(bath), data = LAhomes) |&gt;\n  extract_fit_engine()\n\nggplot(LAhomes,aes(x=log(bath),y=log(price)))+\n  geom_point()+\n  geom_smooth(method=lm, se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\ncoeff2 &lt;- slr.model2 |&gt;\n            coef() \n\nslr.model2.est &lt;- get_regression_table(slr.model2)\nglimpse(slr.model2.est)\n\nRows: 2\nColumns: 7\n$ term      &lt;chr&gt; \"intercept\", \"log(bath)\"\n$ estimate  &lt;dbl&gt; 12.231, 1.426\n$ std_error &lt;dbl&gt; 0.028, 0.031\n$ statistic &lt;dbl&gt; 437.232, 46.601\n$ p_value   &lt;dbl&gt; 0, 0\n$ lower_ci  &lt;dbl&gt; 12.176, 1.366\n$ upper_ci  &lt;dbl&gt; 12.286, 1.486\n\npercentile_beta_ci2 &lt;- c(slr.model2.est$lower_ci[2], slr.model2.est$upper_ci[2])\npercentile_beta_ci2\n\n[1] 1.366 1.486\n\n\nThe above plot shows the fitted line log(price)=12.23 + 1.43 log(bath). The point estimate of the slope parameter estimates that for every unit increase in the log of the number of bathrooms in a house the average log of the prices of houses will increase by 1.43 US dollars. Another way of saying this is that each doubling of bathroom produces an estimate of the average price which is 143% higher (i.e. there is a multiplicative effect in the original units).\nBased on the confidence interval, we are 95% confident that the interval from 1.37 up to 1.49 contains the true rate of increase in the average of the logged prices as the log of the number of bathrooms increases. Because this interval does not contain zero we conclude that the relationship between log(price) and log(bath) is statistically significant.\n\n\nFit the multiple linear regression model using the log transform of all the variables price (as the response) and both sqft and bath (as the explanatory variables). Calculate the point and interval estimates of the coefficients of the two predictors separately. Compare their point and interval estimates to those you calculated in parts b. and c. Can you account for the differences?\n\n\nSolution\n\nmlr.model &lt;- linear_reg()\nmlr.model &lt;- mlr.model |&gt;\n  fit(log(price) ~ log(sqft)+log(bath), data = LAhomes) |&gt;\n  extract_fit_engine()\n\ncoeff3 &lt;- mlr.model |&gt;\n            coef() \n\nmlr.model.est &lt;- get_regression_table(mlr.model)\nglimpse(mlr.model.est)\n\nRows: 3\nColumns: 7\n$ term      &lt;chr&gt; \"intercept\", \"log(sqft)\", \"log(bath)\"\n$ estimate  &lt;dbl&gt; 2.514, 1.471, -0.039\n$ std_error &lt;dbl&gt; 0.262, 0.040, 0.045\n$ statistic &lt;dbl&gt; 9.601, 37.221, -0.862\n$ p_value   &lt;dbl&gt; 0.000, 0.000, 0.389\n$ lower_ci  &lt;dbl&gt; 2.000, 1.394, -0.128\n$ upper_ci  &lt;dbl&gt; 3.028, 1.549, 0.050\n\npercentile_beta1_ci &lt;- c(mlr.model.est$lower_ci[2], mlr.model.est$upper_ci[2])\npercentile_beta2_ci &lt;- c(mlr.model.est$lower_ci[3], mlr.model.est$upper_ci[3])\npercentile_beta1_ci\n\n[1] 1.394 1.549\n\npercentile_beta2_ci\n\n[1] -0.128  0.050\n\n\nThe fitted model is log(price)=2.51 + 1.47 log(sqft) - 0.04 log(bath). The first thing we notice is that the parameter associated with (the log of) the number of bathrooms has changed from 1.43 to -0.04. That it, its gone from having a positive relationship with the (log of) house prices in the single explanatory variable model to having a negative relationship when the size of the house was also included in the model. One reason for the switch in sign of the parameter estimate could be that for a house with a given size in (log) square feet, more (log) bathrooms means that less of the (log) square footage is used for bedrooms and other desirable space, thus reflecting a lower average home price. This illustrates the importance of taking all other variables in the model into account and holding them constant when we interpret individual parameter estimates in a multiple linear regression model. (See the “Formal Analysis” section here from Week 6’s lab).\nTurning to the confidence intervals, the model still predicts that the (log) square footage of the home significantly positively affects the average (log) price, since the 95% confidence interval for the log(sqft) parameter is from from 1.39 up to 1.55 which doesn’t contain zero. However, now the number of bathrooms no longer significantly affects the price since the 95% confidence interval for the log(bath) parameter is from from -0.13 up to 0.05 which does contain zero. This suggests that we drop the log(bath) term from the model and return to the simple linear regression model we used in part b.\n\n\nUsing the objective measures for model comparisons, which of the models in parts b., c. and d. would you favour? Is this consistent with your conclusions in part d.?\n\n\nSolution\n\nmodel.comp.values.slr.model1 &lt;- glance(slr.model1)\nmodel.comp.values.slr.model2 &lt;- glance(slr.model2)\nmodel.comp.values.mlr.model &lt;- glance(mlr.model)\n\nModels &lt;- c('SLR(log(sqft))','SLR(log(bath))','MLR') \nbind_rows(model.comp.values.slr.model1, model.comp.values.slr.model2, \n          model.comp.values.mlr.model, .id = \"Model\") |&gt;\n  select(Model,adj.r.squared,AIC,BIC) |&gt;\n  mutate(Model=Models) |&gt;  \n  kable(\n     digits = 2,\n     caption = \"Model comparison values for different models\", \n  )\n\n\nModel comparison values for different models\n\n\nModel\nadj.r.squared\nAIC\nBIC\n\n\n\n\nSLR(log(sqft))\n0.77\n2292.16\n2308.28\n\n\nSLR(log(bath))\n0.58\n3289.80\n3305.92\n\n\nMLR\n0.77\n2293.41\n2314.91\n\n\n\n\n\nThe table lists \\(R_{adj}^2\\), \\(AIC\\) and \\(BIC\\) which can be used to compare the three models. Both the criteria of minimizing \\(AIC\\) and \\(BIC\\) and maximizing \\(R_{adj}^2\\) leads us to prefer the the model\nlog(price)=2.7 + 1.44 log(sqft)\nwhich we first saw in part b. This agrees with our conclusions in part d.\n\n\nYou have been asked to determine the pricing of a New York City (NYC) Italian restaurant’s dinner menu such that it is competitively positioned with other high-end Italian restaurants by analyzing pricing data that have been collected in order to produce a regression model to predict the price of dinner.\n\nData from surveys of customers of 168 Italian restaurants in the target area are available. The data can be found in the file restNYC.csv on the Moodle page. Each row represents one customer survey from Italian restaurants in NYC and includes the key variables:\n\nPrice - price (in $US) of dinner (including a tip and one drink)\nFood - customer rating of the food (from 1 to 30)\nDecor - customer rating of the decor (from 1 to 30)\nService - customer rating of the service (from 1 to 30)\nEast - dummy variable with the value 1 if the restaurant is east of Fifth Avenue, 0 otherwise\n\n\nUse the ggpairs function in the GGally package (see the following code) to generate an informative set of graphical and numerical summaries which illuminate the relationships between pairs of variables. Where do you see the strongest evidence of relationships between price and the potential explanatory variables? Is there evidence of multicollinearity in the data?\n\n\nSolution\n\nlibrary(GGally)\nrestNYC$East &lt;- as.factor(restNYC$East) # East needs to be a factor\nggpairs(restNYC[, 4:8], aes(colour = East, alpha = 0.4)) # Including the `East` factor\n\n\n\n\n\n\n\nggpairs(restNYC[, 4:7], aes(alpha = 0.4)) # Without the `East` factor\n\n\n\n\n\n\n\n\n\nprice shows a moderate to strong correlation with ’Food,ServiceandDecor`.\nThe correlation between Service and Food (0.795) is the strongest evidence of multicollinearity in the data, followed by the correlation between Service and Decor (0.645)\n\n\n\nFit the simple linear model with Price as the response and Service as the predictor and display the fitted model on a scatterplot of the data. Construct a confidence interval for the slope parameter in the model.\nNow fit a multiple regressing model of Price on Service, Food, and Decor. What happens to the significance of Service when additional variables were added to the model?\n\n\nSolution\n\nslr.Service &lt;- linear_reg()\nslr.Service &lt;- slr.Service |&gt; \n  fit(Price ~ Service, data=restNYC) |&gt;\n  extract_fit_engine()\n\nggplot(restNYC,aes(x=Service,y=Price))+\n  geom_point()+\n  geom_smooth(method=lm, se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\ncoeff.Service &lt;- slr.Service |&gt;\n                  coef() \n\nget_regression_table(slr.Service) |&gt;  \n  kable(\n     digits = 2,\n     caption = \"Parameter estimates for MLR model of Price on Service, Food, and Decor\", \n  )\n\n\nParameter estimates for MLR model of Price on Service, Food, and Decor\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n-11.98\n5.11\n-2.34\n0.02\n-22.07\n-1.89\n\n\nService\n2.82\n0.26\n10.76\n0.00\n2.30\n3.34\n\n\n\n\nmlr.Service.Food.Decor &lt;- linear_reg()\nmlr.Service.Food.Decor &lt;- mlr.Service.Food.Decor  |&gt;\n  fit(Price ~ Service + Food + Decor, data=restNYC) |&gt;\n  extract_fit_engine()\n\nget_regression_table(mlr.Service.Food.Decor) |&gt;  \n  kable(\n     digits = 2,\n     caption = \"Parameter estimates for MLR model of Price on Service, Food, and Decor\", \n  )\n\n\nParameter estimates for MLR model of Price on Service, Food, and Decor\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n-24.64\n4.75\n-5.18\n0.00\n-34.03\n-15.26\n\n\nService\n0.14\n0.40\n0.34\n0.73\n-0.65\n0.92\n\n\nFood\n1.56\n0.37\n4.17\n0.00\n0.82\n2.29\n\n\nDecor\n1.85\n0.22\n8.49\n0.00\n1.42\n2.28\n\n\n\n\n\nWhen only Service is included in the model, it appears significant (confidence interval doesn’t contain zero). However, once Food and Decor are added into the model, that is no longer the case as can be seen in the confidence interval table above, which does contain zero.\n\n\nWhat is the correct interpretation of the coefficient on Service in the linear model which regresses Price on Service, Food, and Decor?\n\n\nSolution\nWhen Food and Decor are in the model, Service is not statistically significant, therefore we cannot know whether it has a significant effect on modeling Price."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Week 3: Regression modelling part 2",
    "section": "",
    "text": "Let’s expand upon what we learned last week by revisiting the instructor evaluation data set evals. In Week 3 you were tasked with examining the relationship between teaching score (score) and age (age). Now, let’s also introduce the additional (binary) categorical explanatory variable gender (gender). That is, we we will be examining:\n\nthe teaching score (score) as our outcome variable \\(y\\);\nage (age) as our numerical explanatory variable \\(x_1\\); and\ngender (gender) as our categorical explanatory variable \\(x_2\\).\n\n\nStart by subsetting the evals data set so that we only have the variables we are interested in, that is, score, age and gender. Note, it is best to give your new data set a different name than evals as to not overwrite the original evals data set. Your new data set should look like the one below.\n\n\n\n\n\n\n Question\n\n\n\nYou can also view your data set using the glimpse function, or by opening a spreadsheet view in RStudio using the View function.\nUse the skim function to obtain some summary statistics from our data:\n\n\nAnswer\n\n\nCodeeval.score |&gt;\n  skim()\n\n\nData summary\n\n\nName\neval.score\n\n\nNumber of rows\n463\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n7\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\ngender\n0\n1\nFALSE\n2\nmal: 268, fem: 195\n\n\nethnicity\n0\n1\nFALSE\n2\nnot: 399, min: 64\n\n\nlanguage\n0\n1\nFALSE\n2\neng: 435, non: 28\n\n\nrank\n0\n1\nFALSE\n3\nten: 253, ten: 108, tea: 102\n\n\npic_outfit\n0\n1\nFALSE\n2\nnot: 386, for: 77\n\n\npic_color\n0\n1\nFALSE\n2\ncol: 385, bla: 78\n\n\ncls_level\n0\n1\nFALSE\n2\nupp: 306, low: 157\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1\n232.00\n133.80\n1.00\n116.50\n232.00\n347.5\n463.00\n▇▇▇▇▇\n\n\nprof_ID\n0\n1\n45.15\n27.55\n1.00\n20.00\n43.00\n70.5\n94.00\n▇▇▆▆▆\n\n\nscore\n0\n1\n4.17\n0.54\n2.30\n3.80\n4.30\n4.6\n5.00\n▁▁▅▇▇\n\n\nage\n0\n1\n48.37\n9.80\n29.00\n42.00\n48.00\n57.0\n73.00\n▅▆▇▆▁\n\n\nbty_avg\n0\n1\n4.42\n1.53\n1.67\n3.17\n4.33\n5.5\n8.17\n▃▇▇▃▂\n\n\ncls_did_eval\n0\n1\n36.62\n45.02\n5.00\n15.00\n23.00\n40.0\n380.00\n▇▁▁▁▁\n\n\ncls_students\n0\n1\n55.18\n75.07\n8.00\n19.00\n29.00\n60.0\n581.00\n▇▁▁▁▁\n\n\n\n\n\n\n\n\nNow, let’s compute the correlation coefficient between our outcome variable score and our numerical explanatory variable age:\n\nCodeeval.score |&gt; \n  get_correlation(formula = score ~ age)\n\n# A tibble: 1 × 1\n     cor\n   &lt;dbl&gt;\n1 -0.107\n\n\n\n\n\n\n\n\n Question\n\n\n\nWhy do we not include the categorical variable gender when calculating the correlation?\n\n\nAnswer\n\nThe correlation coefficient only exists between numerical variables, which is why we do not include our categorical variable gender.\n\n\n\nWe can now visualise our data by producing a scatterplot, where seeing as we have the categorical variable gender, we shall plot the points using different colours for each gender:\n\nCodeggplot(eval.score, aes(x = age, y = score, color = gender)) +\n  geom_jitter() +\n  labs(x = \"Age\", y = \"Teaching Score\", color = \"Gender\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n Instructor evaluation scores by age and gender. The points have been jittered.\n\n\n\nNote: The above code has jittered the points, however, this is not necessary and geom_point would suffice. To plot separate points by gender we simply add the color argument to the aes function and pass to it gender.\nFrom the scatterplot we can see that:\n\nThere are very few women over the age of 60 in our data set.\nFrom the plotted regression lines we can see that the lines have different slopes for men and women. That is, the associated effect of increasing age appears to be more severe for women than it does for men, i.e. the teaching score of women drops faster with age.\n\nHere, we shall begin by fitting what is referred to as a parallel regression lines model. This model implies that the slope of relationship between teaching score (score) and age (age) is the same for both males and females, with only the intercept of the regression lines changing. Hence, our parallel regression lines model is given as:\n\\[\\begin{align}\ny_{i} &= \\alpha + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i \\nonumber \\\\\n&= \\alpha + \\beta_{\\mbox{age}} \\cdot \\mbox{age} + \\beta_{\\mbox{male}} \\cdot \\mathbb{I}_{\\mbox{male}}(x) + \\epsilon_i, \\nonumber\n\\end{align}\\]\nwhere\n\n\\(\\alpha\\) is the intercept of the regression line for females;\n\\(\\beta_{\\mbox{age}}\\) is the slope of the regression line for both males and females;\n\\(\\beta_{\\mbox{male}}\\) is the additional term added to \\(\\alpha\\) to get the intercept of the regression line for males; and\n\n\\(\\mathbb{I}_{\\mbox{male}}(x)\\) is an indicator function such that\n\\[\\mathbb{I}_{\\mbox{male}}(x)=\\left\\{\n            \\begin{array}{ll}\n              1 ~~~ \\mbox{If gender} ~ x ~ \\mbox{is male},\\\\\n              0 ~~~ \\mbox{Otherwise}.\\\\\n            \\end{array}\n          \\right.\\]\n\n\nWe can fit the parallel regression lines model as follows:\n\nCodelm_spec &lt;- linear_reg() |&gt; set_engine(\"lm\")\npar.model &lt;- lm_spec |&gt; fit(score ~ age + gender, data = eval.score)\npar.model &lt;- par.model |&gt; extract_fit_engine()\nget_regression_table(par.model)\n\n# A tibble: 3 × 7\n  term         estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept       4.48      0.125     35.8    0        4.24     4.73 \n2 age            -0.009     0.003     -3.28   0.001   -0.014   -0.003\n3 gender: male    0.191     0.052      3.63   0        0.087    0.294\n\n\nHence, the regression line for females is given by:\n\\[\\widehat{\\mbox{score}} = 4.48 - 0.009 \\cdot \\mbox{age},\\]\nwhile the regression line for males is given by:\n\\[\\widehat{\\mbox{score}} = 4.48 - 0.009 \\cdot \\mbox{age} + 0.191 = 4.671 - 0.009 \\cdot \\mbox{age}.\\] Now, let’s superimpose our parallel regression lines onto the scatterplot of teaching score against age:\n\nCodeggplot(eval.score, aes(x = age, y = score, color = gender)) +\n  geom_jitter() +\n  labs(x = \"Age\", y = \"Teaching Score\", color = \"Gender\") +\n  geom_parallel_slopes(se = FALSE)\n\n\n\n Instructor evaluation scores by age and gender with parallel regression lines superimposed.\n\n\n\nFrom the parallel regression lines model both males and females have the same slope, that is, the associated effect of age on teaching score is the same for both men and women. Hence, for every one year increase in age, there is an associated decrease in teaching score of 0.009. However, male instructors have a higher intercept term, that is, there is a vertical bump in the regression line for males in teaching scores. This is linked to the average difference in teaching scores that males obtain relative to females.\n\n\n\n\n\n\n Question\n\n\n\nWhat is different between our previous scatterplot of teaching score against age and the one we just created with our parallel lines superimposed?\n\n\nAnswer\n\nIn the original plot we have what is referred to as an interaction effect between age and gender. Hence, gender interacts in different ways for both males and females by age, and as such we should have different intercepts and slopes.\n\n\n\n\nThere is an interaction effect if the associated effect of one variable depends on the value of another variable. For example, the effect of age here will depend on whether the instructor is male or female, that is, the effect of age on teaching scores will differ by gender. The interaction model can be written as:\n\\[\\begin{align}\ny_{i} &= \\alpha + \\beta_1  x_{1i} + \\beta_2  x_{2i} + \\beta_3  x_{1i}  x_{2i} + \\epsilon_i \\nonumber \\\\\n&= \\alpha + \\beta_{\\mbox{age}} \\cdot \\mbox{age} + \\beta_{\\mbox{male}} \\cdot \\mathbb{I}_{\\mbox{male}}(x) + \\beta_{\\mbox{age, male}} \\cdot \\mbox{age} \\cdot \\mathbb{I}_{\\mbox{male}}(x) + \\epsilon_i, \\nonumber\n\\end{align}\\]\nwhere \\(\\beta_{\\mbox{age, male}} \\cdot \\mbox{age} \\cdot \\mathbb{I}_{\\mbox{male}}(x)\\) corresponds to the interaction term.\nMore concretely:\n\\[y_{i}=\\left\\{\n                \\begin{array}{ll}\n                  \\alpha + \\beta_{\\mbox{age}} \\cdot \\mbox{age} + \\epsilon_i ~~~ \\mbox{If gender} ~ x ~ \\mbox{is female},\\\\\n                  (\\alpha + \\beta_{\\mbox{male}}) + (\\beta_{\\mbox{age}} + \\beta_{\\mbox{age, male}}) \\cdot \\mbox{age} + \\epsilon_i ~~~ \\mbox{Otherwise}.\\\\\n                \\end{array}\n              \\right.\\]\nIn order to fit an interaction term within our regression model we replace the + sign with the * sign as follows:\n\nCodelm_spec &lt;- linear_reg()\nint.model &lt;- lm_spec |&gt; fit(score ~ age * gender, data = eval.score)\nint.model &lt;- int.model |&gt; extract_fit_engine()\nget_regression_table(int.model)\n\n# A tibble: 4 × 7\n  term           estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept         4.88      0.205     23.8    0        4.48     5.29 \n2 age              -0.018     0.004     -3.92   0       -0.026   -0.009\n3 gender: male     -0.446     0.265     -1.68   0.094   -0.968    0.076\n4 age:gendermale    0.014     0.006      2.45   0.015    0.003    0.024\n\n\nHence, the regression line for females is given by:\n\\[\\widehat{\\mbox{score}} = 4.88 - 0.018 \\cdot \\mbox{age},\\] while the regression line for males is given by:\n\\[\\widehat{\\mbox{score}} = 4.88 - 0.018 \\cdot \\mbox{age} - 0.446 + 0.014 \\cdot \\mbox{age} = 4.434 - 0.004 \\cdot \\mbox{age}.\\]\n\n\n\n\n\n\n Question\n\n\n\nHow do they compare with the teaching score values from the parallel regression lines model?\n\n\nAnswer\n\nHere, we can see that, although the intercept for male instructors may be lower, the associated average decrease in teaching score with age (0.004) is not as severe as it is for female instructors (0.018).\n\n\n\n\nNow we have to assess the fit of the model by looking at plots of the residuals. We shall do this for the interaction model. First, we need to obtain the fitted values and residuals from the interaction model as follows:\n\nCoderegression.points &lt;- get_regression_points(int.model)\n\n# A tibble: 463 × 6\n      ID score   age gender score_hat residual\n   &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1     1   4.7    36 female      4.25    0.448\n 2     2   4.1    36 female      4.25   -0.152\n 3     3   3.9    36 female      4.25   -0.352\n 4     4   4.8    36 female      4.25    0.548\n 5     5   4.6    59 male        4.20    0.399\n 6     6   4.3    59 male        4.20    0.099\n 7     7   2.8    59 male        4.20   -1.40 \n 8     8   4.1    51 male        4.23   -0.133\n 9     9   3.4    51 male        4.23   -0.833\n10    10   4.5    40 female      4.18    0.318\n# ℹ 453 more rows\n\n\nLet’s start by looking at a scatterplot of the residuals against the explanatory variable by gender:\n\nCodeggplot(regression.points, aes(x = age, y = residual)) +\n  geom_point() +\n  labs(x = \"age\", y = \"Residual\") +\n  geom_hline(yintercept = 0, col = \"blue\", size = 1) +\n  facet_wrap(~ gender)\n\n\n\nResiduals vs the explanatory variable age by gender.\n\n\n\nNow, we can plot the residuals against the fitted values:\n\nCodeggplot(regression.points, aes(x = score_hat, y = residual)) +\n  geom_point() +\n  labs(x = \"Fitted values\", y = \"Residual\") +\n  geom_hline(yintercept = 0, col = \"blue\", size = 1) +\n  facet_wrap(~ gender)\n\n\n\nResiduals vs the fitted values.\n\n\n\n\nFinally, let’s plot histograms of the residuals to assess whether they are normally distributed with mean zero:\n\nCodeggplot(regression.points, aes(x = residual)) +\n  geom_histogram(binwidth = 0.25, color = \"white\") +\n  labs(x = \"Residual\") +\n  facet_wrap(~gender)\n\n\n\nHistograms of the residuals by gender."
  },
  {
    "objectID": "index.html#exploratory-data-analysis",
    "href": "index.html#exploratory-data-analysis",
    "title": "Week 3: Regression modelling part 2",
    "section": "",
    "text": "Start by subsetting the evals data set so that we only have the variables we are interested in, that is, score, age and gender. Note, it is best to give your new data set a different name than evals as to not overwrite the original evals data set. Your new data set should look like the one below.\n\n\n\n\n\n\n Question\n\n\n\nYou can also view your data set using the glimpse function, or by opening a spreadsheet view in RStudio using the View function.\nUse the skim function to obtain some summary statistics from our data:\n\n\nAnswer\n\n\nCodeeval.score |&gt;\n  skim()\n\n\nData summary\n\n\nName\neval.score\n\n\nNumber of rows\n463\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n7\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\ngender\n0\n1\nFALSE\n2\nmal: 268, fem: 195\n\n\nethnicity\n0\n1\nFALSE\n2\nnot: 399, min: 64\n\n\nlanguage\n0\n1\nFALSE\n2\neng: 435, non: 28\n\n\nrank\n0\n1\nFALSE\n3\nten: 253, ten: 108, tea: 102\n\n\npic_outfit\n0\n1\nFALSE\n2\nnot: 386, for: 77\n\n\npic_color\n0\n1\nFALSE\n2\ncol: 385, bla: 78\n\n\ncls_level\n0\n1\nFALSE\n2\nupp: 306, low: 157\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1\n232.00\n133.80\n1.00\n116.50\n232.00\n347.5\n463.00\n▇▇▇▇▇\n\n\nprof_ID\n0\n1\n45.15\n27.55\n1.00\n20.00\n43.00\n70.5\n94.00\n▇▇▆▆▆\n\n\nscore\n0\n1\n4.17\n0.54\n2.30\n3.80\n4.30\n4.6\n5.00\n▁▁▅▇▇\n\n\nage\n0\n1\n48.37\n9.80\n29.00\n42.00\n48.00\n57.0\n73.00\n▅▆▇▆▁\n\n\nbty_avg\n0\n1\n4.42\n1.53\n1.67\n3.17\n4.33\n5.5\n8.17\n▃▇▇▃▂\n\n\ncls_did_eval\n0\n1\n36.62\n45.02\n5.00\n15.00\n23.00\n40.0\n380.00\n▇▁▁▁▁\n\n\ncls_students\n0\n1\n55.18\n75.07\n8.00\n19.00\n29.00\n60.0\n581.00\n▇▁▁▁▁\n\n\n\n\n\n\n\n\nNow, let’s compute the correlation coefficient between our outcome variable score and our numerical explanatory variable age:\n\nCodeeval.score |&gt; \n  get_correlation(formula = score ~ age)\n\n# A tibble: 1 × 1\n     cor\n   &lt;dbl&gt;\n1 -0.107\n\n\n\n\n\n\n\n\n Question\n\n\n\nWhy do we not include the categorical variable gender when calculating the correlation?\n\n\nAnswer\n\nThe correlation coefficient only exists between numerical variables, which is why we do not include our categorical variable gender.\n\n\n\nWe can now visualise our data by producing a scatterplot, where seeing as we have the categorical variable gender, we shall plot the points using different colours for each gender:\n\nCodeggplot(eval.score, aes(x = age, y = score, color = gender)) +\n  geom_jitter() +\n  labs(x = \"Age\", y = \"Teaching Score\", color = \"Gender\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n Instructor evaluation scores by age and gender. The points have been jittered.\n\n\n\nNote: The above code has jittered the points, however, this is not necessary and geom_point would suffice. To plot separate points by gender we simply add the color argument to the aes function and pass to it gender.\nFrom the scatterplot we can see that:\n\nThere are very few women over the age of 60 in our data set.\nFrom the plotted regression lines we can see that the lines have different slopes for men and women. That is, the associated effect of increasing age appears to be more severe for women than it does for men, i.e. the teaching score of women drops faster with age."
  },
  {
    "objectID": "index.html#multiple-regression-parallel-slopes-model",
    "href": "index.html#multiple-regression-parallel-slopes-model",
    "title": "Week 3: Regression modelling part 2",
    "section": "",
    "text": "Here, we shall begin by fitting what is referred to as a parallel regression lines model. This model implies that the slope of relationship between teaching score (score) and age (age) is the same for both males and females, with only the intercept of the regression lines changing. Hence, our parallel regression lines model is given as:\n\\[\\begin{align}\ny_{i} &= \\alpha + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i \\nonumber \\\\\n&= \\alpha + \\beta_{\\mbox{age}} \\cdot \\mbox{age} + \\beta_{\\mbox{male}} \\cdot \\mathbb{I}_{\\mbox{male}}(x) + \\epsilon_i, \\nonumber\n\\end{align}\\]\nwhere\n\n\\(\\alpha\\) is the intercept of the regression line for females;\n\\(\\beta_{\\mbox{age}}\\) is the slope of the regression line for both males and females;\n\\(\\beta_{\\mbox{male}}\\) is the additional term added to \\(\\alpha\\) to get the intercept of the regression line for males; and\n\n\\(\\mathbb{I}_{\\mbox{male}}(x)\\) is an indicator function such that\n\\[\\mathbb{I}_{\\mbox{male}}(x)=\\left\\{\n            \\begin{array}{ll}\n              1 ~~~ \\mbox{If gender} ~ x ~ \\mbox{is male},\\\\\n              0 ~~~ \\mbox{Otherwise}.\\\\\n            \\end{array}\n          \\right.\\]\n\n\nWe can fit the parallel regression lines model as follows:\n\nCodelm_spec &lt;- linear_reg() |&gt; set_engine(\"lm\")\npar.model &lt;- lm_spec |&gt; fit(score ~ age + gender, data = eval.score)\npar.model &lt;- par.model |&gt; extract_fit_engine()\nget_regression_table(par.model)\n\n# A tibble: 3 × 7\n  term         estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept       4.48      0.125     35.8    0        4.24     4.73 \n2 age            -0.009     0.003     -3.28   0.001   -0.014   -0.003\n3 gender: male    0.191     0.052      3.63   0        0.087    0.294\n\n\nHence, the regression line for females is given by:\n\\[\\widehat{\\mbox{score}} = 4.48 - 0.009 \\cdot \\mbox{age},\\]\nwhile the regression line for males is given by:\n\\[\\widehat{\\mbox{score}} = 4.48 - 0.009 \\cdot \\mbox{age} + 0.191 = 4.671 - 0.009 \\cdot \\mbox{age}.\\] Now, let’s superimpose our parallel regression lines onto the scatterplot of teaching score against age:\n\nCodeggplot(eval.score, aes(x = age, y = score, color = gender)) +\n  geom_jitter() +\n  labs(x = \"Age\", y = \"Teaching Score\", color = \"Gender\") +\n  geom_parallel_slopes(se = FALSE)\n\n\n\n Instructor evaluation scores by age and gender with parallel regression lines superimposed.\n\n\n\nFrom the parallel regression lines model both males and females have the same slope, that is, the associated effect of age on teaching score is the same for both men and women. Hence, for every one year increase in age, there is an associated decrease in teaching score of 0.009. However, male instructors have a higher intercept term, that is, there is a vertical bump in the regression line for males in teaching scores. This is linked to the average difference in teaching scores that males obtain relative to females.\n\n\n\n\n\n\n Question\n\n\n\nWhat is different between our previous scatterplot of teaching score against age and the one we just created with our parallel lines superimposed?\n\n\nAnswer\n\nIn the original plot we have what is referred to as an interaction effect between age and gender. Hence, gender interacts in different ways for both males and females by age, and as such we should have different intercepts and slopes."
  },
  {
    "objectID": "index.html#multiple-regression-interaction-model",
    "href": "index.html#multiple-regression-interaction-model",
    "title": "Week 3: Regression modelling part 2",
    "section": "",
    "text": "There is an interaction effect if the associated effect of one variable depends on the value of another variable. For example, the effect of age here will depend on whether the instructor is male or female, that is, the effect of age on teaching scores will differ by gender. The interaction model can be written as:\n\\[\\begin{align}\ny_{i} &= \\alpha + \\beta_1  x_{1i} + \\beta_2  x_{2i} + \\beta_3  x_{1i}  x_{2i} + \\epsilon_i \\nonumber \\\\\n&= \\alpha + \\beta_{\\mbox{age}} \\cdot \\mbox{age} + \\beta_{\\mbox{male}} \\cdot \\mathbb{I}_{\\mbox{male}}(x) + \\beta_{\\mbox{age, male}} \\cdot \\mbox{age} \\cdot \\mathbb{I}_{\\mbox{male}}(x) + \\epsilon_i, \\nonumber\n\\end{align}\\]\nwhere \\(\\beta_{\\mbox{age, male}} \\cdot \\mbox{age} \\cdot \\mathbb{I}_{\\mbox{male}}(x)\\) corresponds to the interaction term.\nMore concretely:\n\\[y_{i}=\\left\\{\n                \\begin{array}{ll}\n                  \\alpha + \\beta_{\\mbox{age}} \\cdot \\mbox{age} + \\epsilon_i ~~~ \\mbox{If gender} ~ x ~ \\mbox{is female},\\\\\n                  (\\alpha + \\beta_{\\mbox{male}}) + (\\beta_{\\mbox{age}} + \\beta_{\\mbox{age, male}}) \\cdot \\mbox{age} + \\epsilon_i ~~~ \\mbox{Otherwise}.\\\\\n                \\end{array}\n              \\right.\\]\nIn order to fit an interaction term within our regression model we replace the + sign with the * sign as follows:\n\nCodelm_spec &lt;- linear_reg()\nint.model &lt;- lm_spec |&gt; fit(score ~ age * gender, data = eval.score)\nint.model &lt;- int.model |&gt; extract_fit_engine()\nget_regression_table(int.model)\n\n# A tibble: 4 × 7\n  term           estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept         4.88      0.205     23.8    0        4.48     5.29 \n2 age              -0.018     0.004     -3.92   0       -0.026   -0.009\n3 gender: male     -0.446     0.265     -1.68   0.094   -0.968    0.076\n4 age:gendermale    0.014     0.006      2.45   0.015    0.003    0.024\n\n\nHence, the regression line for females is given by:\n\\[\\widehat{\\mbox{score}} = 4.88 - 0.018 \\cdot \\mbox{age},\\] while the regression line for males is given by:\n\\[\\widehat{\\mbox{score}} = 4.88 - 0.018 \\cdot \\mbox{age} - 0.446 + 0.014 \\cdot \\mbox{age} = 4.434 - 0.004 \\cdot \\mbox{age}.\\]\n\n\n\n\n\n\n Question\n\n\n\nHow do they compare with the teaching score values from the parallel regression lines model?\n\n\nAnswer\n\nHere, we can see that, although the intercept for male instructors may be lower, the associated average decrease in teaching score with age (0.004) is not as severe as it is for female instructors (0.018)."
  },
  {
    "objectID": "index.html#assessing-model-fit",
    "href": "index.html#assessing-model-fit",
    "title": "Week 3: Regression modelling part 2",
    "section": "",
    "text": "Now we have to assess the fit of the model by looking at plots of the residuals. We shall do this for the interaction model. First, we need to obtain the fitted values and residuals from the interaction model as follows:\n\nCoderegression.points &lt;- get_regression_points(int.model)\n\n# A tibble: 463 × 6\n      ID score   age gender score_hat residual\n   &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1     1   4.7    36 female      4.25    0.448\n 2     2   4.1    36 female      4.25   -0.152\n 3     3   3.9    36 female      4.25   -0.352\n 4     4   4.8    36 female      4.25    0.548\n 5     5   4.6    59 male        4.20    0.399\n 6     6   4.3    59 male        4.20    0.099\n 7     7   2.8    59 male        4.20   -1.40 \n 8     8   4.1    51 male        4.23   -0.133\n 9     9   3.4    51 male        4.23   -0.833\n10    10   4.5    40 female      4.18    0.318\n# ℹ 453 more rows\n\n\nLet’s start by looking at a scatterplot of the residuals against the explanatory variable by gender:\n\nCodeggplot(regression.points, aes(x = age, y = residual)) +\n  geom_point() +\n  labs(x = \"age\", y = \"Residual\") +\n  geom_hline(yintercept = 0, col = \"blue\", size = 1) +\n  facet_wrap(~ gender)\n\n\n\nResiduals vs the explanatory variable age by gender.\n\n\n\nNow, we can plot the residuals against the fitted values:\n\nCodeggplot(regression.points, aes(x = score_hat, y = residual)) +\n  geom_point() +\n  labs(x = \"Fitted values\", y = \"Residual\") +\n  geom_hline(yintercept = 0, col = \"blue\", size = 1) +\n  facet_wrap(~ gender)\n\n\n\nResiduals vs the fitted values.\n\n\n\n\nFinally, let’s plot histograms of the residuals to assess whether they are normally distributed with mean zero:\n\nCodeggplot(regression.points, aes(x = residual)) +\n  geom_histogram(binwidth = 0.25, color = \"white\") +\n  labs(x = \"Residual\") +\n  facet_wrap(~gender)\n\n\n\nHistograms of the residuals by gender."
  },
  {
    "objectID": "index.html#inference-using-sample-statistics",
    "href": "index.html#inference-using-sample-statistics",
    "title": "Week 3: Regression modelling part 2",
    "section": "\n2.1 Inference using sample statistics",
    "text": "2.1 Inference using sample statistics\nThe table below lists a variety of contexts where sample statistics can be used to estimate population parameters. In all 6 cases, the point estimate/sample statistic estimates the unknown population parameter. It does so by computing summary statistics based on a sample of size \\(n\\). We’ll cover Scenarios 5 and 6, namely construct CIs for the parameters in simple and multiple linear regression models. We will consider CIs based on theoretical results when standard assumptions hold, although sampling procedures such as bootstrap also exist. We will also consider how to use CIs for variable selection and finish by considering a model selection strategy based on objective measures for model comparisons.\n\n\n\n\n\n\n\n\n\n\nTable 1: Scenarios of sample statistics for inference.\n\n\n\n\n\n\n\n\n\nScenario\nPopulation Parameter\nPopulation Notation\nSample Statistic\nSample Notation\n\n\n\n1\nPopulation proportion\n\\(p\\)\nSample proportion\n\\(\\widehat{p}\\)\n\n\n2\nPopulation mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\n\n3\nDiff.in pop. props\n\\(p_1 - p_2\\)\nDiff. in sample props\n\\(\\widehat{p}_1 - \\widehat{p}_2\\)\n\n\n4\nDiff. in pop. means\n\\(\\mu_1 - \\mu_2\\)\nDiff. in sample means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\n\n5\nPop. intercept\n\\(\\beta_0\\)\nSample intercept\n\n\\(\\widehat{\\beta}_0\\) or \\(b_0\\)\n\n\n\n6\nPop. slope\n\\(\\beta_1\\)\nSample slope\n\n\\(\\widehat{\\beta}_1\\) or \\(b_1\\)\n\n\n\n\nIn reality, we don’t have access to the population parameter values (if we did, why would we need to estimate them?) we only have a single sample of data from a larger population. We’d like to be able to make some reasonable guesses about population parameters using that single sample to create a range of plausible values for a population parameter. This range of plausible values is known as a confidence interval.\nThere are theoretical ways of defining confidence intervals for these different scenarios (such as you saw in ‘Statistical Inference’ in Semester 1). But we can also use a single sample to get some idea of how other samples might vary in terms of their sample statistics, i.e. to estimate the sampling distributions of sample statistics. One common way this is done is via a process known as bootstrapping.\nThe confidence intervals we will see this week are calculated using the theoretical results based on the standard assumptions that you will have seen in Regression Modelling in first semester. These values are not based on bootstrapping techniques since these become much harder to implement when working with multiple variables and its beyond the scope of this course."
  },
  {
    "objectID": "index.html#confidence-intervals-for-regression-parameters",
    "href": "index.html#confidence-intervals-for-regression-parameters",
    "title": "Week 3: Regression modelling part 2",
    "section": "\n3.1 Confidence Intervals for Regression Parameters",
    "text": "3.1 Confidence Intervals for Regression Parameters\nTo illustrate this, let’s have another look at teaching evaluations data evals in the moderndive package that we used in Week 3 and start with the SLR model with age as the the single explanatory variable and the instructors’ evaluation scores as the response variable. This data and the fitted model are shown here.\n\nCodeslr.model &lt;- linear_reg()\nslr.model &lt;- slr.model |&gt; fit(score ~ age, data = evals)\n\n (Intercept)          age \n 4.461932354 -0.005938225 \n\n\n\n\n\n\nSLR model applied to the teaching evaluation Data.\n\n\n\nThe point estimate of the slope parameter here is \\(\\widehat{\\beta}=\\) -0.006.\nLet’s continue with the teaching evaluations data by fitting the multiple regression model with one numerical and one categorical explanatory variable. In this model:\n\n\n\\(y\\): response variable of instructor evaluation score\n\nexplanatory variables\n\n\n\\(x_1\\): numerical explanatory variable of age\n\n\n\\(x_2\\): categorical explanatory variable of gender\n\n\n\n\n\nCodeevals_multiple &lt;- evals |&gt;\n                  select(score, gender, age)\n\n\nFirst, recall that we had two competing potential models to explain professors’ teaching evaluation scores:\n\nModel 1: Parallel lines model (no interaction term) - both male and female professors have the same slope describing the associated effect of age on teaching score\nModel 2: Interaction model - allowing for male and female professors to have different slopes describing the associated effect of age on teaching score\n\nRefresher: Visualisations\nRecall the plots we made for both these models:\n\n\n\n\nModel 1: Parallel regression lines.\n\n\n\n\n\n\n\nModel 2: Separate regression lines.\n\n\n\nRefresher: Regression tables\nLet’s also recall the regression models. First, the regression model with no interaction effect: note the use of + in the formula.\n\nCodepar.model &lt;- linear_reg()\npar.model &lt;- par.model |&gt; \n  fit(score ~ age + gender, data = evals_multiple) |&gt; \n  extract_fit_engine()\n\nget_regression_table(par.model) |&gt; \n  knitr::kable(\n               digits = 3,\n               caption = \"Model 1: Regression model with no interaction effect included.\", \n               booktabs = TRUE\n        )\n\n\nModel 1: Regression model with no interaction effect included.\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\nintercept\n4.484\n0.125\n35.792\n0.000\n4.238\n4.730\n\n\nage\n-0.009\n0.003\n-3.280\n0.001\n-0.014\n-0.003\n\n\ngender: male\n0.191\n0.052\n3.632\n0.000\n0.087\n0.294\n\n\n\n\n\nSecond, the regression model with an interaction effect: note the use of * in the formula.\n\nCodeint.model &lt;- linear_reg()\nint.model &lt;- int.model  |&gt;\n  fit(score ~ age * gender, data = evals_multiple) |&gt;\n  extract_fit_engine()\n\nget_regression_table(int.model) |&gt; \n  knitr::kable(\n                digits = 3,\n                caption = \"Model 2: Regression model with interaction effect included.\", \n                booktabs = TRUE\n      )\n\n\nModel 2: Regression model with interaction effect included.\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\nintercept\n4.883\n0.205\n23.795\n0.000\n4.480\n5.286\n\n\nage\n-0.018\n0.004\n-3.919\n0.000\n-0.026\n-0.009\n\n\ngender: male\n-0.446\n0.265\n-1.681\n0.094\n-0.968\n0.076\n\n\nage:gendermale\n0.014\n0.006\n2.446\n0.015\n0.003\n0.024\n\n\n\n\n\nNotice that, together with the estimated parameter values, the tables include other information about each estimated parameter in the model, namely:\n\n\nstd_error: the standard error of each parameter estimate;\n\nstatistic: the test statistic value used to test the null hypothesis that the population parameter is zero;\n\np_value: the \\(p\\) value associated with the test statistic under the null hypothesis; and\n\nlower_ci and upper_ci: the lower and upper bounds of the 95% confidence interval for the population parameter\n\nThese values are calculated using the theoretical results based on the standard assumptions that you will have seen in Regression Modelling in first semester. Theses values are not based on bootstrapping techniques but theoretical results since these become much harder to implement when working with multiple variables and its beyond the scope of this course."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Week 3 Tasks",
    "section": "",
    "text": "Tasks\n\nAssess the model assumptions for the parallel regression lines/slopes model. Do they appear valid?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReturn to the Credit data set and fit a multiple regression model with Balance as the outcome variable, and Income and Age as the explanatory variables, respectively. Assess the assumptions of the multiple regression model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReturn to the Credit data set and fit a parallel regression lines model with Balance as the outcome variable, and Income and Student as the explanatory variables, respectively. Assess the assumptions of the fitted model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrickier\n\nLoad the library datasets and look at the iris data set of Edgar Anderson containing measurements (in centimetres) on 150 different flowers across three different species of iris. Fit an interaction model with Sepal.Width as the outcome variable, and Sepal.Length and Species as the explanatory variables. Assess the assumptions of the fitted model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFurther Tasks\nYou are encouraged to complete the following tasks by using Quarto to produce a single document which summarises all your work, i.e. the original questions, your R code, your comments and reflections, etc.\n\nData was collected on the characteristics of homes in the American city of Los Angeles (LA) in 2010 and can be found in the file LAhomes.csv on the Moodle page. The data contain the following variables:\n\n\ncity - the district of LA where the house was located\ntype - either SFR (Single Family Residences) or Condo/Twh (Condominium/Town House)\nbed - the number of bedrooms\nbath - the number of bathrooms\ngarage - the number of car spaces in the garage\nsqft - the floor area of the house (in square feet)\npool - Y if the house has a pool\nspa - TRUE if the house has a spa\nprice - the most recent sales price ($US)\nWe are interested in exploring the relationships between price and the other variables.\nRead the data into an object called LAhomes and answer the following questions.\n\n\nBy looking at the univariate and bivariate distributions on the price and sqft variables below, what would be a sensible way to proceed if we wanted to model this data? What care must be taken if you were to proceed this way?\n\n\nlibrary(gridExtra) # Package to display plots side by side \n\nhist1 &lt;- ggplot(LAhomes, aes(x = price)) +\n          geom_histogram()\n\nhist2 &lt;- ggplot(LAhomes, aes(x = sqft)) +\n          geom_histogram()\n\nhist1log &lt;- ggplot(LAhomes, aes(x = log(price))) +\n             geom_histogram()\n\nhist2log &lt;- ggplot(LAhomes, aes(x = log(sqft))) +\n             geom_histogram()\n\nplot1 &lt;- ggplot(LAhomes, aes(x = sqft, y = price)) +\n          geom_point()\n\nplot2 &lt;- ggplot(LAhomes, aes(x = log(sqft), y = log(price))) +\n          geom_point()\n\ngrid.arrange(hist1, hist2, hist1log, hist2log, plot1, plot2,\n             ncol = 2, nrow = 3)\n\n\n\n\n\n\n\n\n\nFit the simple linear model with log(price) as the response and log(sqft) as the predictor. Display the fitted model on a scatterplot of the data and construct a confidence interval for the slope parameter in the model and interpret its point and interval estimates.\nRepeat the analysis in part b. but with the log of the number of bathrooms (bath) as the single explanatory variable.\nFit the multiple linear regression model using the log transform of all the variables price (as the response) and both sqft and bath (as the explanatory variables). Calculate the point and interval estimates of the coefficients of the two predictors separately. Compare their point and interval estimates to those you calculated in parts b. and c. Can you account for the differences?\nUsing the objective measures for model comparisons, which of the models in parts b., c. and d. would you favour? Is this consistent with your conclusions in part d.?\n\n\n\nYou have been asked to determine the pricing of a New York City (NYC) Italian restaurant’s dinner menu such that it is competitively positioned with other high-end Italian restaurants by analysing pricing data that have been collected in order to produce a regression model to predict the price of dinner.\nData from surveys of customers of 168 Italian restaurants in the target area are available. The data can be found in the file restNYC.csv on the Moodle page. Each row represents one customer survey from Italian restaurants in NYC and includes the key variables:\n\n\nPrice - price (in $US) of dinner (including a tip and one drink)\nFood - customer rating of the food (from 1 to 30)\nDecor - customer rating of the decor (from 1 to 30)\nService - customer rating of the service (from 1 to 30)\nEast - dummy variable with the value 1 if the restaurant is east of Fifth Avenue, 0 otherwise\n\n\nUse the ggpairs function in the GGally package (see the following code) to generate an informative set of graphical and numerical summaries which illuminate the relationships between pairs of variables. Where do you see the strongest evidence of relationships between price and the potential explanatory variables? Is there evidence of multicollineatity in the data?\n\n\nlibrary(GGally) # Package to produce matrix of 'pairs' plots and more!\nrestNYC$East &lt;- as.factor(restNYC$East) # East needs to be a factor\n# Including the `East` factor\nggpairs(restNYC[, 4:8], aes(colour = East, alpha = 0.4)) \n# Without the `East` factor\nggpairs(restNYC[, 4:7], aes(alpha = 0.4)) \n\n\nFit the simple linear model with Price as the response and Service as the predictor and display the fitted model on a scatterplot of the data. Construct a confidence interval for the slope parameter in the model.\nNow fit a multiple regressing model of Price on Service, Food, and Decor. What happens to the significance of Service when additional variables were added to the model?\nWhat is the correct interpretation of the coefficient on Service in the linear model which regresses Price on Service, Food, and Decor?"
  },
  {
    "objectID": "notes_24_25.html",
    "href": "notes_24_25.html",
    "title": "Week 3: Regression modelling part 2",
    "section": "",
    "text": "Let’s expand upon what we learned last week by revisiting the instructor evaluation data set evals. In Week 3 you were tasked with examining the relationship between teaching score (score) and age (age). Now, let’s also introduce the additional (binary) categorical explanatory variable gender (gender). That is, we we will be examining:\n\nthe teaching score (score) as our outcome variable \\(y\\);\nage (age) as our numerical explanatory variable \\(x_1\\); and\ngender (gender) as our categorical explanatory variable \\(x_2\\).\n\n\nStart by subsetting the evals data set so that we only have the variables we are interested in, that is, score, age and gender. Note, it is best to give your new data set a different name than evals as to not overwrite the original evals data set. Your new data set should look like the one below.\n\n\n\n\n\n\n Question\n\n\n\nYou can also view your data set using the glimpse function, or by opening a spreadsheet view in RStudio using the View function.\nUse the skim function to obtain some summary statistics from our data:\n\n\nAnswer\n\n\nCodeeval.score |&gt;\n  skim()\n\n\nData summary\n\n\nName\neval.score\n\n\nNumber of rows\n463\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n7\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\ngender\n0\n1\nFALSE\n2\nmal: 268, fem: 195\n\n\nethnicity\n0\n1\nFALSE\n2\nnot: 399, min: 64\n\n\nlanguage\n0\n1\nFALSE\n2\neng: 435, non: 28\n\n\nrank\n0\n1\nFALSE\n3\nten: 253, ten: 108, tea: 102\n\n\npic_outfit\n0\n1\nFALSE\n2\nnot: 386, for: 77\n\n\npic_color\n0\n1\nFALSE\n2\ncol: 385, bla: 78\n\n\ncls_level\n0\n1\nFALSE\n2\nupp: 306, low: 157\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1\n232.00\n133.80\n1.00\n116.50\n232.00\n347.5\n463.00\n▇▇▇▇▇\n\n\nprof_ID\n0\n1\n45.15\n27.55\n1.00\n20.00\n43.00\n70.5\n94.00\n▇▇▆▆▆\n\n\nscore\n0\n1\n4.17\n0.54\n2.30\n3.80\n4.30\n4.6\n5.00\n▁▁▅▇▇\n\n\nage\n0\n1\n48.37\n9.80\n29.00\n42.00\n48.00\n57.0\n73.00\n▅▆▇▆▁\n\n\nbty_avg\n0\n1\n4.42\n1.53\n1.67\n3.17\n4.33\n5.5\n8.17\n▃▇▇▃▂\n\n\ncls_did_eval\n0\n1\n36.62\n45.02\n5.00\n15.00\n23.00\n40.0\n380.00\n▇▁▁▁▁\n\n\ncls_students\n0\n1\n55.18\n75.07\n8.00\n19.00\n29.00\n60.0\n581.00\n▇▁▁▁▁\n\n\n\n\n\n\n\n\nNow, let’s compute the correlation coefficient between our outcome variable score and our numerical explanatory variable age:\n\nCodeeval.score |&gt; \n  get_correlation(formula = score ~ age)\n\n# A tibble: 1 × 1\n     cor\n   &lt;dbl&gt;\n1 -0.107\n\n\n\n\n\n\n\n\n Question\n\n\n\nWhy do we not include the categorical variable gender when calculating the correlation?\n\n\nAnswer\n\nThe correlation coefficient only exists between numerical variables, which is why we do not include our categorical variable gender.\n\n\n\nWe can now visualise our data by producing a scatterplot, where seeing as we have the categorical variable gender, we shall plot the points using different colours for each gender:\n\nCodeggplot(eval.score, aes(x = age, y = score, color = gender)) +\n  geom_jitter() +\n  labs(x = \"Age\", y = \"Teaching Score\", color = \"Gender\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n Instructor evaluation scores by age and gender. The points have been jittered.\n\n\n\nNote: The above code has jittered the points, however, this is not necessary and geom_point would suffice. To plot separate points by gender we simply add the color argument to the aes function and pass to it gender.\nFrom the scatterplot we can see that:\n\nThere are very few women over the age of 60 in our data set.\nFrom the plotted regression lines we can see that the lines have different slopes for men and women. That is, the associated effect of increasing age appears to be more severe for women than it does for men, i.e. the teaching score of women drops faster with age.\n\nHere, we shall begin by fitting what is referred to as a parallel regression lines model. This model implies that the slope of relationship between teaching score (score) and age (age) is the same for both males and females, with only the intercept of the regression lines changing. Hence, our parallel regression lines model is given as:\n\\[\\begin{align}\ny_{i} &= \\alpha + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i \\nonumber \\\\\n&= \\alpha + \\beta_{\\mbox{age}} \\cdot \\mbox{age} + \\beta_{\\mbox{male}} \\cdot \\mathbb{I}_{\\mbox{male}}(x) + \\epsilon_i, \\nonumber\n\\end{align}\\]\nwhere\n\n\\(\\alpha\\) is the intercept of the regression line for females;\n\\(\\beta_{\\mbox{age}}\\) is the slope of the regression line for both males and females;\n\\(\\beta_{\\mbox{male}}\\) is the additional term added to \\(\\alpha\\) to get the intercept of the regression line for males; and\n\n\\(\\mathbb{I}_{\\mbox{male}}(x)\\) is an indicator function such that\n\\[\\mathbb{I}_{\\mbox{male}}(x)=\\left\\{\n            \\begin{array}{ll}\n              1 ~~~ \\mbox{if gender} ~ x ~ \\mbox{is male},\\\\\n              0 ~~~ \\mbox{Otherwise}.\\\\\n            \\end{array}\n          \\right.\\]\n\n\nWe can fit the parallel regression lines model as follows:\n\nCodelm_spec &lt;- linear_reg() |&gt; set_engine(\"lm\")\npar.model &lt;- lm_spec |&gt; fit(score ~ age + gender, data = eval.score)\npar.model &lt;- par.model |&gt; extract_fit_engine()\nget_regression_table(par.model)\n\n# A tibble: 3 × 7\n  term         estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept       4.48      0.125     35.8    0        4.24     4.73 \n2 age            -0.009     0.003     -3.28   0.001   -0.014   -0.003\n3 gender: male    0.191     0.052      3.63   0        0.087    0.294\n\n\nHence, the regression line for females is given by:\n\\[\\widehat{\\mbox{score}} = 4.48 - 0.009 \\cdot \\mbox{age},\\]\nwhile the regression line for males is given by:\n\\[\\widehat{\\mbox{score}} = 4.48 - 0.009 \\cdot \\mbox{age} + 0.191 = 4.671 - 0.009 \\cdot \\mbox{age}.\\] Now, let’s superimpose our parallel regression lines onto the scatterplot of teaching score against age:\n\nCodeggplot(eval.score, aes(x = age, y = score, color = gender)) +\n  geom_jitter() +\n  labs(x = \"Age\", y = \"Teaching Score\", color = \"Gender\") +\n  geom_parallel_slopes(se = FALSE)\n\n\n\n Instructor evaluation scores by age and gender with parallel regression lines superimposed.\n\n\n\nNote: go through the code used to create coeff and slopes and make sure you understand it.\nFrom the parallel regression lines model both males and females have the same slope, that is, the associated effect of age on teaching score is the same for both men and women. Hence, for every one year increase in age, there is an associated decrease in teaching score of 0.009. However, male instructors have a higher intercept term, that is, there is a vertical bump in the regression line for males in teaching scores. This is linked to the average difference in teaching scores that males obtain relative to females.\n\n\n\n\n\n\n Question\n\n\n\nWhat is different between our previous scatterplot of teaching score against age (Figure 6) and the one we just created with our parallel lines superimposed (Figure 7)?\n\n\nAnswer\n\nIn the original plot we have what is referred to as an interaction effect between age and gender. Hence, gender interacts in different ways for both males and females by age, and as such we should have different intercepts and slopes.\n\n\n\n\nThere is an interaction effect if the associated effect of one variable depends on the value of another variable. For example, the effect of age here will depend on whether the instructor is male or female, that is, the effect of age on teaching scores will differ by gender. The interaction model can be written as:\n\\[\\begin{align}\ny_{i} &= \\alpha + \\beta_1  x_{1i} + \\beta_2  x_{2i} + \\beta_3  x_{1i}  x_{2i} + \\epsilon_i \\nonumber \\\\\n&= \\alpha + \\beta_{\\mbox{age}} \\cdot \\mbox{age} + \\beta_{\\mbox{male}} \\cdot \\mathbb{I}_{\\mbox{male}}(x) + \\beta_{\\mbox{age, male}} \\cdot \\mbox{age} \\cdot \\mathbb{I}_{\\mbox{male}}(x) + \\epsilon_i, \\nonumber\n\\end{align}\\]\nwhere \\(\\beta_{\\mbox{age, male}} \\cdot \\mbox{age} \\cdot \\mathbb{I}_{\\mbox{male}}(x)\\) corresponds to the interaction term.\nIn order to fit an interaction term within our regression model we replace the + sign with the * sign as follows:\n\nCodelm_spec &lt;- linear_reg()\nint.model &lt;- lm_spec |&gt; fit(score ~ age * gender, data = eval.score)\nint.model &lt;- int.model |&gt; extract_fit_engine()\nget_regression_table(int.model)\n\n# A tibble: 4 × 7\n  term           estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept         4.88      0.205     23.8    0        4.48     5.29 \n2 age              -0.018     0.004     -3.92   0       -0.026   -0.009\n3 gender: male     -0.446     0.265     -1.68   0.094   -0.968    0.076\n4 age:gendermale    0.014     0.006      2.45   0.015    0.003    0.024\n\n\nHence, the regression line for females is given by:\n\\[\\widehat{\\mbox{score}} = 4.88 - 0.018 \\cdot \\mbox{age},\\] while the regression line for males is given by:\n\\[\\widehat{\\mbox{score}} = 4.88 - 0.018 \\cdot \\mbox{age} - 0.446 + 0.014 \\cdot \\mbox{age} = 4.434 - 0.004 \\cdot \\mbox{age}.\\]\n\n\n\n\n\n\n Question\n\n\n\nHow do they compare with the teaching score values from the parallel regression lines model?\n\n\nAnswer\n\nHere, we can see that, although the intercept for male instructors may be lower, the associated average decrease in teaching score with age (0.004) is not as severe as it is for female instructors (0.018).\n\n\n\n\nNow we have to assess the fit of the model by looking at plots of the residuals. We shall do this for the interaction model. First, we need to obtain the fitted values and residuals from the interaction model as follows:\n\nCoderegression.points &lt;- get_regression_points(int.model)\n\n# A tibble: 463 × 6\n      ID score   age gender score_hat residual\n   &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1     1   4.7    36 female      4.25    0.448\n 2     2   4.1    36 female      4.25   -0.152\n 3     3   3.9    36 female      4.25   -0.352\n 4     4   4.8    36 female      4.25    0.548\n 5     5   4.6    59 male        4.20    0.399\n 6     6   4.3    59 male        4.20    0.099\n 7     7   2.8    59 male        4.20   -1.40 \n 8     8   4.1    51 male        4.23   -0.133\n 9     9   3.4    51 male        4.23   -0.833\n10    10   4.5    40 female      4.18    0.318\n# ℹ 453 more rows\n\n\nLet’s start by looking at a scatterplot of the residuals against the explanatory variable by gender:\n\nCodeggplot(regression.points, aes(x = age, y = residual)) +\n  geom_point() +\n  labs(x = \"age\", y = \"Residual\") +\n  geom_hline(yintercept = 0, col = \"blue\", size = 1) +\n  facet_wrap(~ gender)\n\n\n\nResiduals vs the explanatory variable age by gender.\n\n\n\nNow, we can plot the residuals against the fitted values:\n\nCodeggplot(regression.points, aes(x = score_hat, y = residual)) +\n  geom_point() +\n  labs(x = \"Fitted values\", y = \"Residual\") +\n  geom_hline(yintercept = 0, col = \"blue\", size = 1) +\n  facet_wrap(~ gender)\n\n\n\nResiduals vs the fitted values.\n\n\n\n\nFinally, let’s plot histograms of the residuals to assess whether they are normally distributed with mean zero:\n\nCodeggplot(regression.points, aes(x = residual)) +\n  geom_histogram(binwidth = 0.25, color = \"white\") +\n  labs(x = \"Residual\") +\n  facet_wrap(~gender)\n\n\n\nHistograms of the residuals by gender."
  },
  {
    "objectID": "notes_24_25.html#exploratory-data-analysis",
    "href": "notes_24_25.html#exploratory-data-analysis",
    "title": "Week 3: Regression modelling part 2",
    "section": "",
    "text": "Start by subsetting the evals data set so that we only have the variables we are interested in, that is, score, age and gender. Note, it is best to give your new data set a different name than evals as to not overwrite the original evals data set. Your new data set should look like the one below.\n\n\n\n\n\n\n Question\n\n\n\nYou can also view your data set using the glimpse function, or by opening a spreadsheet view in RStudio using the View function.\nUse the skim function to obtain some summary statistics from our data:\n\n\nAnswer\n\n\nCodeeval.score |&gt;\n  skim()\n\n\nData summary\n\n\nName\neval.score\n\n\nNumber of rows\n463\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n7\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\ngender\n0\n1\nFALSE\n2\nmal: 268, fem: 195\n\n\nethnicity\n0\n1\nFALSE\n2\nnot: 399, min: 64\n\n\nlanguage\n0\n1\nFALSE\n2\neng: 435, non: 28\n\n\nrank\n0\n1\nFALSE\n3\nten: 253, ten: 108, tea: 102\n\n\npic_outfit\n0\n1\nFALSE\n2\nnot: 386, for: 77\n\n\npic_color\n0\n1\nFALSE\n2\ncol: 385, bla: 78\n\n\ncls_level\n0\n1\nFALSE\n2\nupp: 306, low: 157\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1\n232.00\n133.80\n1.00\n116.50\n232.00\n347.5\n463.00\n▇▇▇▇▇\n\n\nprof_ID\n0\n1\n45.15\n27.55\n1.00\n20.00\n43.00\n70.5\n94.00\n▇▇▆▆▆\n\n\nscore\n0\n1\n4.17\n0.54\n2.30\n3.80\n4.30\n4.6\n5.00\n▁▁▅▇▇\n\n\nage\n0\n1\n48.37\n9.80\n29.00\n42.00\n48.00\n57.0\n73.00\n▅▆▇▆▁\n\n\nbty_avg\n0\n1\n4.42\n1.53\n1.67\n3.17\n4.33\n5.5\n8.17\n▃▇▇▃▂\n\n\ncls_did_eval\n0\n1\n36.62\n45.02\n5.00\n15.00\n23.00\n40.0\n380.00\n▇▁▁▁▁\n\n\ncls_students\n0\n1\n55.18\n75.07\n8.00\n19.00\n29.00\n60.0\n581.00\n▇▁▁▁▁\n\n\n\n\n\n\n\n\nNow, let’s compute the correlation coefficient between our outcome variable score and our numerical explanatory variable age:\n\nCodeeval.score |&gt; \n  get_correlation(formula = score ~ age)\n\n# A tibble: 1 × 1\n     cor\n   &lt;dbl&gt;\n1 -0.107\n\n\n\n\n\n\n\n\n Question\n\n\n\nWhy do we not include the categorical variable gender when calculating the correlation?\n\n\nAnswer\n\nThe correlation coefficient only exists between numerical variables, which is why we do not include our categorical variable gender.\n\n\n\nWe can now visualise our data by producing a scatterplot, where seeing as we have the categorical variable gender, we shall plot the points using different colours for each gender:\n\nCodeggplot(eval.score, aes(x = age, y = score, color = gender)) +\n  geom_jitter() +\n  labs(x = \"Age\", y = \"Teaching Score\", color = \"Gender\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n Instructor evaluation scores by age and gender. The points have been jittered.\n\n\n\nNote: The above code has jittered the points, however, this is not necessary and geom_point would suffice. To plot separate points by gender we simply add the color argument to the aes function and pass to it gender.\nFrom the scatterplot we can see that:\n\nThere are very few women over the age of 60 in our data set.\nFrom the plotted regression lines we can see that the lines have different slopes for men and women. That is, the associated effect of increasing age appears to be more severe for women than it does for men, i.e. the teaching score of women drops faster with age."
  },
  {
    "objectID": "notes_24_25.html#multiple-regression-parallel-slopes-model",
    "href": "notes_24_25.html#multiple-regression-parallel-slopes-model",
    "title": "Week 3: Regression modelling part 2",
    "section": "",
    "text": "Here, we shall begin by fitting what is referred to as a parallel regression lines model. This model implies that the slope of relationship between teaching score (score) and age (age) is the same for both males and females, with only the intercept of the regression lines changing. Hence, our parallel regression lines model is given as:\n\\[\\begin{align}\ny_{i} &= \\alpha + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i \\nonumber \\\\\n&= \\alpha + \\beta_{\\mbox{age}} \\cdot \\mbox{age} + \\beta_{\\mbox{male}} \\cdot \\mathbb{I}_{\\mbox{male}}(x) + \\epsilon_i, \\nonumber\n\\end{align}\\]\nwhere\n\n\\(\\alpha\\) is the intercept of the regression line for females;\n\\(\\beta_{\\mbox{age}}\\) is the slope of the regression line for both males and females;\n\\(\\beta_{\\mbox{male}}\\) is the additional term added to \\(\\alpha\\) to get the intercept of the regression line for males; and\n\n\\(\\mathbb{I}_{\\mbox{male}}(x)\\) is an indicator function such that\n\\[\\mathbb{I}_{\\mbox{male}}(x)=\\left\\{\n            \\begin{array}{ll}\n              1 ~~~ \\mbox{if gender} ~ x ~ \\mbox{is male},\\\\\n              0 ~~~ \\mbox{Otherwise}.\\\\\n            \\end{array}\n          \\right.\\]\n\n\nWe can fit the parallel regression lines model as follows:\n\nCodelm_spec &lt;- linear_reg() |&gt; set_engine(\"lm\")\npar.model &lt;- lm_spec |&gt; fit(score ~ age + gender, data = eval.score)\npar.model &lt;- par.model |&gt; extract_fit_engine()\nget_regression_table(par.model)\n\n# A tibble: 3 × 7\n  term         estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept       4.48      0.125     35.8    0        4.24     4.73 \n2 age            -0.009     0.003     -3.28   0.001   -0.014   -0.003\n3 gender: male    0.191     0.052      3.63   0        0.087    0.294\n\n\nHence, the regression line for females is given by:\n\\[\\widehat{\\mbox{score}} = 4.48 - 0.009 \\cdot \\mbox{age},\\]\nwhile the regression line for males is given by:\n\\[\\widehat{\\mbox{score}} = 4.48 - 0.009 \\cdot \\mbox{age} + 0.191 = 4.671 - 0.009 \\cdot \\mbox{age}.\\] Now, let’s superimpose our parallel regression lines onto the scatterplot of teaching score against age:\n\nCodeggplot(eval.score, aes(x = age, y = score, color = gender)) +\n  geom_jitter() +\n  labs(x = \"Age\", y = \"Teaching Score\", color = \"Gender\") +\n  geom_parallel_slopes(se = FALSE)\n\n\n\n Instructor evaluation scores by age and gender with parallel regression lines superimposed.\n\n\n\nNote: go through the code used to create coeff and slopes and make sure you understand it.\nFrom the parallel regression lines model both males and females have the same slope, that is, the associated effect of age on teaching score is the same for both men and women. Hence, for every one year increase in age, there is an associated decrease in teaching score of 0.009. However, male instructors have a higher intercept term, that is, there is a vertical bump in the regression line for males in teaching scores. This is linked to the average difference in teaching scores that males obtain relative to females.\n\n\n\n\n\n\n Question\n\n\n\nWhat is different between our previous scatterplot of teaching score against age (Figure 6) and the one we just created with our parallel lines superimposed (Figure 7)?\n\n\nAnswer\n\nIn the original plot we have what is referred to as an interaction effect between age and gender. Hence, gender interacts in different ways for both males and females by age, and as such we should have different intercepts and slopes."
  },
  {
    "objectID": "notes_24_25.html#multiple-regression-interaction-model",
    "href": "notes_24_25.html#multiple-regression-interaction-model",
    "title": "Week 3: Regression modelling part 2",
    "section": "",
    "text": "There is an interaction effect if the associated effect of one variable depends on the value of another variable. For example, the effect of age here will depend on whether the instructor is male or female, that is, the effect of age on teaching scores will differ by gender. The interaction model can be written as:\n\\[\\begin{align}\ny_{i} &= \\alpha + \\beta_1  x_{1i} + \\beta_2  x_{2i} + \\beta_3  x_{1i}  x_{2i} + \\epsilon_i \\nonumber \\\\\n&= \\alpha + \\beta_{\\mbox{age}} \\cdot \\mbox{age} + \\beta_{\\mbox{male}} \\cdot \\mathbb{I}_{\\mbox{male}}(x) + \\beta_{\\mbox{age, male}} \\cdot \\mbox{age} \\cdot \\mathbb{I}_{\\mbox{male}}(x) + \\epsilon_i, \\nonumber\n\\end{align}\\]\nwhere \\(\\beta_{\\mbox{age, male}} \\cdot \\mbox{age} \\cdot \\mathbb{I}_{\\mbox{male}}(x)\\) corresponds to the interaction term.\nIn order to fit an interaction term within our regression model we replace the + sign with the * sign as follows:\n\nCodelm_spec &lt;- linear_reg()\nint.model &lt;- lm_spec |&gt; fit(score ~ age * gender, data = eval.score)\nint.model &lt;- int.model |&gt; extract_fit_engine()\nget_regression_table(int.model)\n\n# A tibble: 4 × 7\n  term           estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept         4.88      0.205     23.8    0        4.48     5.29 \n2 age              -0.018     0.004     -3.92   0       -0.026   -0.009\n3 gender: male     -0.446     0.265     -1.68   0.094   -0.968    0.076\n4 age:gendermale    0.014     0.006      2.45   0.015    0.003    0.024\n\n\nHence, the regression line for females is given by:\n\\[\\widehat{\\mbox{score}} = 4.88 - 0.018 \\cdot \\mbox{age},\\] while the regression line for males is given by:\n\\[\\widehat{\\mbox{score}} = 4.88 - 0.018 \\cdot \\mbox{age} - 0.446 + 0.014 \\cdot \\mbox{age} = 4.434 - 0.004 \\cdot \\mbox{age}.\\]\n\n\n\n\n\n\n Question\n\n\n\nHow do they compare with the teaching score values from the parallel regression lines model?\n\n\nAnswer\n\nHere, we can see that, although the intercept for male instructors may be lower, the associated average decrease in teaching score with age (0.004) is not as severe as it is for female instructors (0.018)."
  },
  {
    "objectID": "notes_24_25.html#assessing-model-fit",
    "href": "notes_24_25.html#assessing-model-fit",
    "title": "Week 3: Regression modelling part 2",
    "section": "",
    "text": "Now we have to assess the fit of the model by looking at plots of the residuals. We shall do this for the interaction model. First, we need to obtain the fitted values and residuals from the interaction model as follows:\n\nCoderegression.points &lt;- get_regression_points(int.model)\n\n# A tibble: 463 × 6\n      ID score   age gender score_hat residual\n   &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1     1   4.7    36 female      4.25    0.448\n 2     2   4.1    36 female      4.25   -0.152\n 3     3   3.9    36 female      4.25   -0.352\n 4     4   4.8    36 female      4.25    0.548\n 5     5   4.6    59 male        4.20    0.399\n 6     6   4.3    59 male        4.20    0.099\n 7     7   2.8    59 male        4.20   -1.40 \n 8     8   4.1    51 male        4.23   -0.133\n 9     9   3.4    51 male        4.23   -0.833\n10    10   4.5    40 female      4.18    0.318\n# ℹ 453 more rows\n\n\nLet’s start by looking at a scatterplot of the residuals against the explanatory variable by gender:\n\nCodeggplot(regression.points, aes(x = age, y = residual)) +\n  geom_point() +\n  labs(x = \"age\", y = \"Residual\") +\n  geom_hline(yintercept = 0, col = \"blue\", size = 1) +\n  facet_wrap(~ gender)\n\n\n\nResiduals vs the explanatory variable age by gender.\n\n\n\nNow, we can plot the residuals against the fitted values:\n\nCodeggplot(regression.points, aes(x = score_hat, y = residual)) +\n  geom_point() +\n  labs(x = \"Fitted values\", y = \"Residual\") +\n  geom_hline(yintercept = 0, col = \"blue\", size = 1) +\n  facet_wrap(~ gender)\n\n\n\nResiduals vs the fitted values.\n\n\n\n\nFinally, let’s plot histograms of the residuals to assess whether they are normally distributed with mean zero:\n\nCodeggplot(regression.points, aes(x = residual)) +\n  geom_histogram(binwidth = 0.25, color = \"white\") +\n  labs(x = \"Residual\") +\n  facet_wrap(~gender)\n\n\n\nHistograms of the residuals by gender."
  },
  {
    "objectID": "notes_24_25.html#inference-using-sample-statistics",
    "href": "notes_24_25.html#inference-using-sample-statistics",
    "title": "Week 3: Regression modelling part 2",
    "section": "\n2.1 Inference using sample statistics",
    "text": "2.1 Inference using sample statistics\nThe table below lists a variety of contexts where sample statistics can be used to estimate population parameters. In all 6 cases, the point estimate/sample statistic estimates the unknown population parameter. It does so by computing summary statistics based on a sample of size \\(n\\). We’ll cover Scenarios 5 and 6, namely construct CIs for the parameters in simple and multiple linear regression models. We will consider CIs based on theoretical results when standard assumptions hold, although sampling procedures such as bootstrap also exist. We will also consider how to use CIs for variable selection and finish by considering a model selection strategy based on objective measures for model comparisons.\n\n\n\n\n\n\n\n\n\n\nTable 1: Scenarios of sample statistics for inference.\n\n\n\n\n\n\n\n\n\nScenario\nPopulation Parameter\nPopulation Notation\nSample Statistic\nSample Notation\n\n\n\n1\nPopulation proportion\n\\(p\\)\nSample proportion\n\\(\\widehat{p}\\)\n\n\n2\nPopulation mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\n\n3\nDiff.in pop. props\n\\(p_1 - p_2\\)\nDiff. in sample props\n\\(\\widehat{p}_1 - \\widehat{p}_2\\)\n\n\n4\nDiff. in pop. means\n\\(\\mu_1 - \\mu_2\\)\nDiff. in sample means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\n\n5\nPop. intercept\n\\(\\beta_0\\)\nSample intercept\n\n\\(\\widehat{\\beta}_0\\) or \\(b_0\\)\n\n\n\n6\nPop. slope\n\\(\\beta_1\\)\nSample slope\n\n\\(\\widehat{\\beta}_1\\) or \\(b_1\\)\n\n\n\n\nIn reality, we don’t have access to the population parameter values (if we did, why would we need to estimate them?) we only have a single sample of data from a larger population. We’d like to be able to make some reasonable guesses about population parameters using that single sample to create a range of plausible values for a population parameter. This range of plausible values is known as a confidence interval.\nThere are theoretical ways of defining confidence intervals for these different scenarios (such as you saw in ‘Statistical Inference’ in Semester 1). But we can also use a single sample to get some idea of how other samples might vary in terms of their sample statistics, i.e. to estimate the sampling distributions of sample statistics. One common way this is done is via a process known as bootstrapping.\nThe confidence intervals we will see this week are calculated using the theoretical results based on the standard assumptions that you will have seen in Regression Modelling in first semester. These values are not based on bootstrapping techniques since these become much harder to implement when working with multiple variables and its beyond the scope of this course."
  },
  {
    "objectID": "notes_24_25.html#confidence-intervals-for-regression-parameters",
    "href": "notes_24_25.html#confidence-intervals-for-regression-parameters",
    "title": "Week 3: Regression modelling part 2",
    "section": "\n3.1 Confidence Intervals for Regression Parameters",
    "text": "3.1 Confidence Intervals for Regression Parameters\nTo illustrate this, let’s have another look at teaching evaluations data evals in the moderndive package that we used in Week 3 and start with the SLR model with age as the the single explanatory variable and the instructors’ evaluation scores as the response variable. This data and the fitted model are shown here.\n\nCodeslr.model &lt;- linear_reg()\nslr.model &lt;- slr.model |&gt; fit(score ~ age, data = evals)\n\n (Intercept)          age \n 4.461932354 -0.005938225 \n\n\n\n\n\n\nFigure 1: SLR model applied to the teaching evaluation Data.\n\n\n\nThe point estimate of the slope parameter here is \\(\\widehat{\\beta}=\\) -0.006.\nLet’s continue with the teaching evaluations data by fitting the multiple regression model with one numerical and one categorical explanatory variable. In this model:\n\n\n\\(y\\): response variable of instructor evaluation score\n\nexplanatory variables\n\n\n\\(x_1\\): numerical explanatory variable of age\n\n\n\\(x_2\\): categorical explanatory variable of gender\n\n\n\n\n\nCodeevals_multiple &lt;- evals |&gt;\n                  select(score, gender, age)\n\n\nFirst, recall that we had two competing potential models to explain professors’ teaching evaluation scores:\n\nModel 1: Parallel lines model (no interaction term) - both male and female professors have the same slope describing the associated effect of age on teaching score\nModel 2: Interaction model - allowing for male and female professors to have different slopes describing the associated effect of age on teaching score\n\nRefresher: Visualisations\nRecall the plots we made for both these models:\n\n\n\n\nModel 1: Parallel regression lines.\n\n\n\n\n\n\n\nModel 2: Separate regression lines.\n\n\n\nRefresher: Regression tables\nLet’s also recall the regression models. First, the regression model with no interaction effect: note the use of + in the formula.\n\nCodepar.model &lt;- linear_reg()\npar.model &lt;- par.model |&gt; \n  fit(score ~ age + gender, data = evals_multiple) |&gt; \n  extract_fit_engine()\n\nget_regression_table(par.model) |&gt; \n  knitr::kable(\n               digits = 3,\n               caption = \"Model 1: Regression model with no interaction effect included.\", \n               booktabs = TRUE\n        )\n\n\nModel 1: Regression model with no interaction effect included.\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\nintercept\n4.484\n0.125\n35.792\n0.000\n4.238\n4.730\n\n\nage\n-0.009\n0.003\n-3.280\n0.001\n-0.014\n-0.003\n\n\ngender: male\n0.191\n0.052\n3.632\n0.000\n0.087\n0.294\n\n\n\n\n\nSecond, the regression model with an interaction effect: note the use of * in the formula.\n\nCodeint.model &lt;- linear_reg()\nint.model &lt;- int.model  |&gt;\n  fit(score ~ age * gender, data = evals_multiple) |&gt;\n  extract_fit_engine()\n\nget_regression_table(int.model) |&gt; \n  knitr::kable(\n                digits = 3,\n                caption = \"Model 2: Regression model with interaction effect included.\", \n                booktabs = TRUE\n      )\n\n\nModel 2: Regression model with interaction effect included.\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\nintercept\n4.883\n0.205\n23.795\n0.000\n4.480\n5.286\n\n\nage\n-0.018\n0.004\n-3.919\n0.000\n-0.026\n-0.009\n\n\ngender: male\n-0.446\n0.265\n-1.681\n0.094\n-0.968\n0.076\n\n\nage:gendermale\n0.014\n0.006\n2.446\n0.015\n0.003\n0.024\n\n\n\n\n\nNotice that, together with the estimated parameter values, the tables include other information about each estimated parameter in the model, namely:\n\n\nstd_error: the standard error of each parameter estimate;\n\nstatistic: the test statistic value used to test the null hypothesis that the population parameter is zero;\n\np_value: the \\(p\\) value associated with the test statistic under the null hypothesis; and\n\nlower_ci and upper_ci: the lower and upper bounds of the 95% confidence interval for the population parameter\n\nThese values are calculated using the theoretical results based on the standard assumptions that you will have seen in Regression Modelling in first semester. Theses values are not based on bootstrapping techniques but theoretical results since these become much harder to implement when working with multiple variables and its beyond the scope of this course."
  }
]