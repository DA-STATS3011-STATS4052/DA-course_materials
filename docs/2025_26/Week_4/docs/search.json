[
  {
    "objectID": "slides/slides.html#simple-linear-regression",
    "href": "slides/slides.html#simple-linear-regression",
    "title": "Week 4: Regression modelling part 2",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nLet the general form of the simple linear model:\n\\[y_i = \\beta_0 + \\beta_1 x_{i,1} + \\epsilon_i,  \\ i = 1,..n \\\\\n\\mathrm{such \\  that } \\ \\epsilon \\sim \\mathrm{Normal}(0,\\sigma^2)\\]\nused to describe the relationship between a response y and a set of variables or predictors x\n\n\\(y_i\\) is the \\(i^{th}\\) observation of the response variable;\n\\(\\alpha\\) is the intercept of the regression line;\n\\(\\beta\\) is the slope of the regression line;\n\\(x_i\\) is the \\(i^{th}\\) observation of the explanatory variable; and\n\\(\\epsilon_i\\) is the \\(i^{th}\\) random component."
  },
  {
    "objectID": "slides/slides.html#fitting-a-simple-linear-regression-in-r",
    "href": "slides/slides.html#fitting-a-simple-linear-regression-in-r",
    "title": "Week 4: Regression modelling part 2",
    "section": "Fitting a Simple Linear Regression in R",
    "text": "Fitting a Simple Linear Regression in R\nData\nStudent evaluations for a sample of 463 courses taught by 94 professors from the University of Texas at Austins, including information on the professor evaluation score and a the age of each professor.\n\nStatistical modelR-Code\n\n\n\\(\\text{score} = \\alpha + \\beta \\  \\text{age} + \\ \\epsilon\\)\n\n\n\nmodel &lt;- lm(score ~ age, data = evals.scores)\ntab_model(model)\n\n\n\n\n\n \nscore\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n4.46\n4.21 – 4.71\n&lt;0.001\n\n\nage\n-0.01\n-0.01 – -0.00\n0.021\n\n\nObservations\n463\n\n\nR2 / R2 adjusted\n0.011 / 0.009"
  },
  {
    "objectID": "slides/slides.html#visualize-a-simple-linear-regression",
    "href": "slides/slides.html#visualize-a-simple-linear-regression",
    "title": "Week 4: Regression modelling part 2",
    "section": "Visualize a Simple Linear Regression",
    "text": "Visualize a Simple Linear Regression\n\nBest fitting lineModel Diagnostics\n\n\n\nggplot(evals.scores, aes(x = age, y = score)) +\n  geom_point() +\n  labs(x = \"Age\", y = \"Teaching Score\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nlibrary(performance)\ncheck_model(model,check=c(\"homogeneity\",\"linearity\",\"qq\",\"normality\"))"
  },
  {
    "objectID": "slides/slides.html#simple-linear-regression-with-a-categorical-variable",
    "href": "slides/slides.html#simple-linear-regression-with-a-categorical-variable",
    "title": "Week 4: Regression modelling part 2",
    "section": "Simple Linear Regression with a Categorical variable",
    "text": "Simple Linear Regression with a Categorical variable\nHere, we will fit a simple linear regression model where the explanatory variable is categorical, i.e. a factor with two or more levels\n\nStatistical ModelR-Code\n\n\n\\[ y_{i} = \\alpha + \\beta_{\\text{male}} \\cdot \\mathbb{I}_{\\text{male}}(x) + \\epsilon_i \\]\n\\[\\mathbb{I}_{\\text{male}}(x)=\\left\\{\n            \\begin{array}{ll}\n              1 ~~~ \\text{If gender} ~ x ~ \\text{is male},\\\\\n              0 ~~~ \\text{Otherwise}.\\\\\n            \\end{array}\n          \\right.\\]\n\n\n\nmodel_2 &lt;- lm(score ~ gender, data = evals.scores_2)\ntab_model(model_2)\n\n\n\n\n\n \nscore\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n4.09\n4.02 – 4.17\n&lt;0.001\n\n\ngender [male]\n0.14\n0.04 – 0.24\n0.006\n\n\nObservations\n463\n\n\nR2 / R2 adjusted\n0.017 / 0.014"
  },
  {
    "objectID": "slides/slides.html#multiple-linear-regression",
    "href": "slides/slides.html#multiple-linear-regression",
    "title": "Week 4: Regression modelling part 2",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nLet the general form of the linear model:\n\\[y_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + ... +  \\beta_p x_{i,p} + \\epsilon_i,  \\ i = 1,..n\\]\n\n\\(y_i\\) is our response of the \\(i^{th}\\) observation;\n\\(\\beta_0\\) is the intercept;\n\\(\\beta_1\\) is the coefficient for the first explanatory variable \\(x_1\\);\n\\(\\beta_2\\) is the coefficient for the second explanatory variable \\(x_2\\);\n\\(\\beta_p\\) is the coefficient for the \\(p^{th}\\) explanatory variable \\(x_p\\);\n\\(\\epsilon_i\\) is the \\(i^{th}\\) random error component.\n\nLast week we covered the case with two numerical explanatory variables. This week, we will have a look at the case with one numerical and one categorical explanatory variable."
  },
  {
    "objectID": "slides/slides.html#parallel-lines-model",
    "href": "slides/slides.html#parallel-lines-model",
    "title": "Week 4: Regression modelling part 2",
    "section": "Parallel lines model",
    "text": "Parallel lines model\nWe already explored the relationship between teaching score (score) and age (age). Now, let’s also introduce the additional (binary) categorical explanatory variable gender (gender).\n\nStatistical ModelR Code\n\n\n\\[\\begin{align}\ny_{i} &= \\alpha + \\beta_{\\text{age}} \\cdot \\text{age} + \\beta_{\\text{male}} \\cdot \\mathbb{I}_{\\text{male}}(x) + \\epsilon_i, \\nonumber\n\\end{align}\\]\n\n\\(\\alpha\\) is the intercept of the regression line for females;\n\\(\\beta_{\\text{age}}\\) is the slope of the regression line for both males and females;\n\\(\\beta_{\\text{male}}\\) is the additional term added to \\(\\alpha\\) to get the intercept of the regression line for males; and\n\\(\\mathbb{I}_{\\text{male}}(x)\\) is an indicator function such that takes the value of 1 if the person is a male and zero otherwise.\n\n\n\n\nevals_multiple &lt;- evals %&gt;%\n                  select(score, gender, age)\npar.model &lt;- lm(score ~ age + gender, data = evals_multiple)\ntab_model(par.model)\n\n\n\n\n\n \nscore\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n4.48\n4.24 – 4.73\n&lt;0.001\n\n\nage\n-0.01\n-0.01 – -0.00\n0.001\n\n\ngender [male]\n0.19\n0.09 – 0.29\n&lt;0.001\n\n\nObservations\n463\n\n\nR2 / R2 adjusted\n0.039 / 0.035\n\n\n\n\n\n\n\n\nHence, the regression line for females is given by:\n\\[\\widehat{\\text{score}} = 4.48 - 0.009 \\cdot \\text{age},\\]\nwhile the regression line for males is given by:\n\\[\\widehat{\\text{score}} = 4.48 - 0.009 \\cdot \\text{age} + 0.191 = 4.671 - 0.009 \\cdot \\text{age}.\\]"
  },
  {
    "objectID": "slides/slides.html#visualize-the-parallel-regression-lines",
    "href": "slides/slides.html#visualize-the-parallel-regression-lines",
    "title": "Week 4: Regression modelling part 2",
    "section": "Visualize the parallel regression lines",
    "text": "Visualize the parallel regression lines\n\n\nplot_model(model = par.model,                 \n           type=\"pred\",              \n           terms = c(\"age\",\"gender\"), \n           grid=T,                    \n           show.data = T,             \n           jitter=T,                  \n           ci.lvl = NA,               \n           title=\"\") \n\n\n\n\n\n\n\n\n\n\n\nFrom the parallel regression lines model the associated effect of age on teaching score is the same for both men and women.\nFor every one year increase in age, there is an associated decrease in teaching score of 0.009.\nMale instructors have a higher intercept terms. This is linked to the average difference in teaching scores that males obtain relative to females."
  },
  {
    "objectID": "slides/slides.html#multiple-regression-interaction-model",
    "href": "slides/slides.html#multiple-regression-interaction-model",
    "title": "Week 4: Regression modelling part 2",
    "section": "Multiple regression: interaction model",
    "text": "Multiple regression: interaction model\nIn this model, the effect of age on teaching scores will differ by gender.\n\nStatistical modelR Code\n\n\n\\[\\begin{align}\ny_{i} &= \\alpha + \\beta_{\\text{age}} \\cdot \\text{age} + \\beta_{\\text{male}} \\cdot \\mathbb{I}_{\\text{male}}(x) + \\beta_{\\text{age, male}} \\cdot \\text{age} \\cdot \\mathbb{I}_{\\text{male}}(x) + \\epsilon_i, \\nonumber\n\\end{align}\\]\nwhere \\(\\beta_{\\text{age, male}} \\cdot \\text{age} \\cdot \\mathbb{I}_{\\text{male}}(x)\\) corresponds to the interaction term.\nMore concretely:\n\\[y_{i}=\\left\\{\n                \\begin{array}{ll}\n                  \\alpha + \\beta_{\\text{age}} \\cdot \\text{age} + \\epsilon_i ~~~ \\text{If gender} ~ x ~ \\text{is female},\\\\\n                  (\\alpha + \\beta_{\\text{male}}) + (\\beta_{\\text{age}} + \\beta_{\\text{age, male}}) \\cdot \\text{age} + \\epsilon_i ~~~ \\text{Otherwise}.\\\\\n                \\end{array}\n              \\right.\\]\n\n\n\nint.model &lt;- lm(score ~ age * gender, data = evals_multiple)\nbroom::tidy(int.model)\n\n# A tibble: 4 × 5\n  term           estimate std.error statistic  p.value\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)      4.88     0.205       23.8  4.02e-82\n2 age             -0.0175   0.00447     -3.92 1.03e- 4\n3 gendermale      -0.446    0.265       -1.68 9.35e- 2\n4 age:gendermale   0.0135   0.00553      2.45 1.48e- 2\n\n\n\nThe regression line for females is given by:\n\\[\n\\widehat{\\text{score}} = 4.88 - 0.018 \\cdot \\text{age},\n\\]\nThe regression line for males is given by:\n\n\\[\\widehat{\\text{score}} = 4.88 - 0.018 \\cdot \\text{age} - 0.446 + 0.014 \\cdot \\text{age} = 4.434 - 0.004 \\cdot \\text{age}.\\]"
  },
  {
    "objectID": "slides/slides.html#visualize-the-regression-model-with-interaction",
    "href": "slides/slides.html#visualize-the-regression-model-with-interaction",
    "title": "Week 4: Regression modelling part 2",
    "section": "Visualize the regression model with interaction",
    "text": "Visualize the regression model with interaction\n\n\nplot_model(model = int.model,        \n           type=  \"pred\",              \n           terms = c(\"age\",\"gender\"), \n           grid=F,                    \n           show.data = T,           \n           jitter=T,                  \n           ci.lvl = NA,               \n           title =\"\",\n           colors = c(\"purple\", \"orange\"))"
  },
  {
    "objectID": "slides/slides.html#assessing-model-fit",
    "href": "slides/slides.html#assessing-model-fit",
    "title": "Week 4: Regression modelling part 2",
    "section": "Assessing model fit",
    "text": "Assessing model fit\nNow we have to assess the fit of the model by looking at plots of the residuals.\n\nint.model_output &lt;-  evals_multiple %&gt;% \n  mutate(score_hat  = int.model$fitted.values,\n         residual = int.model$residuals)\n\n\nResiduals vs. ageResiduals vs. fitted valuesHistograms & QQ plots\n\n\n\nggplot(int.model_output, aes(x = age, y = residual)) +\n  geom_point() +\n  labs(x = \"age\", y = \"Residual\") +\n  geom_hline(yintercept = 0, col = \"blue\", linewidth = 1) +\n  facet_wrap(~ gender)\n\n\n\n\n\n\n\n\n\n\n\ncheck_model(int.model,check = c(\"linearity\",\"homogeneity\"))\n\n\n\n\n\n\n\n\n\n\n\ncheck_model(int.model,check = c(\"qq\",\"normality\"))"
  },
  {
    "objectID": "slides/slides.html#which-model-do-we-select",
    "href": "slides/slides.html#which-model-do-we-select",
    "title": "Week 4: Regression modelling part 2",
    "section": "Which model do we select?",
    "text": "Which model do we select?"
  },
  {
    "objectID": "slides/slides.html#inference-using-confidence-intervals",
    "href": "slides/slides.html#inference-using-confidence-intervals",
    "title": "Week 4: Regression modelling part 2",
    "section": "Inference using Confidence Intervals",
    "text": "Inference using Confidence Intervals\n\nA confidence interval gives a range of plausible values for a population parameter.\nIt depends on a specified confidence level.\nInstead of estimating an unknown parameter by using a single point estimate/sample statistic we can use a range of possible values based around our sample statistic to make a plausible guess as to the location of the parameter."
  },
  {
    "objectID": "slides/slides.html#confidence-intervals-for-regression-parameters",
    "href": "slides/slides.html#confidence-intervals-for-regression-parameters",
    "title": "Week 4: Regression modelling part 2",
    "section": "Confidence Intervals for Regression Parameters",
    "text": "Confidence Intervals for Regression Parameters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nparallel lines model\ninteraction model\n\n\nPredictors\nEstimates\nstd. Error\nStatistic\np\nEstimates\nstd. Error\nStatistic\np\n\n\n(Intercept)\n4.48\n(4.24 – 4.73)\n0.13\n35.79\n&lt;0.001\n4.88\n(4.48 – 5.29)\n0.21\n23.80\n&lt;0.001\n\n\nage\n-0.01\n(-0.01 – -0.00)\n0.00\n-3.28\n0.001\n-0.02\n(-0.03 – -0.01)\n0.00\n-3.92\n&lt;0.001\n\n\ngender [male]\n0.19\n(0.09 – 0.29)\n0.05\n3.63\n&lt;0.001\n-0.45\n(-0.97 – 0.08)\n0.27\n-1.68\n0.094\n\n\nage × gender [male]\n\n\n\n\n0.01\n(0.00 – 0.02)\n0.01\n2.45\n0.015\n\n\nObservations\n463\n463\n\n\nR2 / R2 adjusted\n0.039 / 0.035\n0.051 / 0.045\n\n\n\n\n\n\n\n\n\n\nThe tables includes:\n\nstd_error: the standard error of each parameter estimate;\nstatistic: the test statistic value used to test the null hypothesis that the population parameter is zero;\np_value: the \\(p\\) value associated with the test statistic under the null hypothesis; and\nlower_ci and upper_ci: the lower and upper bounds of the 95% confidence interval for the population parameter"
  },
  {
    "objectID": "slides/slides.html#variable-selection-using-confidence-intervals",
    "href": "slides/slides.html#variable-selection-using-confidence-intervals",
    "title": "Week 4: Regression modelling part 2",
    "section": "Variable selection using confidence intervals",
    "text": "Variable selection using confidence intervals\n\nA confidence interval gives a range of plausible values for a population parameter.\nWhen there is more than one explanatory variable in a model, the parameter associated with each explanatory variable is interpreted as the change in the mean response based on a 1-unit change in the corresponding explanatory variable keeping all other variables held constant.\nWhen interpreting the confidence intervals of each parameter by acknowledging that each are plausible values conditional on all the other explanatory variables in the model.\n\nWe will introduce some of the ideas in the simple case where we have 2 potential explanatory variables (\\(x_1\\) and \\(x_2\\)) and use confidence intervals to decide which variables will be useful in predicting the response variable (\\(y\\))."
  },
  {
    "objectID": "slides/slides.html#variable-selection-using-confidence-intervals-1",
    "href": "slides/slides.html#variable-selection-using-confidence-intervals-1",
    "title": "Week 4: Regression modelling part 2",
    "section": "Variable selection using confidence intervals",
    "text": "Variable selection using confidence intervals\n\n\nOne approach is to consider a hierarchy of models:\n\\[y_i = \\alpha + \\beta_1 x_{1i} + \\beta_2 x_{2i}\\]\n\\[y_i = \\alpha + \\beta_1 x_{1i} \\qquad  \\qquad y_i = \\alpha + \\beta_2 x_{2i}\\]\n\\[y_i = \\alpha\\]\n\nWithin this structure we might take a top-down approach:\n\nFit the most general model.\nConstruct confidence intervals for \\(\\beta_1 ~\\textrm{and} ~\\beta_2\\)\n\nIf both intervals exclude 0 then retain the model with both \\(x_1\\) and \\(x_2\\).\nIf the interval for \\(\\beta_1\\) contains 0 but that for \\(\\beta_2\\) does not, fit the model with \\(x_2\\) alone.\nIf the interval for \\(\\beta_2\\) contains 0 but that for \\(\\beta_1\\) does not, fit the model with \\(x_1\\) alone.\nIf both intervals include 0 it may still be that a model with one variable is useful."
  },
  {
    "objectID": "slides/slides.html#example-variable-selection-using-confidence-intervals",
    "href": "slides/slides.html#example-variable-selection-using-confidence-intervals",
    "title": "Week 4: Regression modelling part 2",
    "section": "Example: variable selection using confidence intervals",
    "text": "Example: variable selection using confidence intervals\nRecall that as well as age and gender, there is also a potential explanatory variable bty_avg in the evals data, i.e. the numerical variable of the average beauty score from a panel of six students’ scores between 1 and 10.\n\nmlr.model &lt;- lm(score ~ age + bty_avg, data = evals) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nscore\n\n\nPredictors\nEstimates\nstd. Error\np\n\n\n(Intercept)\n4.05\n(3.72 – 4.39)\n0.17\n&lt;0.001\n\n\nage\n-0.00\n(-0.01 – 0.00)\n0.00\n0.251\n\n\nbty avg\n0.06\n(0.03 – 0.09)\n0.02\n&lt;0.001\n\n\nR2 / R2 adjusted\n0.038 / 0.034\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\nWhich variable should we drop?"
  },
  {
    "objectID": "slides/slides.html#model-comparisons-using-objective-criteria",
    "href": "slides/slides.html#model-comparisons-using-objective-criteria",
    "title": "Week 4: Regression modelling part 2",
    "section": "Model comparisons using objective criteria",
    "text": "Model comparisons using objective criteria\nWhen the number of potential explanatory variables is large the problem of selecting which variables to include in the final model becomes more difficult.\nWhen we do model selection we compromise:\n\nPredictive accuracy (improved by including more predictor/explanatory variables)\nInterpretability (achieved by having less predictor/explanatory variables)\n\nThere are many objective criteria for comparing different models applied to the same data set.\nAll of them trade off the two objectives above, i.e. fit to the data against complexity. Common examples include:"
  },
  {
    "objectID": "slides/slides.html#model-comparisons-using-objective-criteria-1",
    "href": "slides/slides.html#model-comparisons-using-objective-criteria-1",
    "title": "Week 4: Regression modelling part 2",
    "section": "Model comparisons using objective criteria",
    "text": "Model comparisons using objective criteria\n\nR-squaredAICBIC\n\n\nThe \\(R^2_{adj}\\) values, i.e. the proportion of the total variation of the response variable explained by the models.\n\\[R_{adj}^2 = 1 - \\frac{RSS/(n-p-1)}{SST/(n-1)} = 100 \\times \\Bigg[ 1-\\frac{\\sum_{i=1}^n(y_i-\\widehat{y}_i)^2/(n-p-1)}{\\sum_{i=1}^n(y_i-\\bar y_i)^2/(n-1)}\\Bigg]\\]\n\nwhere\n\n\\(n\\) is the sample size and \\(p\\) is the number of parameters in the model\n\\(RSS\\) is the residual sum of squares from the fitted model\n\\(SST\\) is the total sum of squares around the mean response.\n\n\\(R_{adj}^2\\) values are used for nested models, i.e. where one model is a particular case of the other\n\n\n\nAkaike’s Information Criteria (AIC)\n\\[AIC = -2 \\cdot \\text{log-likeihood} + 2p = n\\ln\\left(\\frac{RSS}{n}\\right)+2p\\]\n\nA value based on the maximum likelihood function of the parameters in the fitted model penalised by the number of parameters in the model\nIt be used to compare any models fitted to the same response variable\nThe smaller the AIC the ‘better’ the model, i.e. no distributional results are employed to assess differences\nSee the stepAIC function from the MASS library.\n\n\n\nBayesian Information Criteria\n\\[BIC = -2 \\cdot \\text{log-likeihood} +\\ln(n)p\\]\nA popular data analysis strategy that can be adopted is to calculate \\(R_{adj}^2\\), \\(AIC\\) and \\(BIC\\) and compare the models which minimise the \\(AIC\\) and \\(BIC\\) with the model that maximises the \\(R_{adj}^2\\)."
  },
  {
    "objectID": "slides/slides.html#example-model-comparisons-using-objective-criteria",
    "href": "slides/slides.html#example-model-comparisons-using-objective-criteria",
    "title": "Week 4: Regression modelling part 2",
    "section": "Example: Model comparisons using objective criteria",
    "text": "Example: Model comparisons using objective criteria\nTo illustrate this, let’s return to the evals data and the MLR on the teaching evaluation score score with the two continuous explanatory variables age and bty_avg and compare this with the SLR model with just bty_avg.\n\nlm(score ~ age, data = evals) %&gt;% # Model 1: score = age\nglance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0115       0.00931 0.541      5.34  0.0213     1  -372.  750.  762.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nlm(score ~ bty_avg, data = evals) %&gt;%  # Model 2: score = bty_avg\nglance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0350        0.0329 0.535      16.7 0.0000508     1  -366.  738.  751.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nlm(score ~ age + bty_avg, data = evals) %&gt;% # Model 3: score = age + bty_avg\nglance(mlr.model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0378        0.0336 0.535      9.03 0.000142     2  -366.  739.  756.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;"
  },
  {
    "objectID": "slides/slides.html#a-final-word-on-model-selection",
    "href": "slides/slides.html#a-final-word-on-model-selection",
    "title": "Week 4: Regression modelling part 2",
    "section": "A final word on model selection",
    "text": "A final word on model selection\n\nA great deal of care should be taken in selecting explanatory variables for a model because the values of the regression coefficients depend upon the variables included in the model.\nOne thing NOT to do is select hundreds of random predictors, bung them all into a regression analysis and hope for the best.\nThere are automatic strategies, such as Stepwise and Best Subsets regression, based on systematically searching through the entire list of variables not in the current model to make decisions on whether each should be included.\nThese strategies need to be handled with care,\n\nOur best strategy is a mixture of judgement on what variables should be included as potential explanatory variables, together with an interval estimation and hypothesis testing strategy for assessing these. The judgement should be made in the light of advice from the problem context."
  },
  {
    "objectID": "slides/slides.html#a-final-word-on-model-selection-1",
    "href": "slides/slides.html#a-final-word-on-model-selection-1",
    "title": "Week 4: Regression modelling part 2",
    "section": "A final word on model selection",
    "text": "A final word on model selection\n\n\n\n\n\n\nImportant\n\n\nGolden rule for modelling\nThe key to modelling data is to only use the objective measures as a rough guide. In the end the choice of model will involve your own judgement. You have to be able to defend why you chose a particular model."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Week 4 Tasks",
    "section": "",
    "text": "You are encouraged to complete the following tasks by using Quarto to produce a single document which summarises all your work, i.e. the original questions, your R code, your comments and reflections, etc.\n\nData was collected on the characteristics of homes in the American city of Los Angeles (LA) in 2010 and can be downloaded below:\n Download LAhomes dataset \nThe data contain the following variables:\n\ncity - the district of LA where the house was located\ntype - either SFR (Single Family Residences) or Condo/Twh (Condominium/Town House)\nbed - the number of bedrooms\nbath - the number of bathrooms\ngarage - the number of car spaces in the garage\nsqft - the floor area of the house (in square feet)\npool - Y if the house has a pool\nspa - TRUE if the house has a spa\nprice - the most recent sales price ($US)\n\nWe are interested in exploring the relationships between price and the other variables.\nRead the data into an object called LAhomes and complete the following tasks\n\nCodeLAhomes &lt;- read.csv(\"LAhomes.csv\",stringsAsFactors = T)\n\n\n\n\n\n\n\n\n Task 1\n\n\n\nBy looking at the univariate and bivariate distributions on the price and sqft variables below, what would be a sensible way to proceed if we wanted to model this data? What care must be taken if you were to proceed this way?\n\n\n\nClick here to see the solution\n\nCodehist1 &lt;- ggplot(LAhomes, aes(x = price)) +\n          geom_histogram()\nhist2 &lt;- ggplot(LAhomes, aes(x = sqft)) +\n          geom_histogram()\n\n# Explore log transformation\nhist1log &lt;- ggplot(LAhomes, aes(x = log(price))) +\n             geom_histogram()\nhist2log &lt;- ggplot(LAhomes, aes(x = log(sqft))) +\n             geom_histogram()\n\nplot1 &lt;- ggplot(LAhomes, aes(x = sqft, y = price)) +\n          geom_point()\nplot2 &lt;- ggplot(LAhomes, aes(x = log(sqft), y = log(price))) +\n          geom_point()\n\ngrid.arrange(hist1, hist2, hist1log, hist2log, plot1, plot2,\n             ncol = 2, nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task 2\n\n\n\nFit the simple linear model with log(price) as the response and log(sqft) as the predictor. Display the fitted model on a scatterplot of the data and construct a confidence interval for the slope parameter in the model and interpret its point and interval estimates.\n\n\n\nClick here to see the solution\n\nCodeslr_LAprices &lt;- lm(log(price) ~ log(sqft), data = LAhomes)\n\nggplot(LAhomes, aes(x =  log(sqft), y = log(price))) +\n  geom_point(alpha=0.25) +\n  labs(x = \"log(aquare feet)\", y = \"log(price)\") +\n  geom_smooth(method = \"lm\", level =0.95)\n\n\n\n\n\n\nCodetab_model(slr_LAprices)\n\n\n\n\n \nlog(price)\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n2.70\n2.42 – 2.98\n&lt;0.001\n\n\nsqft [log]\n1.44\n1.40 – 1.48\n&lt;0.001\n\n\nObservations\n1594\n\n\nR2 / R2 adjusted\n0.774 / 0.774\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task 3\n\n\n\nRe-do your analysis but now using log(bath) as the explanatory variable. Calculate the point and interval estimates of the coefficients.\n\n\n\nClick here to see the solution\n\nCodeslr_LAprices2 &lt;- lm(log(price) ~ log(bath), data = LAhomes)\n\nggplot(LAhomes, aes(x =  log(bath), y = log(price))) +\n  geom_point(alpha=0.25) +\n  labs(x = \"log(aquare feet)\", y = \"log(price)\") +\n  geom_smooth(method = \"lm\", level =0.95)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCodetab_model(slr_LAprices2)\n\n\n\n\n \nlog(price)\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n12.23\n12.18 – 12.29\n&lt;0.001\n\n\nbath [log]\n1.43\n1.37 – 1.49\n&lt;0.001\n\n\nObservations\n1594\n\n\nR2 / R2 adjusted\n0.577 / 0.577\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task 4\n\n\n\nFit the multiple linear regression model using the log transform of all the variables price (as the response) and both sqft and bath (as the explanatory variables). Calculate the point and interval estimates of the coefficients of the two predictors separately. Compare their point and interval estimates to those you calculated before. Can you account for the differences?\n\n\n\nClick here to see the solution\n\nCodemlr_LAprices &lt;- lm(log(price) ~ log(sqft) + log(bath), data = LAhomes)\ntab_model(mlr_LAprices,slr_LAprices,slr_LAprices2)\n\n\n\n\n \nlog(price)\nlog(price)\nlog(price)\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n2.51\n2.00 – 3.03\n&lt;0.001\n2.70\n2.42 – 2.98\n&lt;0.001\n12.23\n12.18 – 12.29\n&lt;0.001\n\n\nsqft [log]\n1.47\n1.39 – 1.55\n&lt;0.001\n1.44\n1.40 – 1.48\n&lt;0.001\n\n\n\n\n\nbath [log]\n-0.04\n-0.13 – 0.05\n0.389\n\n\n\n1.43\n1.37 – 1.49\n&lt;0.001\n\n\nObservations\n1594\n1594\n1594\n\n\nR2 / R2 adjusted\n0.774 / 0.774\n0.774 / 0.774\n0.577 / 0.577\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task 5\n\n\n\nUsing the objective measures for model comparisons, which of the models in task 2 ,3 and 4 would you favour? Is this consistent with your conclusions in task 4?\n\n\n\nClick here to see the solution\n\nCodetab_model(mlr_LAprices,slr_LAprices,slr_LAprices2,show.aic = T)\n\n\n\n\n \nlog(price)\nlog(price)\nlog(price)\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n2.51\n2.00 – 3.03\n&lt;0.001\n2.70\n2.42 – 2.98\n&lt;0.001\n12.23\n12.18 – 12.29\n&lt;0.001\n\n\nsqft [log]\n1.47\n1.39 – 1.55\n&lt;0.001\n1.44\n1.40 – 1.48\n&lt;0.001\n\n\n\n\n\nbath [log]\n-0.04\n-0.13 – 0.05\n0.389\n\n\n\n1.43\n1.37 – 1.49\n&lt;0.001\n\n\nObservations\n1594\n1594\n1594\n\n\nR2 / R2 adjusted\n0.774 / 0.774\n0.774 / 0.774\n0.577 / 0.577\n\n\nAIC\n44587.722\n44586.467\n45584.113\n\n\n\n\n\n\n\n\n\nYou have been asked to determine the pricing of a New York City (NYC) Italian restaurant’s dinner menu such that it is competitively positioned with other high-end Italian restaurants by analysing pricing data that have been collected in order to produce a regression model to predict the price of dinner.\nData from surveys of customers of 168 Italian restaurants in the target area are available. The data can be found in the file restNYC.csv.\n Download restNYC dataset \nEach row represents one customer survey from Italian restaurants in NYC and includes the key variables:\n\n\nPrice - price (in $US) of dinner (including a tip and one drink)\n\nFood - customer rating of the food (from 1 to 30)\n\nDecor - customer rating of the decor (from 1 to 30)\n\nService - customer rating of the service (from 1 to 30)\n\nEast - dummy variable with the value 1 if the restaurant is east of Fifth Avenue, 0 otherwise\n\n\nCoderestNYC &lt;- read.csv(\"restNYC.csv\",stringsAsFactors = T)\n\n\n\n\n\n\n\n\n Task 6\n\n\n\nUsing the ggpairs function in the GGally package (see the following code) we can generate an informative set of graphical and numerical summaries which illuminate the relationships between pairs of variables. Where do you see the strongest evidence of relationships between price and the potential explanatory variables? Is there evidence of multicollineatity in the data?\n\nlibrary(GGally) # Package to produce matrix of 'pairs' plots and more!\nrestNYC$East &lt;- as.factor(restNYC$East) # East needs to be a factor\n# Including the `East` factor\n# ggpairs(restNYC[, 4:8], aes(fill = East, alpha = 0.4),progress = F) \n\n# Without the `East` factor\nggpairs(restNYC[, 4:7], aes(alpha = 0.4),progress = F) \n\n\n\n\n\n\n\n\n\nSolution\n\nThere seems to be a strong positive linear association between service and food, which could lead to some collinearity issues.\n\n\n\n\n\n\n\n\n\n Task 7\n\n\n\nFit the simple linear model with Price as the response and Service as the predictor and display the fitted model on a scatterplot of the data. Construct a confidence interval for the slope parameter in the model.\n\n\n\nClick here to see the solution\n\nCodeprice_serv_LM &lt;- lm(Price  ~ Service  , data = restNYC)\nggplot(restNYC, aes(x = Service, y = Price)) +\n  geom_point() +\n  geom_jitter() +\n  labs(x = \"Service score\", y = \"Price\")+\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\nCodebroom::tidy(price_serv_LM,conf.int = T)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -12.0      5.11      -2.34 2.02e- 2   -22.1      -1.89\n2 Service         2.82     0.262     10.8  7.88e-21     2.30      3.34\n\n\n\n\n\n\n\n\n\n\n\n Task 8\n\n\n\nNow fit a multiple regressing model of Price on Service, Food, and Decor. What happens to the significance of Service when additional variables were added to the model?\n\n\n\nClick here to see the solution\n\nCodeprice_serv_MLR &lt;- lm(Price  ~ Service + Food + Decor , data = restNYC)\ntab_model(price_serv_MLR,collapse.ci = T)\n\n\n\n\n\n\n\n\n\n\n \nPrice\n\n\nPredictors\nEstimates\np\n\n\n(Intercept)\n-24.64\n(-34.03 – -15.25)\n&lt;0.001\n\n\nService\n0.14\n(-0.65 – 0.92)\n0.733\n\n\nFood\n1.56\n(0.82 – 2.29)\n&lt;0.001\n\n\nDecor\n1.85\n(1.42 – 2.28)\n&lt;0.001\n\n\nObservations\n168\n\n\nR2 / R2 adjusted\n0.617 / 0.610\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task 9\n\n\n\nWhat is the correct interpretation of the coefficient on Service in the linear model which regresses Price on Service, Food, and Decor?\n\n\nSee solution\n\nAfter controlling for Food and Decor, a 1-point increase in the Service rating is associated with an estimated $0.135 increase in the. average Price, but this effect is not statistically significant (p \\(&gt;\\) 0.05)"
  },
  {
    "objectID": "about.html#part-1",
    "href": "about.html#part-1",
    "title": "Week 4 Tasks",
    "section": "",
    "text": "Data was collected on the characteristics of homes in the American city of Los Angeles (LA) in 2010 and can be downloaded below:\n Download LAhomes dataset \nThe data contain the following variables:\n\ncity - the district of LA where the house was located\ntype - either SFR (Single Family Residences) or Condo/Twh (Condominium/Town House)\nbed - the number of bedrooms\nbath - the number of bathrooms\ngarage - the number of car spaces in the garage\nsqft - the floor area of the house (in square feet)\npool - Y if the house has a pool\nspa - TRUE if the house has a spa\nprice - the most recent sales price ($US)\n\nWe are interested in exploring the relationships between price and the other variables.\nRead the data into an object called LAhomes and complete the following tasks\n\nCodeLAhomes &lt;- read.csv(\"LAhomes.csv\",stringsAsFactors = T)\n\n\n\n\n\n\n\n\n Task 1\n\n\n\nBy looking at the univariate and bivariate distributions on the price and sqft variables below, what would be a sensible way to proceed if we wanted to model this data? What care must be taken if you were to proceed this way?\n\n\n\nClick here to see the solution\n\nCodehist1 &lt;- ggplot(LAhomes, aes(x = price)) +\n          geom_histogram()\nhist2 &lt;- ggplot(LAhomes, aes(x = sqft)) +\n          geom_histogram()\n\n# Explore log transformation\nhist1log &lt;- ggplot(LAhomes, aes(x = log(price))) +\n             geom_histogram()\nhist2log &lt;- ggplot(LAhomes, aes(x = log(sqft))) +\n             geom_histogram()\n\nplot1 &lt;- ggplot(LAhomes, aes(x = sqft, y = price)) +\n          geom_point()\nplot2 &lt;- ggplot(LAhomes, aes(x = log(sqft), y = log(price))) +\n          geom_point()\n\ngrid.arrange(hist1, hist2, hist1log, hist2log, plot1, plot2,\n             ncol = 2, nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task 2\n\n\n\nFit the simple linear model with log(price) as the response and log(sqft) as the predictor. Display the fitted model on a scatterplot of the data and construct a confidence interval for the slope parameter in the model and interpret its point and interval estimates.\n\n\n\nClick here to see the solution\n\nCodeslr_LAprices &lt;- lm(log(price) ~ log(sqft), data = LAhomes)\n\nggplot(LAhomes, aes(x =  log(sqft), y = log(price))) +\n  geom_point(alpha=0.25) +\n  labs(x = \"log(aquare feet)\", y = \"log(price)\") +\n  geom_smooth(method = \"lm\", level =0.95)\n\n\n\n\n\n\nCodetab_model(slr_LAprices)\n\n\n\n\n \nlog(price)\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n2.70\n2.42 – 2.98\n&lt;0.001\n\n\nsqft [log]\n1.44\n1.40 – 1.48\n&lt;0.001\n\n\nObservations\n1594\n\n\nR2 / R2 adjusted\n0.774 / 0.774\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task 3\n\n\n\nRe-do your analysis but now using log(bath) as the explanatory variable. Calculate the point and interval estimates of the coefficients.\n\n\n\nClick here to see the solution\n\nCodeslr_LAprices2 &lt;- lm(log(price) ~ log(bath), data = LAhomes)\n\nggplot(LAhomes, aes(x =  log(bath), y = log(price))) +\n  geom_point(alpha=0.25) +\n  labs(x = \"log(aquare feet)\", y = \"log(price)\") +\n  geom_smooth(method = \"lm\", level =0.95)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCodetab_model(slr_LAprices2)\n\n\n\n\n \nlog(price)\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n12.23\n12.18 – 12.29\n&lt;0.001\n\n\nbath [log]\n1.43\n1.37 – 1.49\n&lt;0.001\n\n\nObservations\n1594\n\n\nR2 / R2 adjusted\n0.577 / 0.577\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task 4\n\n\n\nFit the multiple linear regression model using the log transform of all the variables price (as the response) and both sqft and bath (as the explanatory variables). Calculate the point and interval estimates of the coefficients of the two predictors separately. Compare their point and interval estimates to those you calculated before. Can you account for the differences?\n\n\n\nClick here to see the solution\n\nCodemlr_LAprices &lt;- lm(log(price) ~ log(sqft) + log(bath), data = LAhomes)\ntab_model(mlr_LAprices,slr_LAprices,slr_LAprices2)\n\n\n\n\n \nlog(price)\nlog(price)\nlog(price)\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n2.51\n2.00 – 3.03\n&lt;0.001\n2.70\n2.42 – 2.98\n&lt;0.001\n12.23\n12.18 – 12.29\n&lt;0.001\n\n\nsqft [log]\n1.47\n1.39 – 1.55\n&lt;0.001\n1.44\n1.40 – 1.48\n&lt;0.001\n\n\n\n\n\nbath [log]\n-0.04\n-0.13 – 0.05\n0.389\n\n\n\n1.43\n1.37 – 1.49\n&lt;0.001\n\n\nObservations\n1594\n1594\n1594\n\n\nR2 / R2 adjusted\n0.774 / 0.774\n0.774 / 0.774\n0.577 / 0.577\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task 5\n\n\n\nUsing the objective measures for model comparisons, which of the models in task 2 ,3 and 4 would you favour? Is this consistent with your conclusions in task 4?\n\n\n\nClick here to see the solution\n\nCodetab_model(mlr_LAprices,slr_LAprices,slr_LAprices2,show.aic = T)\n\n\n\n\n \nlog(price)\nlog(price)\nlog(price)\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n2.51\n2.00 – 3.03\n&lt;0.001\n2.70\n2.42 – 2.98\n&lt;0.001\n12.23\n12.18 – 12.29\n&lt;0.001\n\n\nsqft [log]\n1.47\n1.39 – 1.55\n&lt;0.001\n1.44\n1.40 – 1.48\n&lt;0.001\n\n\n\n\n\nbath [log]\n-0.04\n-0.13 – 0.05\n0.389\n\n\n\n1.43\n1.37 – 1.49\n&lt;0.001\n\n\nObservations\n1594\n1594\n1594\n\n\nR2 / R2 adjusted\n0.774 / 0.774\n0.774 / 0.774\n0.577 / 0.577\n\n\nAIC\n44587.722\n44586.467\n45584.113"
  },
  {
    "objectID": "about.html#part-2.",
    "href": "about.html#part-2.",
    "title": "Week 4 Tasks",
    "section": "",
    "text": "You have been asked to determine the pricing of a New York City (NYC) Italian restaurant’s dinner menu such that it is competitively positioned with other high-end Italian restaurants by analysing pricing data that have been collected in order to produce a regression model to predict the price of dinner.\nData from surveys of customers of 168 Italian restaurants in the target area are available. The data can be found in the file restNYC.csv.\n Download restNYC dataset \nEach row represents one customer survey from Italian restaurants in NYC and includes the key variables:\n\n\nPrice - price (in $US) of dinner (including a tip and one drink)\n\nFood - customer rating of the food (from 1 to 30)\n\nDecor - customer rating of the decor (from 1 to 30)\n\nService - customer rating of the service (from 1 to 30)\n\nEast - dummy variable with the value 1 if the restaurant is east of Fifth Avenue, 0 otherwise\n\n\nCoderestNYC &lt;- read.csv(\"restNYC.csv\",stringsAsFactors = T)\n\n\n\n\n\n\n\n\n Task 6\n\n\n\nUsing the ggpairs function in the GGally package (see the following code) we can generate an informative set of graphical and numerical summaries which illuminate the relationships between pairs of variables. Where do you see the strongest evidence of relationships between price and the potential explanatory variables? Is there evidence of multicollineatity in the data?\n\nlibrary(GGally) # Package to produce matrix of 'pairs' plots and more!\nrestNYC$East &lt;- as.factor(restNYC$East) # East needs to be a factor\n# Including the `East` factor\n# ggpairs(restNYC[, 4:8], aes(fill = East, alpha = 0.4),progress = F) \n\n# Without the `East` factor\nggpairs(restNYC[, 4:7], aes(alpha = 0.4),progress = F) \n\n\n\n\n\n\n\n\n\nSolution\n\nThere seems to be a strong positive linear association between service and food, which could lead to some collinearity issues.\n\n\n\n\n\n\n\n\n\n Task 7\n\n\n\nFit the simple linear model with Price as the response and Service as the predictor and display the fitted model on a scatterplot of the data. Construct a confidence interval for the slope parameter in the model.\n\n\n\nClick here to see the solution\n\nCodeprice_serv_LM &lt;- lm(Price  ~ Service  , data = restNYC)\nggplot(restNYC, aes(x = Service, y = Price)) +\n  geom_point() +\n  geom_jitter() +\n  labs(x = \"Service score\", y = \"Price\")+\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\nCodebroom::tidy(price_serv_LM,conf.int = T)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -12.0      5.11      -2.34 2.02e- 2   -22.1      -1.89\n2 Service         2.82     0.262     10.8  7.88e-21     2.30      3.34\n\n\n\n\n\n\n\n\n\n\n\n Task 8\n\n\n\nNow fit a multiple regressing model of Price on Service, Food, and Decor. What happens to the significance of Service when additional variables were added to the model?\n\n\n\nClick here to see the solution\n\nCodeprice_serv_MLR &lt;- lm(Price  ~ Service + Food + Decor , data = restNYC)\ntab_model(price_serv_MLR,collapse.ci = T)\n\n\n\n\n\n\n\n\n\n\n \nPrice\n\n\nPredictors\nEstimates\np\n\n\n(Intercept)\n-24.64\n(-34.03 – -15.25)\n&lt;0.001\n\n\nService\n0.14\n(-0.65 – 0.92)\n0.733\n\n\nFood\n1.56\n(0.82 – 2.29)\n&lt;0.001\n\n\nDecor\n1.85\n(1.42 – 2.28)\n&lt;0.001\n\n\nObservations\n168\n\n\nR2 / R2 adjusted\n0.617 / 0.610\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task 9\n\n\n\nWhat is the correct interpretation of the coefficient on Service in the linear model which regresses Price on Service, Food, and Decor?\n\n\nSee solution\n\nAfter controlling for Food and Decor, a 1-point increase in the Service rating is associated with an estimated $0.135 increase in the. average Price, but this effect is not statistically significant (p \\(&gt;\\) 0.05)"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Regression modelling part 2",
    "section": "",
    "text": "Let’s expand upon what we learned last week by revisiting the instructor evaluation data set evals. In Week 3 you were tasked with examining the relationship between teaching score (score) and age (age). Now, let’s also introduce the additional (binary) categorical explanatory variable gender (gender). That is, we we will be examining:\n\nthe teaching score (score) as our outcome variable \\(y\\);\nage (age) as our numerical explanatory variable \\(x_1\\); and\ngender (gender) as our categorical explanatory variable \\(x_2\\).\n\nThe data can be downloaded below:\n Download evals dataset \nYou can download today’s session R script below:\n Download Week 4 R script \n\nBefore we begin, lets load the following packages into R:\n\nCodelibrary(tidyverse)    # Data wrangling \nlibrary(ggplot2)      # Data visualization\nlibrary(performance)  # Model assessment\nlibrary(skimr)        # Exploratory analysis\nlibrary(sjPlot)       # Plot and tables for linear models\nlibrary(broom)        # Linear model tidy summaries\n\n\nNow, lets start by subsetting the evals data set so that we only have the variables we are interested in, that is, score, age and gender.\n\n\n\n\n\n\nNote\n\n\n\nIt is best to give your new data set a different name than evals as to not overwrite the original evals data set. Your new data set should look like the one below.\n\n\nFirst, we read the data using the read.csv() function with stringsAsFactors = TRUE to automatically convert categorical variables into factors:\n\nCodeevals &lt;- read.csv(\"evals.csv\",stringsAsFactors = T)\neval.score &lt;- evals %&gt;%\n  dplyr::select(c(score,age,gender))\n\n\n\n\n\n\n\n\n Task 1\n\n\n\nYou can also view your data set using the glimpse function, or by opening a spreadsheet view in RStudio using the View function. Use the skim function to obtain some summary statistics from our data.\n\n\n\nClick here to see the solution\n\nCodeeval.score %&gt;%\n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n463\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\ngender\n0\n1\nFALSE\n2\nmal: 268, fem: 195\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nscore\n0\n1\n4.17\n0.54\n2.3\n3.8\n4.3\n4.6\n5\n▁▁▅▇▇\n\n\nage\n0\n1\n48.37\n9.80\n29.0\n42.0\n48.0\n57.0\n73\n▅▆▇▆▁\n\n\n\n\n\n\n\n\nNow, let’s compute the correlation between our outcome variable score and our numerical explanatory variable age:\n\nCodeeval.score %&gt;%\n  summarise(rho = cor(score,age))\n\n        rho\n1 -0.107032\n\n\n\n\n\n\n\n\n Question\n\n\n\nWhy do we not include the categorical variable gender when calculating the correlation?\n\n\nAnswer\n\nThe correlation coefficient only exists between numerical variables, which is why we do not include our categorical variable gender.\n\n\n\nFurthermore, we can obtain the correlation for each gender as follows:\n\nCodeeval.score %&gt;%\n  summarise(rho = cor(score,age),\n            .by = gender)\n\n  gender         rho\n1 female -0.26517575\n2   male -0.07645422\n\n\nFrom this we can tell that the negative linear association between age and teaching score appears to be larger for women than it does for men, i.e. the teaching score of women drops faster with age.\nWe can now visualize our data by producing a scatterplot, where seeing as we have the categorical variable gender, we shall plot the points using different colours for each gender:\n\nCodeggplot(eval.score, \n       aes(x = age, y = score, color = gender)) +\n  geom_jitter() +\n  labs(x = \"Age\", y = \"Teaching Score\", color = \"Gender\") \n\n\n\nInstructor evaluation scores by age and gender. The points have been jittered.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe above code has jittered the points, however, this is not necessary and geom_point would suffice. To plot separate points by gender we simply add the color argument to the aes function and pass to it gender.\n\n\nFrom the scatterplot we can see that there are very few women over the age of 60 in our data set and that the variability for women seems to be slighlty larger than for men.\n\nHere, we shall begin by fitting what is referred to as a parallel regression lines model. This model implies that the slope of relationship between teaching score (score) and age (age) is the same for both males and females, with only the intercept of the regression lines changing. Hence, our parallel regression lines model is given as:\n\\[\n\\begin{aligned}\ny_{i} &= \\alpha + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i \\nonumber \\\\&= \\alpha + \\beta_{\\text{age}} \\cdot \\text{age} + \\beta_{\\text{male}} \\cdot \\mathbb{I}_{\\text{male}}(x) + \\epsilon_i,\n\\end{aligned}\n\\tag{1}\\]\nwhere\n\n\\(\\alpha\\) is the intercept of the regression line for females;\n\\(\\beta_{\\text{age}}\\) is the slope of the regression line for both males and females;\n\\(\\beta_{\\text{male}}\\) is the additional term added to \\(\\alpha\\) to get the intercept of the regression line for males; and\n\n\\(\\mathbb{I}_{\\text{male}}(x)\\) is an indicator function such that\n\\[\\mathbb{I}_{\\text{male}}(x)=\\left\\{\n            \\begin{array}{ll}\n              1 ~~~ \\text{if gender} ~ x ~ \\text{is male},\\\\\n              0 ~~~ \\text{Otherwise}.\\\\\n            \\end{array}\n          \\right.\\]\n\n\nWe can fit the parallel regression lines model as follows:\n\nCodepar.model &lt;- lm(score ~ age + gender, data = eval.score)\ntab_model(par.model,show.ci = F)\n\n\n\n\n \nscore\n\n\nPredictors\nEstimates\np\n\n\n(Intercept)\n4.48\n&lt;0.001\n\n\nage\n-0.01\n0.001\n\n\ngender [male]\n0.19\n&lt;0.001\n\n\nObservations\n463\n\n\nR2 / R2 adjusted\n0.039 / 0.035\n\n\n\n\n\nHence, the regression line for females is given by:\n\\[\\widehat{\\text{score}} = 4.48 - 0.01 \\cdot \\text{age},\\]\nwhile the regression line for males is given by:\n\\[\\widehat{\\text{score}} = 4.48 - 0.01 \\cdot \\text{age} + 0.191 = 4.671 - 0.009 \\cdot \\text{age}.\\]\nNow, let’s superimpose our parallel regression lines onto the scatterplot of teaching score against age. To do so we will use the plot_model function from the sjPlot library as follows:\n\nCodeplot_model(model = par.model,                 # The fitted model\n           type=\"pred\",               # type of plot\n           terms = c(\"age\",\"gender\"), # terms to include in the plot\n           grid=T,                    # split into a grid\n           show.data = T,             # show observations\n           jitter=T,                  # jitter the points\n           ci.lvl = NA,               # show/hide confidence intervals\n           title=\"Parallel Regression Model fitted lines\") # Plot title\n\n\n\nInstructor evaluation scores by age and gender with parallel regression lines superimposed.\n\n\n\nHere is a short description of the plot_model function arguments we have used:\n\nmodel is the fitted lm-class model.\ntype = \"pred\" plot the predicted values for specific model terms.\nterms = c(\"age\",\"gender\") the terms to be plotted\ngrid=T logical argument to plot the different fitted lines for each group (i.e., gender) on different panels\nshow.data=T and jitter=T logical arguments to show our observations and jitter the data points.\nci.lvl set to NA to hide the confidence bands (we will talk more about this later)\ntitle title for the plot.\n\nFrom the parallel regression lines model both males and females have the same slope, that is, the associated effect of age on teaching score is the same for both men and women. Hence, for every one year increase in age, there is an associated decrease in teaching score of 0.009. However, male instructors have a higher intercept term, that is, there is a vertical bump in the regression line for males in teaching scores. This is linked to the average difference in teaching scores that males obtain relative to females.\n\nThere is an interaction effect if the associated effect of one variable depends on the value of another variable. For example, the effect of age here will depend on whether the instructor is male or female, that is, the effect of age on teaching scores will differ by gender. The interaction model can be written as:\n\\[\n\\begin{aligned}\ny_{i} &= \\alpha + \\beta_1  x_{1i} + \\beta_2  x_{2i} + \\beta_3  x_{1i}  x_{2i} + \\epsilon_i \\nonumber \\\\&= \\alpha + \\beta_{\\text{age}} \\cdot \\text{age} + \\beta_{\\text{male}} \\cdot \\mathbb{I}_{\\text{male}}(x) + \\beta_{\\text{age, male}} \\cdot \\text{age} \\cdot \\mathbb{I}_{\\text{male}}(x) + \\epsilon_i,\n\\end{aligned}\n\\tag{2}\\]\nwhere \\(\\beta_{\\text{age, male}} \\cdot \\text{age} \\cdot \\mathbb{I}_{\\text{male}}(x)\\) corresponds to the interaction term.\nIn order to fit an interaction term within our regression model we replace the + sign with the * sign as follows:\n\nCodeint.model &lt;-lm(score ~ age * gender, data = eval.score)\ntab_model(int.model,show.ci = F)\n\n\n\n\n \nscore\n\n\nPredictors\nEstimates\np\n\n\n(Intercept)\n4.88\n&lt;0.001\n\n\nage\n-0.02\n&lt;0.001\n\n\ngender [male]\n-0.45\n0.094\n\n\nage × gender [male]\n0.01\n0.015\n\n\nObservations\n463\n\n\nR2 / R2 adjusted\n0.051 / 0.045\n\n\n\n\n\nHence, the regression line for females is given by:\n\\[\\widehat{\\text{score}} = 4.88 - 0.018 \\cdot \\text{age},\\] while the regression line for males is given by:\n\\[\\widehat{\\text{score}} = 4.88 - 0.018 \\cdot \\text{age} - 0.446 + 0.014 \\cdot \\text{age} = 4.434 - 0.004 \\cdot \\text{age}.\\]\n\n\n\n\n\n\n Question\n\n\n\nHow do they compare with the teaching score values from the parallel regression lines model?\n\n\nAnswer\n\nHere, we can see that, although the intercept for male instructors may be lower, the associated average decrease in teaching score with age (0.004) is not as severe as it is for female instructors (0.018).\n\n\n\nWe can plot the fitted model as we did before with the plot_model function. Lets try it now without the panel option (i.e., set grid = F) and selecting our color scheme:\n\nCodeplot_model(model = int.model,         # The fitted intertaction model\n           type=  \"pred\",              # type of plot\n           terms = c(\"age\",\"gender\"), # terms to include in the plot\n           grid=F,                    # split into a grid\n           show.data = T,             # show observations\n           jitter=T,                  # jitter the points\n           ci.lvl = NA,               # show/hide confidence intervals\n           title =\"Gender-age interaction model fitted lines\", # Plot title\n           colors = c(\"purple\", \"orange\")) # color scheme\n\n\n\nInstructor evaluation scores by age and gender with gender-varying regression lines superimposed.\n\n\n\n\nNow we have to assess the fit of the model by looking at plots of the residuals. We shall do this for the interaction model. First, we need to obtain the fitted values and residuals from the interaction model as follows:\n\nCodeint.model_output &lt;-  eval.score %&gt;% \n  mutate(score_hat  = int.model$fitted.values,\n         residual = int.model$residuals)\n\n\nLet’s start by looking at a scatterplot of the residuals against the explanatory variable by gender:\n\nCodeggplot(int.model_output, aes(x = age, y = residual)) +\n  geom_point() +\n  labs(x = \"age\", y = \"Residual\") +\n  geom_hline(yintercept = 0, col = \"blue\", linewidth = 1) +\n  facet_wrap(~ gender)\n\n\n\nResiduals vs the explanatory variable age by gender.\n\n\n\nNow, we can plot the residuals against the fitted values using either the check_model function from the performance package or the plot_model() from sjPlot by setting type=\"diag\":\n\n\nUsing check_model\nUsing plot_model\n\n\n\n\nCodecheck_model(int.model,check = c(\"linearity\",\"homogeneity\"))\n\n\n\n\n\n\n\n\n\n\nCodeint.model_diag &lt;- plot_model(int.model,type = \"diag\")\nint.model_diag[[4]]\n\n\n\n\n\n\n\n\n\n\n\nFinally, let’s plot histograms of the residuals and QQ-plots to assess whether they are normally distributed with mean zero:\n\n\nUsing check_model\nUsing plot_model\n\n\n\n\nCodecheck_model(int.model,check = c(\"qq\",\"normality\"))\n\n\n\n\n\n\n\n\n\n\nCodelibrary(gridExtra) # to arrange the plots side-by-side\nint.model_diag &lt;- plot_model(int.model,type = \"diag\")\n\ngridExtra::grid.arrange(int.model_diag[[2]], # qqplot\n                        int.model_diag[[3]], # histogram\n                        ncol=2) # plot them side by side\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task 2\n\n\n\nUsing ggplot to produce manually:\n\na scatter plot of the residuals vs. fitted values\nhistogram of the residuals to assess normality\n\n\n\nTake a hint\n\nThe int.model_output data we produced above contains all the information you need. Recall that a scatterplots and histograms can be produced with geom_point and geom_histogram layers respectively.\n\n\n\n\nClick here to see the solution\n\nCodep1 &lt;- ggplot(int.model_output,\n             aes(y=residual,x=score_hat))+\n  geom_point()+\n  labs(y=\"Residuals\",x=\"Fitted values\")\np2 &lt;- ggplot(int.model_output,\n             aes(x=residual))+\n  geom_histogram()\ngridExtra::grid.arrange(p1, # resid. vs. fitted\n                        p2, # histogram\n                        ncol=2)"
  },
  {
    "objectID": "notes.html#exploratory-data-analysis",
    "href": "notes.html#exploratory-data-analysis",
    "title": "Regression modelling part 2",
    "section": "",
    "text": "Before we begin, lets load the following packages into R:\n\nCodelibrary(tidyverse)    # Data wrangling \nlibrary(ggplot2)      # Data visualization\nlibrary(performance)  # Model assessment\nlibrary(skimr)        # Exploratory analysis\nlibrary(sjPlot)       # Plot and tables for linear models\nlibrary(broom)        # Linear model tidy summaries\n\n\nNow, lets start by subsetting the evals data set so that we only have the variables we are interested in, that is, score, age and gender.\n\n\n\n\n\n\nNote\n\n\n\nIt is best to give your new data set a different name than evals as to not overwrite the original evals data set. Your new data set should look like the one below.\n\n\nFirst, we read the data using the read.csv() function with stringsAsFactors = TRUE to automatically convert categorical variables into factors:\n\nCodeevals &lt;- read.csv(\"evals.csv\",stringsAsFactors = T)\neval.score &lt;- evals %&gt;%\n  dplyr::select(c(score,age,gender))\n\n\n\n\n\n\n\n\n Task 1\n\n\n\nYou can also view your data set using the glimpse function, or by opening a spreadsheet view in RStudio using the View function. Use the skim function to obtain some summary statistics from our data.\n\n\n\nClick here to see the solution\n\nCodeeval.score %&gt;%\n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n463\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\ngender\n0\n1\nFALSE\n2\nmal: 268, fem: 195\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nscore\n0\n1\n4.17\n0.54\n2.3\n3.8\n4.3\n4.6\n5\n▁▁▅▇▇\n\n\nage\n0\n1\n48.37\n9.80\n29.0\n42.0\n48.0\n57.0\n73\n▅▆▇▆▁\n\n\n\n\n\n\n\n\nNow, let’s compute the correlation between our outcome variable score and our numerical explanatory variable age:\n\nCodeeval.score %&gt;%\n  summarise(rho = cor(score,age))\n\n        rho\n1 -0.107032\n\n\n\n\n\n\n\n\n Question\n\n\n\nWhy do we not include the categorical variable gender when calculating the correlation?\n\n\nAnswer\n\nThe correlation coefficient only exists between numerical variables, which is why we do not include our categorical variable gender.\n\n\n\nFurthermore, we can obtain the correlation for each gender as follows:\n\nCodeeval.score %&gt;%\n  summarise(rho = cor(score,age),\n            .by = gender)\n\n  gender         rho\n1 female -0.26517575\n2   male -0.07645422\n\n\nFrom this we can tell that the negative linear association between age and teaching score appears to be larger for women than it does for men, i.e. the teaching score of women drops faster with age.\nWe can now visualize our data by producing a scatterplot, where seeing as we have the categorical variable gender, we shall plot the points using different colours for each gender:\n\nCodeggplot(eval.score, \n       aes(x = age, y = score, color = gender)) +\n  geom_jitter() +\n  labs(x = \"Age\", y = \"Teaching Score\", color = \"Gender\") \n\n\n\nInstructor evaluation scores by age and gender. The points have been jittered.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe above code has jittered the points, however, this is not necessary and geom_point would suffice. To plot separate points by gender we simply add the color argument to the aes function and pass to it gender.\n\n\nFrom the scatterplot we can see that there are very few women over the age of 60 in our data set and that the variability for women seems to be slighlty larger than for men."
  },
  {
    "objectID": "notes.html#multiple-regression-parallel-slopes-model",
    "href": "notes.html#multiple-regression-parallel-slopes-model",
    "title": "Regression modelling part 2",
    "section": "",
    "text": "Here, we shall begin by fitting what is referred to as a parallel regression lines model. This model implies that the slope of relationship between teaching score (score) and age (age) is the same for both males and females, with only the intercept of the regression lines changing. Hence, our parallel regression lines model is given as:\n\\[\n\\begin{aligned}\ny_{i} &= \\alpha + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i \\nonumber \\\\&= \\alpha + \\beta_{\\text{age}} \\cdot \\text{age} + \\beta_{\\text{male}} \\cdot \\mathbb{I}_{\\text{male}}(x) + \\epsilon_i,\n\\end{aligned}\n\\tag{1}\\]\nwhere\n\n\\(\\alpha\\) is the intercept of the regression line for females;\n\\(\\beta_{\\text{age}}\\) is the slope of the regression line for both males and females;\n\\(\\beta_{\\text{male}}\\) is the additional term added to \\(\\alpha\\) to get the intercept of the regression line for males; and\n\n\\(\\mathbb{I}_{\\text{male}}(x)\\) is an indicator function such that\n\\[\\mathbb{I}_{\\text{male}}(x)=\\left\\{\n            \\begin{array}{ll}\n              1 ~~~ \\text{if gender} ~ x ~ \\text{is male},\\\\\n              0 ~~~ \\text{Otherwise}.\\\\\n            \\end{array}\n          \\right.\\]\n\n\nWe can fit the parallel regression lines model as follows:\n\nCodepar.model &lt;- lm(score ~ age + gender, data = eval.score)\ntab_model(par.model,show.ci = F)\n\n\n\n\n \nscore\n\n\nPredictors\nEstimates\np\n\n\n(Intercept)\n4.48\n&lt;0.001\n\n\nage\n-0.01\n0.001\n\n\ngender [male]\n0.19\n&lt;0.001\n\n\nObservations\n463\n\n\nR2 / R2 adjusted\n0.039 / 0.035\n\n\n\n\n\nHence, the regression line for females is given by:\n\\[\\widehat{\\text{score}} = 4.48 - 0.01 \\cdot \\text{age},\\]\nwhile the regression line for males is given by:\n\\[\\widehat{\\text{score}} = 4.48 - 0.01 \\cdot \\text{age} + 0.191 = 4.671 - 0.009 \\cdot \\text{age}.\\]\nNow, let’s superimpose our parallel regression lines onto the scatterplot of teaching score against age. To do so we will use the plot_model function from the sjPlot library as follows:\n\nCodeplot_model(model = par.model,                 # The fitted model\n           type=\"pred\",               # type of plot\n           terms = c(\"age\",\"gender\"), # terms to include in the plot\n           grid=T,                    # split into a grid\n           show.data = T,             # show observations\n           jitter=T,                  # jitter the points\n           ci.lvl = NA,               # show/hide confidence intervals\n           title=\"Parallel Regression Model fitted lines\") # Plot title\n\n\n\nInstructor evaluation scores by age and gender with parallel regression lines superimposed.\n\n\n\nHere is a short description of the plot_model function arguments we have used:\n\nmodel is the fitted lm-class model.\ntype = \"pred\" plot the predicted values for specific model terms.\nterms = c(\"age\",\"gender\") the terms to be plotted\ngrid=T logical argument to plot the different fitted lines for each group (i.e., gender) on different panels\nshow.data=T and jitter=T logical arguments to show our observations and jitter the data points.\nci.lvl set to NA to hide the confidence bands (we will talk more about this later)\ntitle title for the plot.\n\nFrom the parallel regression lines model both males and females have the same slope, that is, the associated effect of age on teaching score is the same for both men and women. Hence, for every one year increase in age, there is an associated decrease in teaching score of 0.009. However, male instructors have a higher intercept term, that is, there is a vertical bump in the regression line for males in teaching scores. This is linked to the average difference in teaching scores that males obtain relative to females."
  },
  {
    "objectID": "notes.html#multiple-regression-interaction-model",
    "href": "notes.html#multiple-regression-interaction-model",
    "title": "Regression modelling part 2",
    "section": "",
    "text": "There is an interaction effect if the associated effect of one variable depends on the value of another variable. For example, the effect of age here will depend on whether the instructor is male or female, that is, the effect of age on teaching scores will differ by gender. The interaction model can be written as:\n\\[\n\\begin{aligned}\ny_{i} &= \\alpha + \\beta_1  x_{1i} + \\beta_2  x_{2i} + \\beta_3  x_{1i}  x_{2i} + \\epsilon_i \\nonumber \\\\&= \\alpha + \\beta_{\\text{age}} \\cdot \\text{age} + \\beta_{\\text{male}} \\cdot \\mathbb{I}_{\\text{male}}(x) + \\beta_{\\text{age, male}} \\cdot \\text{age} \\cdot \\mathbb{I}_{\\text{male}}(x) + \\epsilon_i,\n\\end{aligned}\n\\tag{2}\\]\nwhere \\(\\beta_{\\text{age, male}} \\cdot \\text{age} \\cdot \\mathbb{I}_{\\text{male}}(x)\\) corresponds to the interaction term.\nIn order to fit an interaction term within our regression model we replace the + sign with the * sign as follows:\n\nCodeint.model &lt;-lm(score ~ age * gender, data = eval.score)\ntab_model(int.model,show.ci = F)\n\n\n\n\n \nscore\n\n\nPredictors\nEstimates\np\n\n\n(Intercept)\n4.88\n&lt;0.001\n\n\nage\n-0.02\n&lt;0.001\n\n\ngender [male]\n-0.45\n0.094\n\n\nage × gender [male]\n0.01\n0.015\n\n\nObservations\n463\n\n\nR2 / R2 adjusted\n0.051 / 0.045\n\n\n\n\n\nHence, the regression line for females is given by:\n\\[\\widehat{\\text{score}} = 4.88 - 0.018 \\cdot \\text{age},\\] while the regression line for males is given by:\n\\[\\widehat{\\text{score}} = 4.88 - 0.018 \\cdot \\text{age} - 0.446 + 0.014 \\cdot \\text{age} = 4.434 - 0.004 \\cdot \\text{age}.\\]\n\n\n\n\n\n\n Question\n\n\n\nHow do they compare with the teaching score values from the parallel regression lines model?\n\n\nAnswer\n\nHere, we can see that, although the intercept for male instructors may be lower, the associated average decrease in teaching score with age (0.004) is not as severe as it is for female instructors (0.018).\n\n\n\nWe can plot the fitted model as we did before with the plot_model function. Lets try it now without the panel option (i.e., set grid = F) and selecting our color scheme:\n\nCodeplot_model(model = int.model,         # The fitted intertaction model\n           type=  \"pred\",              # type of plot\n           terms = c(\"age\",\"gender\"), # terms to include in the plot\n           grid=F,                    # split into a grid\n           show.data = T,             # show observations\n           jitter=T,                  # jitter the points\n           ci.lvl = NA,               # show/hide confidence intervals\n           title =\"Gender-age interaction model fitted lines\", # Plot title\n           colors = c(\"purple\", \"orange\")) # color scheme\n\n\n\nInstructor evaluation scores by age and gender with gender-varying regression lines superimposed."
  },
  {
    "objectID": "notes.html#assessing-model-fit",
    "href": "notes.html#assessing-model-fit",
    "title": "Regression modelling part 2",
    "section": "",
    "text": "Now we have to assess the fit of the model by looking at plots of the residuals. We shall do this for the interaction model. First, we need to obtain the fitted values and residuals from the interaction model as follows:\n\nCodeint.model_output &lt;-  eval.score %&gt;% \n  mutate(score_hat  = int.model$fitted.values,\n         residual = int.model$residuals)\n\n\nLet’s start by looking at a scatterplot of the residuals against the explanatory variable by gender:\n\nCodeggplot(int.model_output, aes(x = age, y = residual)) +\n  geom_point() +\n  labs(x = \"age\", y = \"Residual\") +\n  geom_hline(yintercept = 0, col = \"blue\", linewidth = 1) +\n  facet_wrap(~ gender)\n\n\n\nResiduals vs the explanatory variable age by gender.\n\n\n\nNow, we can plot the residuals against the fitted values using either the check_model function from the performance package or the plot_model() from sjPlot by setting type=\"diag\":\n\n\nUsing check_model\nUsing plot_model\n\n\n\n\nCodecheck_model(int.model,check = c(\"linearity\",\"homogeneity\"))\n\n\n\n\n\n\n\n\n\n\nCodeint.model_diag &lt;- plot_model(int.model,type = \"diag\")\nint.model_diag[[4]]\n\n\n\n\n\n\n\n\n\n\n\nFinally, let’s plot histograms of the residuals and QQ-plots to assess whether they are normally distributed with mean zero:\n\n\nUsing check_model\nUsing plot_model\n\n\n\n\nCodecheck_model(int.model,check = c(\"qq\",\"normality\"))\n\n\n\n\n\n\n\n\n\n\nCodelibrary(gridExtra) # to arrange the plots side-by-side\nint.model_diag &lt;- plot_model(int.model,type = \"diag\")\n\ngridExtra::grid.arrange(int.model_diag[[2]], # qqplot\n                        int.model_diag[[3]], # histogram\n                        ncol=2) # plot them side by side\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Task 2\n\n\n\nUsing ggplot to produce manually:\n\na scatter plot of the residuals vs. fitted values\nhistogram of the residuals to assess normality\n\n\n\nTake a hint\n\nThe int.model_output data we produced above contains all the information you need. Recall that a scatterplots and histograms can be produced with geom_point and geom_histogram layers respectively.\n\n\n\n\nClick here to see the solution\n\nCodep1 &lt;- ggplot(int.model_output,\n             aes(y=residual,x=score_hat))+\n  geom_point()+\n  labs(y=\"Residuals\",x=\"Fitted values\")\np2 &lt;- ggplot(int.model_output,\n             aes(x=residual))+\n  geom_histogram()\ngridExtra::grid.arrange(p1, # resid. vs. fitted\n                        p2, # histogram\n                        ncol=2)"
  },
  {
    "objectID": "notes.html#inference-using-sample-statistics",
    "href": "notes.html#inference-using-sample-statistics",
    "title": "Regression modelling part 2",
    "section": "\n2.1 Inference using sample statistics",
    "text": "2.1 Inference using sample statistics\nThe table below lists a variety of contexts where sample statistics can be used to estimate population parameters. In all 6 cases, the point estimate/sample statistic estimates the unknown population parameter. It does so by computing summary statistics based on a sample of size \\(n\\). We’ll cover Scenarios 5 and 6, namely construct CIs for the parameters in simple and multiple linear regression models. We will consider CIs based on theoretical results when standard assumptions hold, although sampling procedures such as bootstrap also exist. We will also consider how to use CIs for variable selection and finish by considering a model selection strategy based on objective measures for model comparisons.\nTable 1: Scenarios of sample statistics for inference.\n\n\n\n\n\n\n\n\n\nScenario\nPopulation Parameter\nPopulation Notation\nSample Statistic\nSample Notation\n\n\n\n1\nPopulation proportion\n\\(p\\)\nSample proportion\n\\(\\widehat{p}\\)\n\n\n2\nPopulation mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\n\n3\nDiff.in pop. props\n\\(p_1 - p_2\\)\nDiff. in sample props\n\\(\\widehat{p}_1 - \\widehat{p}_2\\)\n\n\n4\nDiff. in pop. means\n\\(\\mu_1 - \\mu_2\\)\nDiff. in sample means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\n\n5\nPop. intercept\n\\(\\beta_0\\)\nSample intercept\n\n\\(\\widehat{\\beta}_0\\) or \\(b_0\\)\n\n\n\n6\nPop. slope\n\\(\\beta_1\\)\nSample slope\n\n\\(\\widehat{\\beta}_1\\) or \\(b_1\\)\n\n\n\n\nIn reality, we don’t have access to the population parameter values (if we did, why would we need to estimate them?) we only have a single sample of data from a larger population. We’d like to be able to make some reasonable guesses about population parameters using that single sample to create a range of plausible values for a population parameter. This range of plausible values is known as a confidence interval. The confidence intervals we will see this week are calculated using the theoretical results based on the standard assumptions that you will have seen in Regression Modelling."
  },
  {
    "objectID": "notes.html#confidence-intervals-for-regression-parameters",
    "href": "notes.html#confidence-intervals-for-regression-parameters",
    "title": "Regression modelling part 2",
    "section": "\n3.1 Confidence Intervals for Regression Parameters",
    "text": "3.1 Confidence Intervals for Regression Parameters\nTo illustrate this, let’s have another look at teaching evaluations data evals with the SLR model with age as the the single explanatory variable and the instructors’ evaluation scores as the response variable. This data and the fitted model are shown here.\n\nCodeslr.model &lt;- lm(score ~ age, data = evals)\n\n (Intercept)          age \n 4.461932354 -0.005938225 \n\n\n\n\n\n\nSLR model applied to the teaching evaluation Data.\n\n\n\nThe point estimate of the slope parameter here is \\(\\widehat{\\beta}=\\) -0.006. But what about the uncertainty of our estimation? Well we can construct confidence interval for this.\nSimple linear regression (SLR)\n\n\n\n\n\n\nRe-cap on SLR\n\n\n\nRecall that for a simple linear regression model we have:\n\n\\(\\text{Var}(\\hat{\\beta}_0) = \\sigma^2 \\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right]\\) and \\(se(\\hat{\\beta_0}) = \\sqrt{\\hat{\\sigma^2} \\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right]}\\)\n\\(\\text{Var}(\\hat{\\beta}_1) = \\frac{\\sigma^2}{S_{xx}}\\) and \\(se(\\hat{\\beta_1}) = \\frac{\\hat{\\sigma^2}}{S_{xx}}\\)\n\nWhere \\(S_{xx} = \\sum_{i=1}^n (x_i-\\bar{x})^2\\) and \\(\\hat{\\sigma^2} = \\frac{\\sum_i y_i-\\hat{y}}{n-2}\\). Then, we get the following pivotal quantities:\n\\[\n\\dfrac{\\hat{\\beta_j} - \\beta_j}{se(\\hat{\\beta_j})} \\sim t_{n-2}\n\\]for \\(j \\in \\{0,1\\}\\).\n\n\nA \\(100(1-\\alpha)\\)% confidence interval for the intercept and slope is given by:\n\n\\(\\hat{\\beta_0} \\pm t_{\\alpha/2,n-2} \\times se(\\hat{\\beta_0})\\)\n\\(\\hat{\\beta_1} \\pm t_{\\alpha/2,n-2} \\times se(\\hat{\\beta_1})\\)\n\nwhere \\(t_{\\alpha/2,n-2}\\) is the critical t-value for a 95% confidence interval (note that sometimes a Gaussian approximation is used such that \\(\\hat{\\beta_j} \\pm 1.96 \\times se(\\hat{\\beta_j})\\) - this assumes \\(\\sigma^2\\) is known).\n\nA confidence interval gives a range of plausible values for a population parameter.\n\nWe can therefore use the confidence interval for \\(\\beta_1\\) to state a range of plausible values and, just as usefully, what values are not plausible. The most common value to compare the confidence interval of \\(\\beta_1\\) with is 0 (zero), since \\(\\beta_1 = 0\\) says there is no (linear) relationship between the response variable (\\(y\\)) and the explanatory variable (\\(x\\)). Therefore, if 0 lies within the confidence interval for \\(\\beta_1\\) then there is insufficient evidence of a linear relationship between \\(y\\) and \\(x\\). However, if 0 does not lie within the confidence interval, then we conclude that \\(\\beta_1\\) is significantly different from zero and therefore that there is evidence of a linear relationship between \\(y\\) and \\(x\\).\nLet’s use the confidence interval based on theoretical results for the slope parameter in the SLR model applied to the teacher evaluation scores with age as the the single explanatory variable and the instructors’ evaluation scores as the outcome variable.\nThe tab_model function allow us to print the \\((1-\\alpha)\\)% confidence intervals for our parameters using the argument show.ci = (1-alpha). Additionally, we can print the estimator standard error by setting show.se=T):\n\nCodetab_model(slr.model,show.se = T,show.ci = 0.95)\n\n\n\n\n \nscore\n\n\nPredictors\nEstimates\nstd. Error\nCI\np\n\n\n(Intercept)\n4.46\n0.13\n4.21 – 4.71\n&lt;0.001\n\n\nage\n-0.01\n0.00\n-0.01 – -0.00\n0.021\n\n\nObservations\n463\n\n\nR2 / R2 adjusted\n0.011 / 0.009\n\n\n\n\n\nThis will give you a nice looking html table. However, if you prefer your output to be a data.frame object, you can use the tidy function from the broom package instead (you can print the CI using conf.int=T):\n\nCodebroom::tidy(slr.model,conf.int=T,conf.level = 0.95)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  4.46      0.127       35.2  1.05e-132   4.21    4.71    \n2 age         -0.00594   0.00257     -2.31 2.13e-  2  -0.0110 -0.000890\n\n\nThen we can plot our fitted SLR using the plot_model function:\n\nCodeplot_model(model = slr.model,\n           terms = c(\"age\"),\n           type  = \"pred\",\n           title = \"Fitted Linear regression model\",\n           show.data = T,\n           jitter = T)\n\n\n\nSLR model applied to the teaching evaluation Data.\n\n\n\n\n\n\n\n\n\n Task 3\n\n\n\nUse ggplot to reproduce the plot above. You may achieve this by adding a geom_smooth() layer.\n\n\nHint\n\nChose method =\"lm\" as argument of the geom_smooth() layer. You may also chance the confidence level via the level argument.\n\n\n\n\nClick here to see the solution\n\nCodeggplot(evals, aes(x = age, y = score)) +\n  geom_jitter() +\n  labs(x = \"Age\", y = \"Teaching Score\") +\n geom_smooth(method = \"lm\",level=.95)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nMultiple Regression confidence intervals\nLet’s continue with the teaching evaluations data by looking into the multiple regression models we have fitted with one numerical and one categorical explanatory variable . In these models:\n\n\n\\(y\\): response variable of instructor evaluation score\n\nexplanatory variables\n\n\n\\(x_1\\): numerical explanatory variable of age\n\n\n\\(x_2\\): categorical explanatory variable of gender\n\n\n\n\nFirst, recall that we had two competing potential models to explain professors’ teaching evaluation scores:\n\nModel 1 ( Equation 1 ): Parallel lines model (no interaction term) - both male and female professors have the same slope describing the associated effect of age on teaching score\nModel 2 ( Equation 2 ): Interaction model - allowing for male and female professors to have different slopes describing the associated effect of age on teaching score\n\nLet’s recall the output of these regression models:\n\nCodetab_model(par.model,\n          int.model,\n          collapse.ci = T,\n          show.stat = T,\n          show.se = T,\n          dv.labels = c(\"parallel lines model\",\"interaction model\") )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nparallel lines model\ninteraction model\n\n\nPredictors\nEstimates\nstd. Error\nStatistic\np\nEstimates\nstd. Error\nStatistic\np\n\n\n(Intercept)\n4.48\n(4.24 – 4.73)\n0.13\n35.79\n&lt;0.001\n4.88\n(4.48 – 5.29)\n0.21\n23.80\n&lt;0.001\n\n\nage\n-0.01\n(-0.01 – -0.00)\n0.00\n-3.28\n0.001\n-0.02\n(-0.03 – -0.01)\n0.00\n-3.92\n&lt;0.001\n\n\ngender [male]\n0.19\n(0.09 – 0.29)\n0.05\n3.63\n&lt;0.001\n-0.45\n(-0.97 – 0.08)\n0.27\n-1.68\n0.094\n\n\nage × gender [male]\n\n\n\n\n0.01\n(0.00 – 0.02)\n0.01\n2.45\n0.015\n\n\nObservations\n463\n463\n\n\nR2 / R2 adjusted\n0.039 / 0.035\n0.051 / 0.045\n\n\n\n\n\n\nNotice that, together with the estimated parameter values, the tables we produced now include other information about each estimated parameter in the model, namely:\n\n\nstd_error: the standard error of each parameter estimate (set show.se = T);\n\nstatistic: the test statistic value used to test the null hypothesis that the population parameter is zero (set show.stat = T);\n\np_value: the \\(p\\) value associated with the test statistic under the null hypothesis; and\n\nlower_ci and upper_ci: the lower and upper bounds of the 95% confidence interval for the population parameter (the options collapse.ci =T can be used to parse the CI next to the parameter estimates)\n\nThese values are calculated using the theoretical results based on the standard assumptions that you will have seen in Regression Modelling."
  }
]