[
  {
    "objectID": "slides/slides.html#ilos",
    "href": "slides/slides.html#ilos",
    "title": "Data Analysis",
    "section": "ILOs",
    "text": "ILOs\nBy the end of this session you will be able to:\n\nFit a logistic regression model with either a numerical or categorical explanatory variable:\nInterpret the regression coefficients of the logistic model in terms of their effects on both the odds and the log odds (of the response)\nConduct model validation and assess model predictive performance"
  },
  {
    "objectID": "slides/slides.html#logistic-regression",
    "href": "slides/slides.html#logistic-regression",
    "title": "Data Analysis",
    "section": "Logistic regression",
    "text": "Logistic regression\nThis week we will learn how to model outcomes of interest that take one of two categorical values (e.g. yes/no, success/failure, alive/dead), i.e.\n\nbinary, taking the value 1 (say success, with probability \\(p_i\\)) or 0 (failure, with probability \\(1-p_i\\))\nWe are interested in \\(\\text{Prob}(Event) = p_i\\). Suppose the a random variable \\(y_i\\) for the event of interest.\n\nIn this case,\n\\[\ny_i \\sim \\mathrm{Bin}(1,p_i)\\\\\ng(p_i) = \\log \\left(\\frac{p_i}{1 - p_i} \\right)\n\\]\nwhich is also referred to as the log-odds (since \\(p_i ~ / ~ 1-p_i\\) is an odds ratio).\n\\[p_i = \\frac{\\exp\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\right)}{1 + \\exp\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\right)} ~~~ \\in [0, 1].\\]"
  },
  {
    "objectID": "slides/slides.html#required-r-packages",
    "href": "slides/slides.html#required-r-packages",
    "title": "Data Analysis",
    "section": "Required R packages",
    "text": "Required R packages\nBefore we proceed, load all the packages needed for this week:\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(sjPlot)\nlibrary(broom)\nlibrary(performance)\nlibrary(yardstick) \n\n\n\nThe first libraries ggplot2 and tidyverse allows us to create nice data visualisations and data manipulation.\nThe second libraries sjPlot and broom allows us summarise and present model output in a tidy format.\nThe final two libraries (performance and yardstick) are used fro model assessment and validation."
  },
  {
    "objectID": "slides/slides.html#first-example---teaching-evaluation-scores",
    "href": "slides/slides.html#first-example---teaching-evaluation-scores",
    "title": "Data Analysis",
    "section": "First example - Teaching evaluation scores",
    "text": "First example - Teaching evaluation scores\nevals data set containing student evaluations for a sample of 463 courses taught by 94 professors from the University of Texas at Austins.\n\n\nCode\nevals.gender &lt;- evals %&gt;%\n                  select(gender, age)"
  },
  {
    "objectID": "slides/slides.html#fitting-a-logistic-regression-in-r",
    "href": "slides/slides.html#fitting-a-logistic-regression-in-r",
    "title": "Data Analysis",
    "section": "Fitting a logistic regression in R",
    "text": "Fitting a logistic regression in R\n\\[\n\\log\\left( \\frac{p}{1-p} \\right) = \\alpha + \\beta\n\\]\nwhere \\(p = \\textrm{Prob}\\left(\\textrm{Male}\\right)\\).\n\nLog OddsOdd Scale\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \ngender\n\n\nPredictors\nLog-Odds\np\n\n\n(Intercept)\n-2.70\n(-3.72 – -1.71)\n&lt;0.001\n\n\nage\n0.06\n(0.04 – 0.08)\n&lt;0.001\n\n\n\n\n\n\n\n\n\n\nthe log-odds of the instructor being male increase by 0.06 for every one unit increase in age.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \ngender\n\n\nPredictors\nOdds Ratios\np\n\n\n(Intercept)\n0.07\n(0.02 – 0.18)\n&lt;0.001\n\n\nage\n1.06\n(1.04 – 1.09)\n&lt;0.001\n\n\n\n\n\n\n\n\n\n\nfor every 1 unit increase in age, the odds of the teaching instructor being male increase by a factor of 1.06."
  },
  {
    "objectID": "slides/slides.html#relationship-between-between-odds-and-probabilities",
    "href": "slides/slides.html#relationship-between-between-odds-and-probabilities",
    "title": "Data Analysis",
    "section": "Relationship between between Odds and Probabilities",
    "text": "Relationship between between Odds and Probabilities\n\n\n\nTable 1: Relationship between Odds and Probabilities\n\n\n\n\n\n\n\n\n\nScale\nEquivalence\n\n\n\n\nOdds\n\\[                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n                                                                                                                                                                                                                                                                                                                                                                                       Odds = \\mathrm{exp}(log Odds) = \\dfrac{P(event)}{1-P(event)}                                                                                                      \n                                                                                                                                                                                                                                                                                                                                                                                       \\]\n\n\nProbability\n\\[                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n                                                                                                                                                                                                                                                                                                                                                                                       P(event) =\\dfrac{\\mathrm{exp}(logOdds)}{1+\\mathrm{exp}(logOdds)}  = \\dfrac{Odds}{1+Odds}                                                                          \n                                                                                                                                                                                                                                                                                                                                                                                       \\]"
  },
  {
    "objectID": "slides/slides.html#model-evaluation",
    "href": "slides/slides.html#model-evaluation",
    "title": "Data Analysis",
    "section": "Model evaluation",
    "text": "Model evaluation\n\nlibrary(performance)\ncheck_model(model, check = c(\"pp_check\",\"binned_residuals\",\"outliers\",\"qq\"))"
  },
  {
    "objectID": "slides/slides.html#predictive-performance-metrics",
    "href": "slides/slides.html#predictive-performance-metrics",
    "title": "Data Analysis",
    "section": "Predictive performance metrics",
    "text": "Predictive performance metrics\nHow well our model predicts new observations?\n\ncompute the predicted classes and compare them against the observed values.\nWe typically classify these probabilities into discrete classes based on a threshold (commonly 0.5 for binary classification)We can set a threshold\n\n\n\nCode\npred_results = model %&gt;% \n  broom::augment(type.predict = c(\"response\")) %&gt;%\n  mutate(predicted_class = \n           factor(ifelse(.fitted &gt; 0.5, \"male\", \"female\")))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngender\nage\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\npredicted_class\n\n\n\n\nfemale\n36\n0.39\n-1.00\n0.01\n1.13\n0\n-1.00\nfemale\n\n\nfemale\n36\n0.39\n-1.00\n0.01\n1.13\n0\n-1.00\nfemale\n\n\nfemale\n36\n0.39\n-1.00\n0.01\n1.13\n0\n-1.00\nfemale\n\n\nfemale\n36\n0.39\n-1.00\n0.01\n1.13\n0\n-1.00\nfemale\n\n\nmale\n59\n0.73\n0.79\n0.00\n1.13\n0\n0.79\nmale"
  },
  {
    "objectID": "slides/slides.html#predictive-performance-metrics-1",
    "href": "slides/slides.html#predictive-performance-metrics-1",
    "title": "Data Analysis",
    "section": "Predictive performance metrics",
    "text": "Predictive performance metrics\nWe can use these predicted classes to compute different predictive performance/evaluation metrics\n\n\n\n\n\n\n\n\n\nThe correct classification rate (CCR) or accuracy describes the overall proportion of teaching instructors (males or females) that were classified correctly.\nThe true positive rate (TPR) or sensitivity (a.k.a. recall), denotes the proportion of actual male instructors that are correctly classified as males by the model.\nThe true negative rate (TNR) or specificity, denotes the proportion of actual females that have been classified correctly as females by the model.\nThe model’s precision or positive predictive value (PPV) represents the proportion of predicted male instructors that were actually male\nThe model’s negative predictive value (NPV) represents the proportion of predicted female instructors ."
  },
  {
    "objectID": "slides/slides.html#roc-curve",
    "href": "slides/slides.html#roc-curve",
    "title": "Data Analysis",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nPlot true positive rate (sensitivity) and the false positive rate (1 - specificity) at various threshold levels.\nThe closer the ROC curve is to the top-left corner, the better the model is at distinguishing between the positive and negative classes"
  },
  {
    "objectID": "slides/slides.html#logistic-regression-with-one-categorical-explanatory-variable",
    "href": "slides/slides.html#logistic-regression-with-one-categorical-explanatory-variable",
    "title": "Data Analysis",
    "section": "Logistic regression with one categorical explanatory variable",
    "text": "Logistic regression with one categorical explanatory variable\nInstead of having a numerical explanatory variable such as age, let’s now use the binary categorical variable ethnicity as our explanatory variable."
  },
  {
    "objectID": "slides/slides.html#fitting-the-model",
    "href": "slides/slides.html#fitting-the-model",
    "title": "Data Analysis",
    "section": "Fitting the model",
    "text": "Fitting the model\n\nevals.ethnic &lt;- evals %&gt;%\n                  select(gender, ethnicity)\nmodel.ethnic &lt;- glm(gender ~ ethnicity,\n                    data = evals.ethnic,\n                    family = binomial) \n\n\n\n\n\n\n\n\n\n\n\n\n \ngender\n\n\nPredictors\nLog-Odds\np\n\n\n(Intercept)\n-0.25\n(-0.75 – 0.24)\n0.319\n\n\nethnicity [not minority]\n0.66\n(0.13 – 1.20)\n0.015"
  },
  {
    "objectID": "slides/slides.html#interpretation-of-model-parameters",
    "href": "slides/slides.html#interpretation-of-model-parameters",
    "title": "Data Analysis",
    "section": "Interpretation of model parameters",
    "text": "Interpretation of model parameters\nLets break this down. The model we have fitted is:\n\\[\n\\mathrm{log}\\left(\\dfrac{p_i}{1-p_i}\\right) = \\alpha + \\beta_{\\text{ethnicity}}  \\times \\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~  minority})\n\\]\n\n\\(\\alpha\\) is the intercept, representing the log-odds when \\(\\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~ minority}) = 0\\)."
  },
  {
    "objectID": "slides/slides.html#interpretation-of-model-parameters-1",
    "href": "slides/slides.html#interpretation-of-model-parameters-1",
    "title": "Data Analysis",
    "section": "Interpretation of model parameters",
    "text": "Interpretation of model parameters\nLets break this down. The model we have fitted is:\n\\[\n\\mathrm{log}\\left(\\dfrac{p_i}{1-p_i}\\right) = \\alpha + \\beta_{\\text{ethnicity}}  \\times \\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~  minority})\n\\]\n\n\\(\\alpha\\) is the intercept, representing the log-odds when \\(\\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~ minority}) = 0\\).\n\nWhen the instructor belongs to the reference category minority the models simplifies to: \\[\\mathrm{log}\\left(\\frac{p_i}{1-p_i}\\right) = \\alpha = -0.25 \\]"
  },
  {
    "objectID": "slides/slides.html#interpretation-of-model-parameters-2",
    "href": "slides/slides.html#interpretation-of-model-parameters-2",
    "title": "Data Analysis",
    "section": "Interpretation of model parameters",
    "text": "Interpretation of model parameters\nLets break this down. The model we have fitted is:\n\\[\n\\mathrm{log}\\left(\\dfrac{p_i}{1-p_i}\\right) = \\alpha + \\beta_{\\text{ethnicity}}  \\times \\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~  minority})\n\\]\n\n\\(\\alpha\\) is the intercept, representing the log-odds when \\(\\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~ minority}) = 0\\).\n\nWhen the instructor belongs to the reference category minority the models simplifies to: \\[\\mathrm{log}\\left(\\frac{p_i}{1-p_i}\\right) = \\alpha = -0.25 \\]\n\\(\\text{Odds}(\\text{male=1|minority=1}) = \\left[\\dfrac{P(\\mathrm{male}=1 |\\mathrm{minority}=1)}{P(\\mathrm{female}= 1 |\\mathrm{minority}=1)}\\right]=\\exp(\\alpha) = \\exp(-0.25) = 0.78\\)"
  },
  {
    "objectID": "slides/slides.html#interpretation-of-model-parameters-3",
    "href": "slides/slides.html#interpretation-of-model-parameters-3",
    "title": "Data Analysis",
    "section": "Interpretation of model parameters",
    "text": "Interpretation of model parameters\nLets break this down. The model we have fitted is:\n\\[\n\\mathrm{log}\\left(\\dfrac{p_i}{1-p_i}\\right) = \\alpha + \\beta_{\\text{ethnicity}}  \\times \\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~  minority})\n\\]\n\n\\(\\alpha\\) is the intercept, representing the log-odds when \\(\\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~ minority}) = 0\\).\n\nWhen the instructor belongs to the reference category minority the models simplifies to: \\[\\mathrm{log}\\left(\\frac{p_i}{1-p_i}\\right) = \\alpha = -0.25 \\]\n\\(\\text{Odds}(\\text{male=1|minority=1}) = \\left[\\dfrac{P(\\mathrm{male}=1 |\\mathrm{minority}=1)}{P(\\mathrm{female}= 1 |\\mathrm{minority}=1)}\\right]=\\exp(\\alpha) = \\exp(-0.25) = 0.78\\)\n\\(\\text{Odds}(\\text{male=0|minority=1}) =  \\left[\\dfrac{P(\\mathrm{male}=1 |\\mathrm{minority}=1)}{P(\\mathrm{female}= 1 |\\mathrm{minority}=1)}\\right]^{-1} = \\exp(\\alpha)^{-1} = \\exp(-0.25)^{-1} = 1.28\\)"
  },
  {
    "objectID": "slides/slides.html#interpretation-of-model-parameters-4",
    "href": "slides/slides.html#interpretation-of-model-parameters-4",
    "title": "Data Analysis",
    "section": "Interpretation of model parameters",
    "text": "Interpretation of model parameters\nLets break this down. The model we have fitted is:\n\\[\n\\mathrm{log}\\left(\\dfrac{p_i}{1-p_i}\\right) = \\alpha + \\beta_{\\text{ethnicity}}  \\times \\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~  minority})\n\\]\n\n\\(\\alpha\\) is the intercept, representing the log-odds when \\(\\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~ minority}) = 0\\).\n\\(\\beta_{\\mathrm{ethnicity}}\\) is the coefficient for the predictor \\(\\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~ minority})\\), which shows how the log-odds change when moving from the reference category (minority) to the other level (not minority)."
  },
  {
    "objectID": "slides/slides.html#interpretation-of-model-parameters-5",
    "href": "slides/slides.html#interpretation-of-model-parameters-5",
    "title": "Data Analysis",
    "section": "Interpretation of model parameters",
    "text": "Interpretation of model parameters\nLets break this down. The model we have fitted is:\n\\[\n\\mathrm{log}\\left(\\dfrac{p_i}{1-p_i}\\right) = \\alpha + \\beta_{\\text{ethnicity}}  \\times \\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~  minority})\n\\]\n\n\\(\\alpha\\) is the intercept, representing the log-odds when \\(\\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~ minority}) = 0\\).\n\\(\\beta_{\\mathrm{ethnicity}}\\) is the coefficient for the predictor \\(\\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~ minority})\\), which shows how the log-odds change when moving from the reference category (minority) to the other level (not minority).\n\nWhen the instructor does not belong to reference category, i.e. \\(\\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~ minority}) = 1\\), the model becomes: \\[\\mathrm{log}\\left(\\dfrac{p_i}{1-p_i}\\right) = \\alpha + \\beta_{\\text{ethnicity}}\\]"
  },
  {
    "objectID": "slides/slides.html#interpretation-of-model-parameters-6",
    "href": "slides/slides.html#interpretation-of-model-parameters-6",
    "title": "Data Analysis",
    "section": "Interpretation of model parameters",
    "text": "Interpretation of model parameters\nLets break this down. The model we have fitted is:\n\\[\n\\mathrm{log}\\left(\\dfrac{p_i}{1-p_i}\\right) = \\alpha + \\beta_{\\text{ethnicity}}  \\times \\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~  minority})\n\\]\n\n\\(\\alpha\\) is the intercept, representing the log-odds when \\(\\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~ minority}) = 0\\).\n\\(\\beta_{\\mathrm{ethnicity}}\\) is the coefficient for the predictor \\(\\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~ minority})\\), which shows how the log-odds change when moving from the reference category (minority) to the other level (not minority).\n\nSo, the log-odds of the instructors being male in the not minority group are \\(\\alpha +\\beta_{\\text{ethnicity}}\\)."
  },
  {
    "objectID": "slides/slides.html#interpretation-of-model-parameters-7",
    "href": "slides/slides.html#interpretation-of-model-parameters-7",
    "title": "Data Analysis",
    "section": "Interpretation of model parameters",
    "text": "Interpretation of model parameters\nLets break this down. The model we have fitted is:\n\\[\n\\mathrm{log}\\left(\\dfrac{p_i}{1-p_i}\\right) = \\alpha + \\beta_{\\text{ethnicity}}  \\times \\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~  minority})\n\\]\n\n\\(\\alpha\\) is the intercept, representing the log-odds when \\(\\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~ minority}) = 0\\).\n\\(\\beta_{\\mathrm{ethnicity}}\\) is the coefficient for the predictor \\(\\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~ minority})\\), which shows how the log-odds change when moving from the reference category (minority) to the other level (not minority).\n\nSo, the log-odds of the instructors being male in the not minority group are \\(\\alpha +\\beta_{\\text{ethnicity}}\\).\nOn The odd scale this is:\n\\[\n\\text{Odds}(\\text{male=1|minority=0}) = \\exp(\\alpha + \\beta_{\\text{ethnicity}} ) = \\exp(-0.25 + 0.66)\n\\]"
  },
  {
    "objectID": "slides/slides.html#odds-ratio",
    "href": "slides/slides.html#odds-ratio",
    "title": "Data Analysis",
    "section": "Odds ratio",
    "text": "Odds ratio\nThe odds-ratio of an instructor being male in the not minority group compared to the minority ethnic group is given by the following Odds ratio:\n\\[\n\\begin{aligned}\n    \\frac{\\mathrm{Odds}(\\mathrm{male} = 1| \\mathrm{minority} = 0)}{\\mathrm{Odds}(\\mathrm{male} = 1| \\mathrm{minority} = 1)} &= \\dfrac{\\frac{p_{(\\mathrm{minority} = 0)}}{1- p_{(\\mathrm{minority} = 0)}}}{\\frac{p_{(\\mathrm{minority}=1)}}{1- p_{(\\mathrm{minority}=1)}}} \\\\\n    &= \\frac{\\mathrm{exp}( \\alpha + \\beta_{\\text{ethnicity}})}{\\exp\\left(\\alpha\\right)}\\\\\n    &= \\exp\\left(\\alpha + \\beta_{\\text{ethnicity}} - \\alpha\\right) \\\\\n    &= \\exp\\left(\\beta_{\\text{ethnicity}}\\right)\n    &= 1.94\n\\end{aligned}\n\\] This means that instructors that are not in the minority groups are significantly 1.94 times more likely to be males compared to instructors in the minority group."
  },
  {
    "objectID": "slides/slides.html#the-steps-ahead",
    "href": "slides/slides.html#the-steps-ahead",
    "title": "Data Analysis",
    "section": "The steps ahead",
    "text": "The steps ahead\n\nCalculate, using R, the logistic regression coefficients on the odds and probability scales when either a continuous or a categorical exploratory variable is used in the model.\nCorrect interpretation of the of the model parameters of a fitted logistic regression (in terms of the log-odds, odds and probabilities) with either a continuous or a categorical exploratory variable .\nCheck how to visualize the results of a fitted logistic regression with with either a continuous or a categorical exploratory variable.\nInterpret the model diagnostic plots and predictive performance metrics of a logistic regression model."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Extra tasks",
    "section": "",
    "text": "1 Case 1: Yanny or Laurel?\nThis auditory illusion first appeared on the internet in May 2018. An explanation of why people hear different things can be found in the following short video, just one of many internet sources discussing the phenomenon.\n\nThe main reason behind the difference appears to be that as we age we lose the ability to hear certain sounds. To see if we could find evidence of such an age effect, we asked students and staff at the School of Mathematics and Statistics at the University of Glasgow to fill out a survey on what they hear. Below you can see summaries of the responses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe proportions hearing Yanny and Laurel are very similar to each other, and there are some respondents who hear both or even something completely different. This may be because people do not listen to the audio file using the same device, something we couldn’t control for in the survey. Ignoring the responses other than Yanny or Laurel, we have 53 observations.\n Download yanny dataset \n\n\n\n\n\n\n Task 1\n\n\n\nDownload the yanny.csv data and fit a logistic regression model with hear as the binary response variable, and age and gender as the explanatory variables. What are your findings?\n\n\nSee Solution\n\nLoad the data:\n\nyanny &lt;- read.csv(\"yanny.csv\",stringsAsFactors = T)\nyanny &lt;- yanny %&gt;%\n          select(hear, gender, age)\n\nExploratory Plots:\n\nggplot(data = yanny, aes(x = hear, y = age, fill = hear)) +\n  geom_boxplot() +\n  labs(x = \"What do you hear?\", y = \"Age\") +\n  theme(legend.position = \"none\")\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\nHearing Yanny/Laurel by age.\n\n\n\nWe see in the boxplot that the people who hear Yanny are, on average, younger, however there is some overlap in the IQR’s.\n\nyanny %&gt;%\n  tabyl(gender, hear) %&gt;%\n  adorn_percentages() %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns() # To show original counts\n\n gender     Laurel      Yanny\n Female 50.0% (14) 50.0% (14)\n   Male 56.0% (14) 44.0% (11)\n\n\n\nggplot(data = yanny, aes(x = hear, group = gender)) +\n  geom_bar(aes(y = ..prop.., fill = gender),\n           stat = \"count\", position = \"dodge\") +\n  labs(x = \"What do you hear?\", y = \"Proportion\")\n\n\n\nBarplot of what participants heard by gender.\n\n\n\nThere is a slightly smaller proportion of men hearing Yanny, but the proportions are very similar overall.\n\nmod.yanny &lt;- glm(hear ~ age + gender, data = yanny,family=\"binomial\")\n\nmod.yanny %&gt;%\n  tab_model(transform = NULL)\n\n\n\n\n \nhear\n\n\nPredictors\nLog-Odds\nCI\np\n\n\n(Intercept)\n1.63\n-0.72 – 4.24\n0.191\n\n\nage\n-0.05\n-0.12 – 0.01\n0.155\n\n\ngender [Male]\n-0.21\n-1.34 – 0.91\n0.717\n\n\nObservations\n52\n\n\nR2 Tjur\n0.045\n\n\n\n\n\nNotice that the coefficient of age is negative, suggesting that older people are less likely to hear Yanny. However, the coefficient of age is not significant (\\(p\\)-value of 0.16). Still, if we wanted to use the estimated coefficient to quantify the effect of age, we would need to look at exp(-0.05) = 0.95. This suggests that for two people who differ by one year in age, the older person’s odds of hearing Yanny are 0.95 times those of the younger person. If we want to look at a ten-year age difference then the odds multiplier becomes exp(-0.05 * 10) = 0.6. Hence, for two people who differ by 10 years in age, the older person’s odds of hearing Yanny are 0.6 times those of the younger person. Conversely, the odds for a younger person of hearing Yanny would be exp(-0.05 * 10)^-1 = 1.65 times the odds (64% higher odds) of hearing Yanny than 10 year older person.\n\nplot_model(mod.yanny, show.values = TRUE,\n           title = \"Odds (Age)\", show.p = TRUE)\n\n\n\nOdds of hearing yanny with age.\n\n\n\n\nplot_model(mod.yanny,\n           type = \"pred\", \n           terms = \"age\", \n           title = \"\",\n           axis.title = c(\"Age\", \"Probability of hearing Yanny\"))\n\n\n\nProbability of hearing Yanny with age.\n\n\n\n\n\n\n\n2 Case 2: Surviving the Titanic\nOn 15th April 1912, during its maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n Download titanic dataset \n\n\n\n\n\n\n Task 2\n\n\n\nDownload the data (titanic.csv) for \\(n = 891\\) passengers aboard the Titanic and fit a logistic regression model with survived as the binary response variable, and age, gender, and passenger.class as the explanatory variables. What are your findings?\n\n\nSee Solution\n\nLoad the data\n\ntitanic &lt;- read.csv(\"titanic.csv\",stringsAsFactors = T)\ntitanic &lt;- titanic %&gt;%\n          select(survived, age, gender, passenger.class) %&gt;%\n  mutate(passenger.class = as.factor(passenger.class),\n         survived_fct = as.factor(survived))\nlevels(titanic$survived_fct) &lt;- c(\"Died\", \"Survived\")\n\n\nggplot(data = titanic, aes(x = survived_fct, y = age, fill = survived_fct)) +\n  geom_boxplot() +\n  labs(x = \"Survived the Titanic?\", y = \"Age\") +\n  theme(legend.position = \"none\")\n\n\n\nTitanic passenger age by survival.\n\n\n\nWe see in the boxplot that there is very little difference in the age of passengers who died or survived the sinking of the Titanic.\n\ntitanic %&gt;%\n  tabyl(gender, survived_fct) %&gt;%\n  adorn_percentages() %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns() # To show original counts\n\n gender        Died    Survived\n female 25.8%  (81) 74.2% (233)\n   male 81.1% (468) 18.9% (109)\n\n\n\nggplot(data = titanic, aes(x = survived_fct, group = gender)) +\n  geom_bar(aes(y = ..prop.., fill = gender), stat = \"count\", position = \"dodge\") +\n  labs(x = \"Survived the Titanic?\", y = \"Proportion\")\n\n\n\nBarplot of passenger survival by gender.\n\n\n\nThere is a clear pattern here with the proportion surviving much higher for females than for males.\n\ntitanic %&gt;%\n  tabyl(passenger.class, survived) %&gt;%\n  adorn_percentages() %&gt;%\n  adorn_pct_formatting() %&gt;%\n  adorn_ns() # To show original counts\n\n passenger.class           0           1\n               1 37.0%  (80) 63.0% (136)\n               2 52.7%  (97) 47.3%  (87)\n               3 75.8% (372) 24.2% (119)\n\n\n\nggplot(data = titanic, aes(x = survived_fct, group = passenger.class)) +\n  geom_bar(aes(y = ..prop.., fill = passenger.class),\n            stat = \"count\", position = \"dodge\") +\n  labs(x = \"Survived the Titanic?\", y = \"Proportion\")\n\n\n\nBarplot of passenger survival by gender.\n\n\n\nThe largest group of passengers who died were third class passengers, while among those who survived the largest group was first class passengers.\nNow we focus on the event \\(\\text{Probability}(\\text{Survival = 1})\\). We will as our response the binary variable survived which takes the value of 1 if the ith passenger survived and zero othewise.\n\nmod.titanic &lt;- glm(survived ~ gender + passenger.class + age, data = titanic) \nmod.titanic %&gt;%\n  tab_model(transform = NULL)\n\n\n\n\n \nsurvived\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n1.10\n1.00 – 1.19\n&lt;0.001\n\n\ngender [male]\n-0.50\n-0.55 – -0.45\n&lt;0.001\n\n\npassenger class [2]\n-0.18\n-0.26 – -0.10\n&lt;0.001\n\n\npassenger class [3]\n-0.37\n-0.44 – -0.30\n&lt;0.001\n\n\nage\n-0.01\n-0.01 – -0.00\n&lt;0.001\n\n\nObservations\n891\n\n\nR2\n\n0.383\n\n\n\n\n\nWe see that the coefficient for males (gendermale) is negative, indicating a lower chance of survival for male passengers. Similarly, the coefficients for second (passenger.class2) and third (passenger.class3) class passengers are negative, with the magnitude of the third class coefficient larger than that of the second class coefficient. This suggests that second class passengers chances of survival were worse in comparison with first class passengers, and that third class passengers chances of survival were even worse. Finally the age coefficient is negative, suggesting that older people were less likely to survive.\n\nplot_model(mod.titanic,\n           transform = \"exp\",\n           show.values = TRUE,\n           title = \"\",\n           show.p = FALSE, \n           value.offset = 0.25)\n\n\n\nOdds of surviving the sinking of the Titanic.\n\n\n\nWe interpret the odds ratios as follows: men’s odds of survival were 0.61 times those of women, third class passengers’ odds of survival were 0.7 times those of first class passengers, and second class passengers’ odds of survival were 0.83 times those of first class passengers. Finally, for each year increase in the passenger’s age, their odds of survival decrease (by a factor of 0.99)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Generalised Linear Models part 1",
    "section": "",
    "text": "In previous weeks we looked at modelling data using linear regression models were we had:\n\na continuous response variable \\(y\\) and\none or more explanatory variables \\(x_1, x_2,\\ldots, x_p\\), which were numerical/categorical variables.\n\nRecall that for data \\((y_i, x_i), ~ i = 1,\\ldots, n\\), where \\(y\\) is a continuous response variable, we can write a simple linear regression model as follows:\n\\[y_i = \\alpha + \\beta x_i + \\epsilon_i, ~~~~ \\epsilon_i \\sim N(0, \\sigma^2),\\] where\n\n\n\\(y_i\\) is the \\(i^{th}\\) observation of the continuous response variable;\n\n\\(\\alpha\\) is the intercept of the regression line;\n\n\\(\\beta\\) is the slope of the regression line;\n\n\\(x_i\\) is the \\(i^{th}\\) observation of the explanatory variable; and\n\n\\(\\epsilon_i\\) is the \\(i^{th}\\) random component.\n\nThus, the full probability model for \\(y_i\\) given \\(x_i\\) (\\(y_i | x_i\\)) can be written as\n\\[y_i | x_i \\sim N(\\alpha + \\beta x_i, \\sigma^2),\\]\nwhere the mean \\(\\alpha + \\beta x_i\\) is given by the deterministic part of the model and the variance \\(\\sigma^2\\) by the random part. Hence we make the assumption that the outcomes \\(y_i\\) are normally distributed with mean \\(\\alpha + \\beta x_i\\) and variance \\(\\sigma^2\\). However, what if our response variable \\(y\\) is not a continuous random variable?\n\nThe main objective this week is to introduce Generalised Linear Models (GLMs), which extend the linear model framework to response variables that don’t follow the normal distribution. GLMs can be used to model non-normal continuous response variables, but they are most frequently used to model binary, categorical or count data. The generalised linear model can be written as:\n\\[\\begin{align}\ny_i &\\sim f(g(\\boldsymbol{\\mu}_i)) \\nonumber \\\\\n\\boldsymbol{\\mu}_i &= \\mathbf{x}_i^\\top \\boldsymbol{\\beta}, \\nonumber\n\\end{align}\\]\nwhere the response \\(y_i\\) is predicted through the linear combination \\(\\boldsymbol{\\mu}_i\\) of explanatory variables by the link function \\(g(\\cdot)\\), assuming some distribution \\(f(\\cdot)\\) for \\(y_i\\), and \\(\\mathbf{x}_i^\\top\\) is the \\(i^{th}\\) row of the design matrix \\(\\boldsymbol{X}\\). For example, the simple linear regression model above for a continuous response variable has the normal distribution distribution as \\(f(\\cdot)\\), with corresponding link function equal to the Identity function, that is, \\(g(\\boldsymbol{\\mu}_i) = \\boldsymbol{\\mu}_i\\).\nThis week we will learn how to model outcomes of interest that take one of two categorical values (e.g. yes/no, success/failure, alive/dead), i.e.\n\n\nbinary, taking the value 1 (say success, with probability \\(p_i\\)) or 0 (failure, with probability \\(1-p_i\\)) or\n\nIn this case, the distribution of \\(y_i\\) is assumed to be binomial Bin\\((1,p_i)\\). Hence, a binary response variable \\(y_i\\) has a binomial distribution with corresponding link function \\(g(\\cdot)\\) , e.g. the logit link function, that is\n\\[g(p_i) = \\log \\left(\\frac{p_i}{1 - p_i} \\right),\\]\nwhich is also referred to as the log-odds (since \\(p_i ~ / ~ 1-p_i\\) is an odds ratio). Why is such a transformation required when looking at a binary response variable? Well here we are interested in modelling the probability of success \\(p_i\\), and as we know probabilities must be between 0 and 1 \\(\\left(p_i \\in [0, 1]\\right)\\). So if we want to model the probability of success using a linear model we need to ensure that the probabilities obtained are between 0 and 1. However, if we just use the identity link function, such that\n\\[p_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta},\\] we would need to ensure that in some way \\(\\mathbf{x}_i^\\top \\boldsymbol{\\beta} \\in [0, 1]\\), that is, the linear combination of the explanatory variables and their corresponding regression coefficients was between 0 and 1. Hence some restrictions of some sort would need to be put in place to ensure this was the case. However, if we use the logit link function, such that\n\\[\\log \\left(\\frac{p_i}{1 - p_i} \\right) = \\mathbf{x}_i^\\top \\boldsymbol{\\beta},\\]\nno restrictions need to be in place on our estimates of the parameter vector \\(\\boldsymbol{\\beta}\\), since the inverse of the logit link function will always gives us valid probabilities since\n\\[p_i = \\frac{\\exp\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\right)}{1 + \\exp\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\right)} ~~~ \\in [0, 1].\\]\nThis linear regression model with a binary response variable and logit link function is referred to as logistic regression. As such, when it comes to looking at binary response variables we shall be looking at odds ratios and probabilities of success/failure. The table below is a reminder of the distribution and link function used for the normal model we have previously looked at as well as the logistic regression model we shall be examining for the rest of this week.\n\n\n\n\n\n\n\n\nModel\nRandom component\nSystematic component\nLink function\n\n\n\nNormal\n\\(y_i\\overset{\\text{indep}}\\sim \\text{N}(\\mu_i,\\sigma^2),\\)\n\\(\\boldsymbol{x}_i^\\top\\boldsymbol{\\beta} =\\beta_0 + \\beta_1x_i + \\beta_2x_i + \\ldots\\)\n\\(g(\\mu_i)=\\mu_i\\)\n\n\nLogistic\n\\(y_i\\overset{\\text{indep}}\\sim \\text{Bin}(1,p_i),\\)\n\\(\\boldsymbol{x}_i^\\top\\boldsymbol{\\beta} =\\beta_0+ \\beta_1x_i + \\beta_2x_i + \\ldots\\)\n\\(g(\\mu_i) = \\log \\left( \\frac{p_i}{1-p_i} \\right)\\)\n\n\n\nBefore we proceed, load all the packages needed for this week:\n\nCodelibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(sjPlot)\nlibrary(broom)\nlibrary(performance)\nlibrary(yardstick)  # model validation"
  },
  {
    "objectID": "index.html#generalised-linear-models",
    "href": "index.html#generalised-linear-models",
    "title": "Generalised Linear Models part 1",
    "section": "",
    "text": "The main objective this week is to introduce Generalised Linear Models (GLMs), which extend the linear model framework to response variables that don’t follow the normal distribution. GLMs can be used to model non-normal continuous response variables, but they are most frequently used to model binary, categorical or count data. The generalised linear model can be written as:\n\\[\\begin{align}\ny_i &\\sim f(g(\\boldsymbol{\\mu}_i)) \\nonumber \\\\\n\\boldsymbol{\\mu}_i &= \\mathbf{x}_i^\\top \\boldsymbol{\\beta}, \\nonumber\n\\end{align}\\]\nwhere the response \\(y_i\\) is predicted through the linear combination \\(\\boldsymbol{\\mu}_i\\) of explanatory variables by the link function \\(g(\\cdot)\\), assuming some distribution \\(f(\\cdot)\\) for \\(y_i\\), and \\(\\mathbf{x}_i^\\top\\) is the \\(i^{th}\\) row of the design matrix \\(\\boldsymbol{X}\\). For example, the simple linear regression model above for a continuous response variable has the normal distribution distribution as \\(f(\\cdot)\\), with corresponding link function equal to the Identity function, that is, \\(g(\\boldsymbol{\\mu}_i) = \\boldsymbol{\\mu}_i\\).\nThis week we will learn how to model outcomes of interest that take one of two categorical values (e.g. yes/no, success/failure, alive/dead), i.e.\n\n\nbinary, taking the value 1 (say success, with probability \\(p_i\\)) or 0 (failure, with probability \\(1-p_i\\)) or\n\nIn this case, the distribution of \\(y_i\\) is assumed to be binomial Bin\\((1,p_i)\\). Hence, a binary response variable \\(y_i\\) has a binomial distribution with corresponding link function \\(g(\\cdot)\\) , e.g. the logit link function, that is\n\\[g(p_i) = \\log \\left(\\frac{p_i}{1 - p_i} \\right),\\]\nwhich is also referred to as the log-odds (since \\(p_i ~ / ~ 1-p_i\\) is an odds ratio). Why is such a transformation required when looking at a binary response variable? Well here we are interested in modelling the probability of success \\(p_i\\), and as we know probabilities must be between 0 and 1 \\(\\left(p_i \\in [0, 1]\\right)\\). So if we want to model the probability of success using a linear model we need to ensure that the probabilities obtained are between 0 and 1. However, if we just use the identity link function, such that\n\\[p_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta},\\] we would need to ensure that in some way \\(\\mathbf{x}_i^\\top \\boldsymbol{\\beta} \\in [0, 1]\\), that is, the linear combination of the explanatory variables and their corresponding regression coefficients was between 0 and 1. Hence some restrictions of some sort would need to be put in place to ensure this was the case. However, if we use the logit link function, such that\n\\[\\log \\left(\\frac{p_i}{1 - p_i} \\right) = \\mathbf{x}_i^\\top \\boldsymbol{\\beta},\\]\nno restrictions need to be in place on our estimates of the parameter vector \\(\\boldsymbol{\\beta}\\), since the inverse of the logit link function will always gives us valid probabilities since\n\\[p_i = \\frac{\\exp\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\right)}{1 + \\exp\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\right)} ~~~ \\in [0, 1].\\]\nThis linear regression model with a binary response variable and logit link function is referred to as logistic regression. As such, when it comes to looking at binary response variables we shall be looking at odds ratios and probabilities of success/failure. The table below is a reminder of the distribution and link function used for the normal model we have previously looked at as well as the logistic regression model we shall be examining for the rest of this week.\n\n\n\n\n\n\n\n\nModel\nRandom component\nSystematic component\nLink function\n\n\n\nNormal\n\\(y_i\\overset{\\text{indep}}\\sim \\text{N}(\\mu_i,\\sigma^2),\\)\n\\(\\boldsymbol{x}_i^\\top\\boldsymbol{\\beta} =\\beta_0 + \\beta_1x_i + \\beta_2x_i + \\ldots\\)\n\\(g(\\mu_i)=\\mu_i\\)\n\n\nLogistic\n\\(y_i\\overset{\\text{indep}}\\sim \\text{Bin}(1,p_i),\\)\n\\(\\boldsymbol{x}_i^\\top\\boldsymbol{\\beta} =\\beta_0+ \\beta_1x_i + \\beta_2x_i + \\ldots\\)\n\\(g(\\mu_i) = \\log \\left( \\frac{p_i}{1-p_i} \\right)\\)"
  },
  {
    "objectID": "index.html#required-r-packages",
    "href": "index.html#required-r-packages",
    "title": "Generalised Linear Models part 1",
    "section": "",
    "text": "Before we proceed, load all the packages needed for this week:\n\nCodelibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(sjPlot)\nlibrary(broom)\nlibrary(performance)\nlibrary(yardstick)  # model validation"
  },
  {
    "objectID": "index.html#teaching-evaluation-scores",
    "href": "index.html#teaching-evaluation-scores",
    "title": "Generalised Linear Models part 1",
    "section": "\n2.1 Teaching evaluation scores",
    "text": "2.1 Teaching evaluation scores\nStudent feedback in higher education is extremely important when it comes to the evaluation of teaching techniques, materials, and improvements in teaching methods and technologies. However, there have been studies into potential bias factors when feedback is provided, such as the physical appearance of the teacher; see Economics of Education Review for details. Here, we shall look at a study from student evaluations of \\(n=463\\) professors from The University of Texas at Austin.\nPreviously, we looked at teaching score as our continuous response variable and beauty score as our explanatory variable. Now we shall consider gender as our response variable, and hence shall have a binary response variable (female/male). We will examine if there is any difference in gender by age of the teaching instructors within the evals data set.\nThe data can be downloaded below:\n Download evals dataset \nYou can download today’s session R script below:\n Download Week 6 R script \nFirst, let’s start by selecting the variables of interest from the evals data set:\n\nCodeevals &lt;- read.csv(\"evals.csv\",stringsAsFactors = T)\nevals.gender &lt;- evals %&gt;%\n                  select(gender, age)\n\n\nNow, let’s look at a boxplot of age by gender to get an initial impression of the data:\n\n\nR plot\nR code\n\n\n\n\n\n\n\nTeaching instructor age by gender.\n\n\n\n\n\n\nggplot(data = evals.gender,\n       aes(x = gender, y = age, fill = gender)) +\n  geom_boxplot() +\n  labs(x = \"Gender\", y = \"Age\") +\n  theme(legend.position = \"none\")\n\n\n\n\nHere we can see that the age of male teaching instructors tends to be higher than that of their female counterparts. Now, let’s fit a logistic regression model to see whether age is a significant predictor of the odds of a teaching instructor being male or female."
  },
  {
    "objectID": "index.html#log-odds",
    "href": "index.html#log-odds",
    "title": "Generalised Linear Models part 1",
    "section": "\n2.2 Log-odds",
    "text": "2.2 Log-odds\nTo fit a logistic regression model we will use the generalised linear model function glm and set the argument family = binomial. The logistic regression model with gender as the response and age as the explanatory variable is given by:\n\\[\n\\begin{aligned}\n\\text{Gender}_i \\sim \\mathrm{Bernoulli}(p_i) \\\\\n\\log\\left(\\frac{p_i}{1-p_i}\\right) = \\alpha_0 +\\beta_1 \\times \\text{Age}_i,\n\\end{aligned}\n\\]\nand can be fitted inR as follows:\n\nmodel &lt;- glm(gender ~ age, data = evals.gender,family = binomial)\n\nThis model uses the logit link function by default.\n\n\n\n\n\n\nNote\n\n\n\nTo use a non-default or link, pass in as an argument to binomial(). For example if we wanted to use the probit link function we could specify the following argument:\n\nglm(gender ~ age,\n    data = evals.gender,\n    family = binomial(link = \"probit\"))\n\n\n\nNow, let’s take a look at the summary produced from our logistic regression model using the tab_model function from the sjPlot library by setting transform = NULL (this will tell R to show the coefficients on the log-odds scale)\n\nmodel %&gt;% tab_model(show.ci = F,transform = NULL)\n\n\n\n\n \ngender\n\n\nPredictors\nLog-Odds\np\n\n\n(Intercept)\n-2.70\n&lt;0.001\n\n\nage\n0.06\n&lt;0.001\n\n\nObservations\n463\n\n\nR2 Tjur\n0.079\n\n\n\n\n\nAlternatively, if we want the output to be a data.frame/tibble we can use the tidy function from the broom package.\n\nmodel %&gt;% broom::tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)  -2.70      0.512      -5.27 0.000000136  \n2 age           0.0630    0.0106      5.95 0.00000000271\n\n\nFirstly, the baseline category for our binary response is female. This is due to the default baseline in R being taken as the one which comes first alphabetically, which can be seen from the levels function:\n\nlevels(evals.gender$gender)\n\n[1] \"female\" \"male\"  \n\n\nThis means that estimates from the logistic regression model are for a change on the log-odds scale for males in comparison to the response baseline females. That is\n\\[\\begin{align}\n\\ln\\left(\\frac{p}{1-p}\\right) &= \\alpha + \\beta \\times \\textrm{age} = -2.7 + 0.06 \\times \\textrm{age}, \\nonumber\n\\end{align}\\]\nwhere \\(p = \\textrm{Prob}\\left(\\textrm{Male}\\right)\\) and \\(1 - p = \\textrm{Prob}\\left(\\textrm{Female}\\right)\\).\nHence, the log-odds of the instructor being male increase by 0.06 for every one unit increase in age.\n\n\n\n\n\n\n Task\n\n\n\nSuppose now that we are interested in the log-odds of the instructor being a female (i.e. treat male as our reference category), how can we calculate the log-odds of this alternative outcome?\n\n\nTake hint\n\nRecall that the logistic regression model we fitted previously predicts the log-odds of being a male (since female is treated as our reference category), i.e.\n\\[\\begin{align}\n\\mathrm{log}\\left(\\dfrac{P(male=1)}{P(male=0)}\\right) &= \\alpha + \\beta \\cdot \\textrm{age} = -2.7 + 0.06 \\cdot \\textrm{age}\n\\end{align}\\]\nNow we are interested in :\n\\[\\begin{align}\n\\mathrm{log}\\left(\\dfrac{P(male=0)}{P(male=1)}\\right) &= -\\mathrm{log}\\left(\\dfrac{P(male=1)}{P(male=0)}\\right) \\\\\n&= -[ \\alpha + \\beta \\cdot \\textrm{age}] = -[-2.7 + 0.06] \\cdot \\textrm{age}\n\\end{align}\\]\nNote: The reference category in your data can be changed by using the relevel() function. see ?relevel for further details.\n\n\n\n\nClick here to see the solution\n\nevals.gender %&gt;% \n  mutate(gender = relevel(gender,ref = \"male\")) %&gt;%\n  glm(gender ~ age, data = . ,family = binomial) %&gt;% \n  tab_model(transform = NULL)\n\n\n\n\n \ngender\n\n\nPredictors\nLog-Odds\nCI\np\n\n\n(Intercept)\n2.70\n1.71 – 3.72\n&lt;0.001\n\n\nage\n-0.06\n-0.08 – -0.04\n&lt;0.001\n\n\nObservations\n463\n\n\nR2 Tjur\n0.079\n\n\n\n\n\n\n\n\nThis provides us with a point estimate of how the log-odds changes with age, however, we are also interested in producing a 95% confidence interval for these log-odds.\nthis can be done by setting confidence level in the tab_model function as follows:\n\nmodel %&gt;% tab_model(transform = NULL,show.ci = 0.95)\n\n\nTable 1: Logistic regression with a continuous covariate log-odds scale estimates\n\n\n\n\n\n \ngender\n\n\nPredictors\nLog-Odds\nCI\np\n\n\n(Intercept)\n-2.70\n-3.72 – -1.71\n&lt;0.001\n\n\nage\n0.06\n0.04 – 0.08\n&lt;0.001\n\n\nObservations\n463\n\n\nR2 Tjur\n0.079\n\n\n\n\n\n\n\n\nAlternativaly, you can set the option conf.int = TRUE within the tidy() function as follows:\n\nmodel %&gt;% broom::tidy(conf.int = TRUE, conf.level = 0.95)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic       p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  -2.70      0.512      -5.27 0.000000136    -3.72     -1.71  \n2 age           0.0630    0.0106      5.95 0.00000000271   0.0426    0.0841\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that a 95% is the default confidence level used in both functions. Thus, unless you need a confidence level other than the 95% CI, you can leave this argument with the default setting as will do in the following exercises.\n\n\nThe point estimate for the log-odds is 0.06, which has a corresponding 95% confidence interval of (0.04, 0.08). This can be displayed graphically using the plot_model function from the sjPlot package by simply passing our model as an argument:\n\nCodeplot_model(model, show.values = TRUE, transform = NULL,\n           title = \"Log-Odds (Male instructor)\", show.p = FALSE)\n\n\n\nThe log-odds of age for male instructors.\n\n\n\nSome of the interesting arguments that can be passed to the plot_model function are:\n\n\nshow.values = TRUE/FALSE: Whether the log-odds/odds values should be displayed;\n\nshow.p = TRUE/FALSE: Adds asterisks that indicate the significance level of estimates to the value labels;\n\ntransform: A character vector naming the function that will be applied to the estimates. The default transformation uses exp to display the odds ratios, while transform = NULL displays the log-odds; and\n\nvline.color: colour of the vertical “zero effect” line.\n\nFurther details on using plot_model can be found here.\n\n\n\n\n\n\n Task\n\n\n\nProduce a 95% Wald confidence interval (i.e., based on asymptotic normality) for the age coefficient by “hand”, i.e. using only the coefficient’s estimate and std. error. Then compare this result with the confidence interval obtained in R.\nNotice that the tidy function will compute a confidence interval using the profile-likelihood by default. Thus, if you want the results to be comparable, you will need to use the confint.default() function.\n\n\nI need a hint\n\nRecall that a 95% Wald CI for \\(\\beta\\) can be computed as \\(\\hat{\\beta} \\pm z_{1-\\alpha/2} \\times SE(\\hat{\\beta})\\), where \\(SE(\\cdot)\\) denotes the standard error of the estimate and \\(z_{1-\\alpha/2}\\) the quantile of the Normal density.\n\n\n\n\nSee the solution\n\nmod.coef.logodds &lt;- model %&gt;% tidy() %&gt;% filter(term==\"age\")\n\n# Compute Wald 95% CI by hand\nmod.coef.logodds$estimate + (c(-1,1)*\nqnorm(0.975)) * mod.coef.logodds$std.error \n\n[1] 0.04221815 0.08371129\n\n# Compare with Wald 95% CI in R\nconfint.default(model)[2,]\n\n     2.5 %     97.5 % \n0.04221815 0.08371129"
  },
  {
    "objectID": "index.html#odds",
    "href": "index.html#odds",
    "title": "Generalised Linear Models part 1",
    "section": "\n2.3 Odds",
    "text": "2.3 Odds\nTypically we would like to work on the odds scale as it is easier to interpret. To obtain the odds we simply exponentiate the log-odds, that is\n\\[\\begin{align}\n\\frac{p}{1-p} &= \\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right), \\nonumber\n\\end{align}\\]\nWhen using the tab_model function we simply set transform = \"exp\"\n\nmodel %&gt;% tab_model(transform = \"exp\",show.ci = NULL)\n\n\n\n\n \ngender\n\n\nPredictors\nOdds Ratios\np\n\n\n(Intercept)\n0.07\n&lt;0.001\n\n\nage\n1.06\n&lt;0.001\n\n\nObservations\n463\n\n\nR2 Tjur\n0.079\n\n\n\n\n\nAlternatively, for a data.frame output via the tidy function we use:\n\nmodel %&gt;% broom::tidy(exponentiate = T)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)   0.0673    0.512      -5.27 0.000000136  \n2 age           1.06      0.0106      5.95 0.00000000271\n\n\n\nOn the odds scale, the value of the intercept (0.07) gives the odds of a teaching instructor being male given their age = 0, which is obviously not a viable age for a teaching instructor, and hence why this value is very close to zero.\nFor age we have an odds of 1.06, which indicates that for every 1 unit increase in age, the odds of the teaching instructor being male increase by a factor of 1.06 (equivalently to say that the odds of being a male are 6% greater than a female for every unit increase in age).\n\nFor example, the odds of a teaching instructor who is 45 years old being male is given by\n\\[\\begin{align} \\text{Odds}(\\text{male|age=45}) &= \\frac{p_{(\\text{age=45})}}{1-p_{(\\text{age=45})}} \\\\\n&= \\exp\\left(\\alpha + \\beta \\cdot \\textrm{age = 45}\\right) \\\\\n&= \\exp\\left(-2.7 + 0.06 \\cdot 45\\right) = 1.15. \\nonumber \\end{align}\\]\nThis can be interpreted as the chances of an instructor who is 45 being male are 15% greater than them being female (Note that \\(p_{(\\text{age=45})}= \\text{Prob}(\\text{Male =1 | age =45})\\)).\nOdds ratio\nHow is the coefficient associated with age calculated? Let’s look at the odds-ratio obtained by comparing the odds of instructors aged 51 and 52 years old, that is, a one unit difference:\n\\[\\begin{align}\n\\frac{\\text{Odds}(\\text{male|age=52})}{\\text{Odds}(\\text{male|age=51})} &= \\left(\\frac{\\frac{p_{\\text{age=52}}}{1 - p_{\\text{age=52}}}}{\\frac{p_{\\text{age=51}}}{1 - p_{\\text{age=51}}}}\\right) \\\\\n&= \\frac{\\exp\\left(\\alpha + \\beta \\cdot 52\\right)}{\\exp\\left(\\alpha + \\beta \\cdot 51\\right)} = \\exp\\left(\\beta \\cdot (52 - 51)\\right) \\\\\n&= \\exp\\left(0.06\\right) = 1.06. \\nonumber\n\\end{align}\\]\nWhich is exactly the estimate we get in Table 2 below.\nLastly, we can obtain a 95% confidence interval for the odds by simply exponentiating the lower and upper bounds of our log-odds interval:\n\nmodel %&gt;% tab_model(transform = \"exp\",show.ci = 0.95) \n\n\nTable 2: Logistic regression with a continuous covariate odd-scale estimates\n\n\n\n\n\n \ngender\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.07\n0.02 – 0.18\n&lt;0.001\n\n\nage\n1.06\n1.04 – 1.09\n&lt;0.001\n\n\nObservations\n463\n\n\nR2 Tjur\n0.079\n\n\n\n\n\n\n\n\n\nmodel %&gt;% broom::tidy(exponentiate = T,conf.int = T) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n0.07\n0.51\n-5.27\n0\n0.02\n0.18\n\n\nage\n1.06\n0.01\n5.95\n0\n1.04\n1.09\n\n\n\n\n\nHence the point estimate for the odds is 1.06, which has a corresponding 95% confidence interval of (1.04, 1.09). This can be displayed graphically using the plot_model function from the sjPlot package by simply passing our model as an argument as well as removing transform = NULL (the default transformation is exponential):\n\nCodeplot_model(model, show.values = TRUE,\n           title = \"Odds (Male instructor)\", show.p = FALSE, axis.lim = c(1, 1.5))\n\n\n\nThe odds of age for male instructors.\n\n\n\nNote: axis.lim is used to zoom in on the 95% confidence interval."
  },
  {
    "objectID": "index.html#probabilities",
    "href": "index.html#probabilities",
    "title": "Generalised Linear Models part 1",
    "section": "\n2.4 Probabilities",
    "text": "2.4 Probabilities\nSince we have used the logit link function to link the linear predictor \\(\\alpha + \\beta \\cdot \\textrm{age}\\) to the probabilities of being a male, we can then obtain back the probability \\(p = \\textrm{Prob}(\\textrm{Male})\\) by using the inverse-logit transformation:\n\\[\\begin{align}\np &= \\frac{\\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right)}{1 + \\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right)} . \\nonumber\n\\end{align}\\]\nFor example, the probability of a teaching instructor who is 52 years old being male is\n\\[\\begin{align}\np &= \\frac{\\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right)}{1 + \\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right)}\n=\\frac{\\exp\\left(-2.7 + 0.06\\cdot 52 \\right)}{1 + \\exp\\left(-2.7 + 0.06\\cdot 52 \\right)}\n= 0.64, \\nonumber\n\\end{align}\\]\nwhich can be computed in R using the inverse logit plogis() function from the stats library:\n\nplogis(model$coefficients[1] \n              + model$coefficients[2] *  52)\n\n(Intercept) \n  0.6401971 \n\n\nFinally, we can plot the probability of being male using sjPlot by specifying the following:\n\nThe model we have fitted\ntype = \"pred\" to plot predicted values (marginal effects) for specific model terms.\nThe model terms of interest. Here, you can also plot the marginal effects at specific values ,e.g. selecting age[30:60] will plot the predictions based on age-values from 30 to 60, while age[all] will plot the predictions across all of the range of our independent variable (You can also provide a custom grid to evaluate you predictions, we will this cover next week).\n\n\nCodeplot_model(model, \n           type = \"pred\", \n           title = \"\", \n           terms=\"age [all]\", \n           axis.title = c(\"Age\", \"Prob. of instructor being male\"))\n\n\n\nProbability of teaching instructor being male by age.\n\n\n\nTable 3 summarises the relationship between between Odds and Probabilities:\n\n\nTable 3: Relationship between Odds and Probabilities\n\n\n\n\n\n\n\nScale\nEquivalence\n\n\n\nOdds\n\\[                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                                           Odds = \\mathrm{exp}(log Odds) = \\dfrac{P(event)}{1-P(event)}                                                                                              \n                                                                                                                                                                                                                                                                                                                           \\]\n\n\nProbability\n\\[                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                                           P(event) =\\dfrac{\\mathrm{exp}(logOdds)}{1+\\mathrm{exp}(logOdds)}  = \\dfrac{Odds}{1+Odds}                                                                  \n                                                                                                                                                                                                                                                                                                                           \\]"
  },
  {
    "objectID": "index.html#diagnostic-plots",
    "href": "index.html#diagnostic-plots",
    "title": "Generalised Linear Models part 1",
    "section": "\n3.1 Diagnostic plots",
    "text": "3.1 Diagnostic plots\nAs usual, now that we have fitted the model we need to assess how well the model fits the data and check whether our assumptions are met. Our assumptions can be checked as usual by using the plot() function in base R. However, we can visualize this with a much nicer and appropiate layout using the check_model() function within the performance library.\n\nCodelibrary(performance)\ncheck_model(model, check = c(\"pp_check\",\"binned_residuals\",\"outliers\",\"qq\"))\n\n\n\nGLM diagnostics for the gender of teaching instructors\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nBe aware that the performance library has quite a few dependencies so check whether any package needs to be updated when you install the package for the first time.\n\n\nThe output shows us 4 different diagnostic plots. These are:\n\nPosterior predictive check (Top-left): Compares the observed data (in this case the number of females and males) against the predicted number of males and females using simulated data. We can look for systematic discrepancies between real and simulated data to assess the goodness of fit (see. (Gabry et al. 2019) and check_predictions() for further details)\nBinned residuals (Top-right): Bin the observations based on their fitted values, and average the residual value in each bin. The plots shows the average residual values versus the average fitted value for each bin” (Gelman and Hill 2006). If the model were true, one would expect about 95% of the residuals to fall inside the error bounds (see ?binned_residuals for more details.)\nInfluential observations (Bottom-left): This plot is used to identify influential observations. If any points in this plot fall outside of Cook’s distance (the dashed lines) then it is considered an influential observation. See check_outliers() for further details.\nUniformity of residuals (Bottom-right): Since our response is not normally distributed, residuals are not expected to be normally distributed either. Thus, we can use QQ plots to check the uniformity of residuals, i.e. the extent to which the observed values deviate from the model expectations using simulated residuals instead of the usual Pearson residuals"
  },
  {
    "objectID": "index.html#predictive-performance-metrics",
    "href": "index.html#predictive-performance-metrics",
    "title": "Generalised Linear Models part 1",
    "section": "\n3.2 Predictive performance metrics",
    "text": "3.2 Predictive performance metrics\nSince our outcome of interest is a binary categorical variable, we can compute the predicted classes and evaluate how accurate our predictions are by comparing them against the observed values. To do so, we can add the predicted (fitted) values to our data with broom::augment() and classify them based on a decision threshold.\n\n\n\n\n\n\nNote\n\n\n\nNote that we can append either the logit-scaled fitted values (by setting type.predict = c(\"link\") or the predicted probabilities (type.predict = c(\"response\")).\n\n\nIn the case of logistic regression, you typically classify these probabilities into discrete classes based on a cutoff (commonly 0.5 for binary classification). Here is an example where predicted probabilities outcomes of \\(\\hat{p} &gt; 0.5\\) are classified as male and as female if \\(\\hat{p} \\leq 0.5\\) .\n\nCodepred_results = model %&gt;% \n  broom::augment(type.predict = c(\"response\")) %&gt;%\n  mutate(predicted_class = \n           factor(ifelse(.fitted &gt; 0.5, \"male\", \"female\")))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngender\nage\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\npredicted_class\n\n\n\nfemale\n36\n0.3938360\n-1.0006045\n0.0058192\n1.132909\n0.0019126\n-1.003529\nfemale\n\n\nfemale\n36\n0.3938360\n-1.0006045\n0.0058192\n1.132909\n0.0019126\n-1.003529\nfemale\n\n\nfemale\n36\n0.3938360\n-1.0006045\n0.0058192\n1.132909\n0.0019126\n-1.003529\nfemale\n\n\nfemale\n36\n0.3938360\n-1.0006045\n0.0058192\n1.132909\n0.0019126\n-1.003529\nfemale\n\n\nmale\n59\n0.7343825\n0.7857803\n0.0047899\n1.133280\n0.0008746\n0.787669\nmale\n\n\n\n\n\nWe can use these predicted classes to compute different predictive performance/evaluation metrics. To do so we can compute a confusion matrix according to the true and predicted classes:\n\n\n\n\nThe table can be interpreted as follows:\n\nThe correct classification rate (CCR) or accuracy describes the overall proportion of teaching instructors (males or females) that were classified correctly among all the \\(S = 463\\) individuals.\nThe true positive rate (TPR) or sensitivity (a.k.a. recall), denotes the proportion of actual male instructors that are correctly classified as males by the model.\nThe true negative rate (TNR) or specificity, denotes the proportion of actual females that have been classified correctly as females by the model. Note: the false positive rate (FPR) computed as (1- specificity) is another popular metric that measures the proportion of actual negatives that are incorrectly predicted as positive.\nThe model’s precision or positive predictive value (PPV) represents the proportion of predicted male instructors that were actually male, i.e. how many of the predicted positive cases were actually positive.\nThe model’s negative predictive value (NPV) represents the proportion of predicted female instructors that were actually females, i.e. how many of the predicted negative cases were actually negative.\n\nTo compute the confusion matrix we can use the conf_mat() function from the yardstick package and use as input the data frame with the predicted and observed classes as follows:\n\nlibrary(yardstick)\nconf_mat(pred_results,truth = gender,estimate = predicted_class)\n\n          Truth\nPrediction female male\n    female     76   60\n    male      119  208\n\n\nRather than computing these predictive performance metrics by hand, we can take advantage of the metric_set() function from the yardstick library to combine multiple metric functions together into a new function that calculates all of them at once. Then we just simply need to supply the same arguments we used for computing the confusion matrix.\n\n# Step (1) create a set with classification metrics we want to compute\neval_metric &lt;- metric_set(accuracy,sensitivity,specificity,ppv,npv)\n# Step (2) call out the metric set and input the data containing the observed and predicted classes\neval_metric(pred_results,\n            truth = gender,\n            estimate = predicted_class,  \n            event_level = \"second\") \n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\naccuracy\nbinary\n0.61\n\n\nsensitivity\nbinary\n0.39\n\n\nspecificity\nbinary\n0.78\n\n\nppv\nbinary\n0.56\n\n\nnpv\nbinary\n0.64\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNotice that we passed on the argument event_level = \"second\" since the default behavior of the eval_metric() function created via metric_set() is to consider the first class of the variable genderas the event. If you recall, female appears as first due alphabetical order, and hence is treated as our reference category in our model and thus we are modelling the event \\(\\mathbb{Pr}(male)\\)\n\nlevels(pred_results$gender)\n\n[1] \"female\" \"male\"  \n\n\nHowever since female appear first, we need to tell our function that we are interested in the second class of the variables gender , i.e. male, as the event and not the other way around.\n\n\nWe can see that our model is not doing a great job in predicting the outcome (accuracy of \\(\\approx\\) 60%).This is due to a particularly poor performance in terms of specificity, i.e. roughly only 40% of truly male instructors are correctly classified as such by our mode. Despite this, our model is not doing a terrible job in terms of sensitivity since almost 80% of the females are actually being predicted as such.\nNote that the threshold of 0.5 is common but may not always be optimal. You can adjust it based on your specific application and the desired balance between sensitivity and specificity (we will see an example of such in the next task).\n\n\n\n\n\n\nNote\n\n\n\nFor this course we have been using only our data to assess the predicative performance of our models. In reality, if we were to properly assess the classification performance of our models, we should look at out-of-sample prediction by first splitting the data into a training and a test set, then fitting the model to the training data and finally predicting the class of the each observation in the test data that has been held back. Otherwise we run the risk of overstating the classification accuracy of the model."
  },
  {
    "objectID": "index.html#roc-curve",
    "href": "index.html#roc-curve",
    "title": "Generalised Linear Models part 1",
    "section": "\n3.3 ROC Curve",
    "text": "3.3 ROC Curve\nAnother way to assess how good the model is at separating the 2 classes of the outcome is through the Receiver Operating Characteristic (ROC) curve. The ROC curve is a graphical representation used to evaluate the performance of a binary classification model. It shows the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) at various threshold settings.\nThe threshold determines the cutoff probability for classifying a sample as positive or negative (male or female in our case).\n\nAt a cutoff \\(=0\\) , all the observations will be classified as positive (i.e. all the instructors are going to be classified as males since we are modelling the event \\(P(male)\\)).\nAt a cutoff \\(=1\\) , all the observations will be classified as negative (i.e. all the instructors are going to be classified as females corresponding to \\(1 - P(male)\\)).\n\nAs the threshold moves between 0 and 1, the ROC curve traces out points that show the model’s performance for various balances between TPR and FPR.\nWe can combine the roc_curve() function from the yardstick library and autoplot() function from ggplot to calculate and visualize the ROC curve. We just need to supply the true classes and the predicted probabilities (NOT the predicted classes!).\n\nroc_curve(pred_results,\n          truth = gender,\n          .fitted,\n          event_level = \"second\") %&gt;%\n  autoplot()\n\n\n\nROC curve for GLM fitted to the evaluation scores data set.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nSimilarly to the the evaluation metric function we had before, we set event_level = \"second\" since the default behavior of the roc_auc() function is also to consider the first class of the variable genderas the event. Thus, we need to tell the roc_auc() that we are interested in the male level as our event, which is the second class of the variable gender.\n\n\nThe closer the ROC curve is to the top-left corner, the better the model is at distinguishing between the positive and negative classes. This means high true positive rates (sensitivity) and low false positive rates. The closer the curve is to the diagonal line (i.e. when TPR = FPR) then the performance is no better than random guessing. We can see here that our model is doing a little bit better than just random guessing.\nWe can also calculate the area under the ROC curve (AUC) as a single value to summarize the model’s performance:\n\nyardstick::roc_auc(pred_results,\n        truth = gender,\n        .fitted,\n        event_level = \"second\")\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\nroc_auc\nbinary\n0.66\n\n\n\n\nThe AUC ranges from 0 to 1:\n\nAUC = 1: Perfect classifier.\nAUC = 0.5: No better than random guessing.\nAUC \\(\\leq\\) 0.5: Worse than random guessing, meaning the model is consistently misclassifying.\n\nAgain, the AUC of 0.66 indicates a moderate-poor fit to the data (mainly due to miss-classifications of male instructors as we saw previously).\n\n\n\n\n\n\n Task\n\n\n\nThe Youden index (Youden’s J statistics) is a summary metric that helps to identify the optimal threshold that maximizes both sensitivity (true positive rate) and specificity (true negative rate) simultaneously. It is computed as follows:\n\\[ J = \\mathrm{sensitivity} + \\mathrm{specificity} -1 \\]\nCompute the Youden index and find the optimal threshold that maximizes both sensitivity and specificity in the logistic regression model fitted to the evaluation scores data set.\n\n\nTake hint\n\nYou can use the roc_curve() function to compute the sensitivity and specificity values at different thresholds. Then use these values to compute Youden’s index and find the threshold value associated with the maximum value of Youden’s index.\n\n\n\n\nClick here to see the solution\n\nroc_curve(pred_results,\n          truth = gender,\n          .fitted,\n          event_level = \"second\") %&gt;% \n  mutate(youden_j = sensitivity + specificity - 1) %&gt;% \n  filter(youden_j == max(youden_j)) %&gt;% \n  select(.threshold)\n\n# A tibble: 1 × 1\n  .threshold\n       &lt;dbl&gt;\n1      0.709"
  },
  {
    "objectID": "index.html#log-odds-1",
    "href": "index.html#log-odds-1",
    "title": "Generalised Linear Models part 1",
    "section": "\n4.1 Log-odds",
    "text": "4.1 Log-odds\nThe logistic regression model is given by:\n\\[\\begin{align}\ny_i &\\sim \\mathrm{Bernoulli}(p_i)\\\\\n\\mathrm{logit}(p_i) &= \\alpha +  \\beta_{\\text{ethnicity}}  \\times \\mathbb{I}_{\\text{ethnicity}}(\\mathrm{not~  minority}).\n\\end{align}\\]\nWhich can be fitted in R as follows:\n\nmodel.ethnic &lt;- glm(gender ~ ethnicity,\n                    data = evals.ethnic,\n                    family = binomial) \n\nHere, \\(y_i\\) denotes the \\(i\\)th instructor’s gender, again, the baseline category for our binary response is female. Thus, \\(p_i = \\mathrm{Prob}(\\mathrm{Male})\\) is linked to the linear predictor through the logit link function. This means that estimates we get from fitting the logistic regression model are for a change on the log-odds scale for males (\\(p_i = \\textrm{Prob}(\\textrm{Males})\\)) in comparison to the response baseline females.\nAlso, the baseline category for our explanatory variable is minority, which, like gender, is done alphabetically by default by R:\n\nlevels(evals.ethnic$ethnicity)\n\n[1] \"minority\"     \"not minority\"\n\n\nThus, \\(\\alpha\\) correspond to the log-odds of the instructors being a male given that they are on the minority baseline category.\nThen, \\(\\mathbb{I}_{\\text{ethnicity}}(\\text{not minority})\\) is an indicator function for those instructors in the not minority group and \\(\\beta_{\\text{ethnicity}}\\) represent the change in the log-odds of a male instructor that is not on the minority group.\nLets break this down. The model we have fitted is:\n\\[\n\\mathrm{log}\\left(\\dfrac{p_i}{1-p_i}\\right) = \\alpha + \\beta_{\\text{ethnicity}}  \\times \\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~  minority})\n\\]\n\n\\(\\alpha\\) is the intercept, representing the log-odds when \\(\\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~ minority}) = 0\\) (i.e., when the instructor is in the minority group). When the instructor belongs to the reference category minority the models simplifies to: \\[\\mathrm{log}\\left(\\frac{p_i}{1-p_i}\\right) = \\alpha \\]\n\\(\\beta_{\\mathrm{ethnicity}}\\) is the coefficient for the predictor \\(\\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~ minority})\\), which shows how the log-odds change when moving from the reference category (minority) to the other level (not minority). When the instructor does not belong to reference category, i.e. \\(\\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~ minority}) = 1\\), the model becomes: \\[\\mathrm{log}\\left(\\dfrac{p_i}{1-p_i}\\right) = \\alpha + \\beta_{\\text{ethnicity}}\\]\n\nSo, the log-odds of the instructors being male in the not minority group are \\(\\alpha +\\beta_{\\text{ethnicity}}\\). Lets compute the model estimates and 95 % confidence intervals for the log-odds:\n\nmodel.ethnic %&gt;%\n  tab_model(transform = NULL)\n\n\nTable 4: Logistic regression with a categorical covariate log-odds scale estimates\n\n\n\n\n\n \ngender\n\n\nPredictors\nLog-Odds\nCI\np\n\n\n(Intercept)\n-0.25\n-0.75 – 0.24\n0.319\n\n\nethnicity [not minority]\n0.66\n0.13 – 1.20\n0.015\n\n\nObservations\n463\n\n\nR2 Tjur\n0.013\n\n\n\n\n\n\n\n\nThe fitted model is:\n\\[\\begin{align}\n\\ln\\left(\\frac{\\hat{p_i}}{1-\\hat{p_i}}\\right) &= -0.25 + 0.66 \\cdot \\mathbb{I}_{\\text{ethnicity}}(\\text{not minority}), \\nonumber\n\\end{align}\\]\nHence, the log-odds of an instructor being male increase by 0.66 if they are in the ethnicity group not minority, which has a corresponding 95% confidence interval of (0.13, 1.2).\nThis can be displayed graphically using the plot_model function from the sjPlot package by simply passing our model as an argument:\n\nCodeplot_model(model.ethnic, \n           show.values = TRUE, \n           transform = NULL, \n           title = \"Log-Odds (Male instructor)\", \n           show.p = FALSE)\n\n\n\nThe log-odds for male instructors by ethnicity (not a minority)."
  },
  {
    "objectID": "index.html#odds-1",
    "href": "index.html#odds-1",
    "title": "Generalised Linear Models part 1",
    "section": "\n4.2 Odds",
    "text": "4.2 Odds\nOn the odds scale the regression coefficients are given by\n\nmodel.ethnic %&gt;%\n  tab_model(transform = \"exp\")\n\n\nTable 5: Logistic regression with a categorical covariate odd-scale estimates\n\n\n\n\n\n \ngender\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.78\n0.47 – 1.27\n0.319\n\n\nethnicity [not minority]\n1.94\n1.14 – 3.33\n0.015\n\n\nObservations\n463\n\n\nR2 Tjur\n0.013\n\n\n\n\n\n\n\n\nThe (Intercept) gives us the odds of the instructor being male given that they are in the minority ethnic group, that is, 0.78 (the indicator function is zero in that case).\nThe odds of the instructor being male given they are in the not minority ethnic group are 1.94 times greater than the odds if they were in the minority ethnic group.\nBefore moving on, let’s break down how these values are computed. First, the odds of the instructor being male given that they are in the minority ethnic group (reference category) can be obtained as follows:\n\\[\\begin{align}\n\\mathrm{Odds}(\\mathrm{male = 1} | \\mathrm{minority} =1) &= \\dfrac{p_{(minority=1)}}{1 - p_{(minority=1)}}\\\\\n&= \\exp\\left(\\alpha\\right) \\\\\n&= \\exp\\left(-0.25\\right) \\\\\n&= 0.78. \\nonumber\n\\end{align}\\]\nHere, \\(p_{(minority=1)} = \\mathrm{Prob}\\left(\\mathrm{Male}=1 | \\mathrm{minority}=1\\right)\\). Thus, the odds of the instructor in the minority group being a male are 0.78 the odds of an instructor being a female in the same group.\nSlightly confusing right? Maybe it makes more sense to interpret this result in terms of the female group, i.e. \\(\\mathrm{Odds}(\\mathrm{female} = 1 | \\mathrm{minority} =1)\\). However you must be aware that this is not a probability! so you can not simply compute \\(1- \\mathrm{exp}(\\alpha)\\). To compute this correctly, we should do as follows:\n\\[\\begin{align}\n\\mathrm{Odds}(\\mathrm{female} = 1 | \\mathrm{minority} =1) &= \\dfrac{P(\\mathrm{female}=1 |\\mathrm{minority}=1)}{P(\\mathrm{male}= 1 |\\mathrm{minority}=1)}\\\\\n&= \\left[\\dfrac{P(\\mathrm{male}=1 |\\mathrm{minority}=1)}{P(\\mathrm{female}= 1 |\\mathrm{minority}=1)}\\right]^{-1}\\\\\n&= \\left[\\mathrm{Odds}(\\mathrm{male}=1|\\mathrm{minority}=1)\\right]^{-1}\\\\\n&= \\mathrm{exp}(\\alpha)^{-1} \\\\ &= \\exp\\left(-0.25\\right)^{-1} = 1.28\n\\end{align}\\]\nThus, the odds of the instructor in the minority group being a female are 28% higher than the odds of an instructor being a male in the same group. However, by looking at the confidence intervals and p-value we can see that this difference is not statistically significant.\n\n\n\n\n\n\n Task\n\n\n\nUse the relevel function to fit a logistic regression that estimates the odds of the instructor being female given that they are in the minority ethnic group directly.\n\n\nTake hint\n\nThe reference category in your data can be changed by using the relevel() function. see ?relevel for further details.\n\n\n\n\nClick here to see the solution\n\nevals.ethnic %&gt;%\n  mutate(gender = relevel(gender,ref = \"male\")) %&gt;% \n  glm(gender ~ ethnicity, data = . ,family = binomial) %&gt;% \n  broom::tidy(exponentiate = T) %&gt;% \n  filter(term==\"(Intercept)\") %&gt;%\n  pull(estimate) \n\n[1] 1.285714\n\n\n\n\n\nThe ethnicity [not minority] coefficient gives us the odds of the instructor being male given they are NOT in the minority group, i.e.,\n\\[\n\\begin{aligned}\n\\text{Odds}(\\text{male = 1 | minority = 0}) &= \\dfrac{p_{(\\text{minority=0})}}{1- p_{(\\text{minority=0})}}\\\\\n&= \\exp(\\alpha + \\beta_{\\text{ethnicity}}) \\\\\n&= \\exp(-0.25 + 0.66) = 1.51\n\\end{aligned}\n\\]\nThus, non-minority-group instructors have 1.51 times higher odds of being male compared to female.\nComparing the odds-ratio\nNow, let’s look a the odds-ratio of an instructor being male in the not minority group compared to the minority ethnic group, i.e., \\(\\dfrac{\\text{Odds}(\\text{male=1|minority=0})}{\\text{Odds}(\\text{male=1|minority=1})}\\).\n\n\nFirst, the odds of an instructor being male given that they are in the minority ethnic group were:\n\\[\\begin{align}\n\\mathrm{Odds}(\\mathrm{male} = 1 | \\mathrm{minority} = 1) =&\n\\dfrac{p_{(minority=1)}}{1 - p_{(minority=1)}} \\\\\n&= \\exp\\left(\\alpha\\right) \\\\ &= 0.78. \\nonumber\n\\end{align}\\]\n\n\nRecall that the log-odds of an instructor not belonging to the minority group (i.e. not minority) being a male are \\(\\alpha +\\beta_{\\text{ethnicity}}\\). On the odds scale this is:\n\\[\\begin{align}\n\\mathrm{Odds}(\\mathrm{male} = 1 | \\mathrm{minority} = 0) =& \\dfrac{p_{(\\mathrm{minority} = 0)}}{1- p_{(\\mathrm{minority} = 0)}}\\\\\n&= \\mathrm{exp}( \\alpha + \\beta_{\\text{ethnicity}}) \\\\\n&= 1.51\n\\end{align}\\]\n\n\nNow, the odds of an instructor that is not in the minority group being male against the odds of an instructor that comes from the minority group is given by the ratio of the odds we just calculated:\n\\[\\begin{align}\n\\frac{\\mathrm{Odds}(\\mathrm{male} = 1| \\mathrm{minority} = 0)}{\\mathrm{Odds}(\\mathrm{male} = 1| \\mathrm{minority} = 1)} &= \\dfrac{\\frac{p_{(\\mathrm{minority} = 0)}}{1- p_{(\\mathrm{minority} = 0)}}}{\\frac{p_{(\\mathrm{minority}=1)}}{1- p_{(\\mathrm{minority}=1)}}} \\\\\n&= \\frac{\\mathrm{exp}( \\alpha + \\beta_{\\text{ethnicity}})}{\\exp\\left(\\alpha\\right)}\\\\\n&= \\exp\\left(\\alpha + \\beta_{\\text{ethnicity}} - \\alpha\\right) \\\\\n&= \\exp\\left(\\beta_{\\text{ethnicity}}\\right) = \\exp\\left(0.66 \\right) \\\\\n&= 1.93. \\nonumber\n\\end{align}\\]\n\n\nWhich is the coefficient estimate we got from the model summaries ( Table 5 ). This means that instructors that are not in the minority groups are significantly 1.93 times more likely to be males compared to instructors in the minority group.\nThe corresponding 95% confidence interval of (1.14, 3.33). Again, we can display this graphically using the plot_model function from the sjPlot package:\n\nCodeplot_model(model.ethnic,\n           transform = \"exp\",\n           show.values = TRUE,\n           title = \"Odds (Male instructor)\",\n           show.p = FALSE)\n\n\n\nThe odds-ratio of a male instructor given they are in the not minority group.\n\n\n\n\n\n\n\n\n\n Task\n\n\n\nCalculate the odds of a male instructor that is not on the minority group against the odds of a female instructor that is in the minority group.\n\n\nTake hint\n\nThe odds ratio of a male instructor that is not on the minority group against the odds of a female instructor that is in the minority group are given by:\n\\[\n\\dfrac{\\text{Odds}(\\text{male=1|minority=0})}{\\text{Odds}(\\text{female=1|minority=1})}\n\\]\n\n\n\nSee Solution\n\n\\[\n\\begin{aligned}\n\\dfrac{\\text{Odds}(\\text{male=1|minority=0})}{\\text{Odds}(\\text{female=1|minority=1})} &= \\dfrac{\\exp(\\alpha+\\beta_{\\text{ethnicity}}) }{\\exp(\\alpha)^{-1}} = \\dfrac{1.51}{1.28} \\approx 1.18\n\\end{aligned}\n\\]\nNon-minority male instructors have 1.18 times the odds of being male compared to a female in the minority group."
  },
  {
    "objectID": "index.html#probabilities-1",
    "href": "index.html#probabilities-1",
    "title": "Generalised Linear Models part 1",
    "section": "\n4.3 Probabilities",
    "text": "4.3 Probabilities\nYou can also use the model to predict the probability of having an instructor being male for each ethnicity group.\nThe probabilities of an instructor being male given they are in the minority and not minority groups are:\n\nplogis(coef(model.ethnic)[1])\n\n(Intercept) \n     0.4375 \n\nplogis(coef(model.ethnic)[1]+coef(model.ethnic)[2])\n\n(Intercept) \n  0.6015038 \n\n\nHence, the probabilities of an instructor being male given they are in the minority and not minority ethnic groups are 0.437 and 0.602, respectively.\nFinally, we can use the plot_model() function from the sjPlot package to produce the estimated probabilities by ethnicity as follows:\n\nCodeplot_model(model.ethnic,\n           type = \"pred\",\n           terms = \"ethnicity\",\n           axis.title = c(\"Ethnicity\", \n                          \"Prob. of instructor being male\"),\n           title = \" \")\n\n\n\nProbability of teaching instructor being male by ethnicity."
  }
]