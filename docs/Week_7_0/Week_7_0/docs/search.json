[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Week 7 Further Tasks",
    "section": "",
    "text": "Case 1: Yanny or Laurel?\nThis auditory illusion first appeared on the internet in May 2018. An explanation of why people hear different things can be found in this short video, just one of many internet sources discussing the phenomenon. The main reason behind the difference appears to be that as we age we lose the ability to hear certain sounds. To see if we could find evidence of such an age effect, we asked students and staff at the School of Mathematics and Statistics at the University of Glasgow to fill out a survey on what they hear. Below you can see summaries of the responses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe proportions hearing Yanny and Laurel are very similar to each other, and there are some respondents who hear both or even something completely different. This may be because people do not listen to the audio file using the same device, something we couldn’t control for in the survey. Ignoring the responses other than Yanny or Laurel, we have 53 observations.\nDownload the yanny.csv data and fit a logistic regression model with hear as the binary response variable, and age and gender as the explanatory variables. What are your findings?\n\n\n Download yanny.csv\n\n\n\n\nCase 2: Surviving the Titanic\nOn 15th April 1912, during its maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\nDownload the data (titanic.csv) for \\(n = 891\\) passengers aboard the Titanic and fit a logistic regression model with survived as the binary response variable, and age, gender, and passenger.class as the explanatory variables. What are your findings?\n\n\n Download titanic.csv"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Week 7: Generalised Linear Models",
    "section": "",
    "text": "In previous weeks we looked at modelling data using linear regression models were we had:\n\na continuous response variable \\(y\\) and\none or more explanatory variables \\(x_1, x_2,\\ldots, x_p\\), which were numerical/categorical variables.\n\nRecall that for data \\((y_i, x_i), ~ i = 1,\\ldots, n\\), where \\(y\\) is a continuous response variable, we can write a simple linear regression model as follows:\n\\[y_i = \\alpha + \\beta x_i + \\epsilon_i, ~~~~ \\epsilon_i \\sim N(0, \\sigma^2),\\] where\n\n\n\\(y_i\\) is the \\(i^{th}\\) observation of the continuous response variable;\n\n\\(\\alpha\\) is the intercept of the regression line;\n\n\\(\\beta\\) is the slope of the regression line;\n\n\\(x_i\\) is the \\(i^{th}\\) observation of the explanatory variable; and\n\n\\(\\epsilon_i\\) is the \\(i^{th}\\) random component.\n\nThus, the full probability model for \\(y_i\\) given \\(x_i\\) (\\(y_i | x_i\\)) can be written as\n\\[y_i | x_i \\sim N(\\alpha + \\beta x_i, \\sigma^2),\\]\nwhere the mean \\(\\alpha + \\beta x_i\\) is given by the deterministic part of the model and the variance \\(\\sigma^2\\) by the random part. Hence we make the assumption that the outcomes \\(y_i\\) are normally distributed with mean \\(\\alpha + \\beta x_i\\) and variance \\(\\sigma^2\\). However, what if our response variable \\(y\\) is not a continuous random variable?\n\nThe main objective this week is to introduce Generalised Linear Models (GLMs), which extend the linear model framework to response variables that don’t follow the normal distribution. GLMs can be used to model non-normal continuous response variables, but they are most frequently used to model binary, categorical or count data. The generalised linear model can be written as:\n\\[\\begin{align}\ny_i &\\sim f(g(\\boldsymbol{\\mu}_i)) \\nonumber \\\\\n\\boldsymbol{\\mu}_i &= \\mathbf{x}_i^\\top \\boldsymbol{\\beta}, \\nonumber\n\\end{align}\\]\nwhere the response \\(y_i\\) is predicted through the linear combination \\(\\boldsymbol{\\mu}_i\\) of explanatory variables by the link function \\(g(\\cdot)\\), assuming some distribution \\(f(\\cdot)\\) for \\(y_i\\), and \\(\\mathbf{x}_i^\\top\\) is the \\(i^{th}\\) row of the design matrix \\(\\boldsymbol{X}\\). For example, the simple linear regression model above for a continuous response variable has the normal distribution distribution as \\(f(\\cdot)\\), with corresponding link function equal to the Identity function, that is, \\(g(\\boldsymbol{\\mu}_i) = \\boldsymbol{\\mu}_i\\).\nThis week we will learn how to model outcomes of interest that take one of two categorical values (e.g. yes/no, success/failure, alive/dead), i.e.\n\n\nbinary, taking the value 1 (say success, with probability \\(p_i\\)) or 0 (failure, with probability \\(1-p_i\\)) or\n\nIn this case, the distribution of \\(y_i\\) is assumed to be binomial Bin\\((1,p_i)\\). Hence, a binary response variable \\(y_i\\) has a binomial distribution with corresponding link function \\(g(\\cdot)\\) , e.g. the logit link function, that is\n\\[g(p_i) = \\log \\left(\\frac{p_i}{1 - p_i} \\right),\\]\nwhich is also referred to as the log-odds (since \\(p_i ~ / ~ 1-p_i\\) is an odds ratio). Why is such a transformation required when looking at a binary response variable? Well here we are interested in modelling the probability of success \\(p_i\\), and as we know probabilities must be between 0 and 1 \\(\\left(p_i \\in [0, 1]\\right)\\). So if we want to model the probability of success using a linear model we need to ensure that the probabilities obtained are between 0 and 1. However, if we just use the identity link function, such that\n\\[p_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta},\\] we would need to ensure that in some way \\(\\mathbf{x}_i^\\top \\boldsymbol{\\beta} \\in [0, 1]\\), that is, the linear combination of the explanatory variables and their corresponding regression coefficients was between 0 and 1. Hence some restrictions of some sort would need to be put in place to ensure this was the case. However, if we use the logit link function, such that\n\\[\\log \\left(\\frac{p_i}{1 - p_i} \\right) = \\mathbf{x}_i^\\top \\boldsymbol{\\beta},\\]\nno restrictions need to be in place on our estimates of the parameter vector \\(\\boldsymbol{\\beta}\\), since the inverse of the logit link function will always gives us valid probabilities since\n\\[p_i = \\frac{\\exp\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\right)}{1 + \\exp\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\right)} ~~~ \\in [0, 1].\\]\nThis linear regression model with a binary response variable and logit link function is referred to as logistic regression. As such, when it comes to looking at binary response variables we shall be looking at odds ratios and probabilities of success/failure. The table below is a reminder of the distribution and link function used for the normal model we have previously looked at as well as the logistic regression model we shall be examining for the rest of this week.\n\n\n\n\n\n\n\n\nModel\nRandom component\nSystematic component\nLink function\n\n\n\nNormal\n\\(y_i\\overset{\\text{indep}}\\sim \\mbox{N}(\\mu_i,\\sigma^2),\\)\n\\(\\boldsymbol{x}_i^\\top\\boldsymbol{\\beta} =\\beta_0 + \\beta_1x_i + \\beta_2x_i + \\ldots\\)\n\\(g(\\mu_i)=\\mu_i\\)\n\n\nLogistic\n\\(y_i\\overset{\\text{indep}}\\sim \\mbox{Bin}(1,p_i),\\)\n\\(\\boldsymbol{x}_i^\\top\\boldsymbol{\\beta} =\\beta_0+ \\beta_1x_i + \\beta_2x_i + \\ldots\\)\n\\(g(\\mu_i) = \\log \\left( \\frac{p_i}{1-p_i} \\right)\\)\n\n\n\nBefore we proceed, load all the packages needed for this week:\n\nCodelibrary(tidyr)\nlibrary(ggplot2)\nlibrary(moderndive)\nlibrary(sjPlot)\nlibrary(tidymodels)\nlibrary(broom)\nlibrary(performance)\nlibrary(janitor)"
  },
  {
    "objectID": "index.html#generalised-linear-models",
    "href": "index.html#generalised-linear-models",
    "title": "Week 7: Generalised Linear Models",
    "section": "",
    "text": "The main objective this week is to introduce Generalised Linear Models (GLMs), which extend the linear model framework to response variables that don’t follow the normal distribution. GLMs can be used to model non-normal continuous response variables, but they are most frequently used to model binary, categorical or count data. The generalised linear model can be written as:\n\\[\\begin{align}\ny_i &\\sim f(g(\\boldsymbol{\\mu}_i)) \\nonumber \\\\\n\\boldsymbol{\\mu}_i &= \\mathbf{x}_i^\\top \\boldsymbol{\\beta}, \\nonumber\n\\end{align}\\]\nwhere the response \\(y_i\\) is predicted through the linear combination \\(\\boldsymbol{\\mu}_i\\) of explanatory variables by the link function \\(g(\\cdot)\\), assuming some distribution \\(f(\\cdot)\\) for \\(y_i\\), and \\(\\mathbf{x}_i^\\top\\) is the \\(i^{th}\\) row of the design matrix \\(\\boldsymbol{X}\\). For example, the simple linear regression model above for a continuous response variable has the normal distribution distribution as \\(f(\\cdot)\\), with corresponding link function equal to the Identity function, that is, \\(g(\\boldsymbol{\\mu}_i) = \\boldsymbol{\\mu}_i\\).\nThis week we will learn how to model outcomes of interest that take one of two categorical values (e.g. yes/no, success/failure, alive/dead), i.e.\n\n\nbinary, taking the value 1 (say success, with probability \\(p_i\\)) or 0 (failure, with probability \\(1-p_i\\)) or\n\nIn this case, the distribution of \\(y_i\\) is assumed to be binomial Bin\\((1,p_i)\\). Hence, a binary response variable \\(y_i\\) has a binomial distribution with corresponding link function \\(g(\\cdot)\\) , e.g. the logit link function, that is\n\\[g(p_i) = \\log \\left(\\frac{p_i}{1 - p_i} \\right),\\]\nwhich is also referred to as the log-odds (since \\(p_i ~ / ~ 1-p_i\\) is an odds ratio). Why is such a transformation required when looking at a binary response variable? Well here we are interested in modelling the probability of success \\(p_i\\), and as we know probabilities must be between 0 and 1 \\(\\left(p_i \\in [0, 1]\\right)\\). So if we want to model the probability of success using a linear model we need to ensure that the probabilities obtained are between 0 and 1. However, if we just use the identity link function, such that\n\\[p_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta},\\] we would need to ensure that in some way \\(\\mathbf{x}_i^\\top \\boldsymbol{\\beta} \\in [0, 1]\\), that is, the linear combination of the explanatory variables and their corresponding regression coefficients was between 0 and 1. Hence some restrictions of some sort would need to be put in place to ensure this was the case. However, if we use the logit link function, such that\n\\[\\log \\left(\\frac{p_i}{1 - p_i} \\right) = \\mathbf{x}_i^\\top \\boldsymbol{\\beta},\\]\nno restrictions need to be in place on our estimates of the parameter vector \\(\\boldsymbol{\\beta}\\), since the inverse of the logit link function will always gives us valid probabilities since\n\\[p_i = \\frac{\\exp\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\right)}{1 + \\exp\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\right)} ~~~ \\in [0, 1].\\]\nThis linear regression model with a binary response variable and logit link function is referred to as logistic regression. As such, when it comes to looking at binary response variables we shall be looking at odds ratios and probabilities of success/failure. The table below is a reminder of the distribution and link function used for the normal model we have previously looked at as well as the logistic regression model we shall be examining for the rest of this week.\n\n\n\n\n\n\n\n\nModel\nRandom component\nSystematic component\nLink function\n\n\n\nNormal\n\\(y_i\\overset{\\text{indep}}\\sim \\mbox{N}(\\mu_i,\\sigma^2),\\)\n\\(\\boldsymbol{x}_i^\\top\\boldsymbol{\\beta} =\\beta_0 + \\beta_1x_i + \\beta_2x_i + \\ldots\\)\n\\(g(\\mu_i)=\\mu_i\\)\n\n\nLogistic\n\\(y_i\\overset{\\text{indep}}\\sim \\mbox{Bin}(1,p_i),\\)\n\\(\\boldsymbol{x}_i^\\top\\boldsymbol{\\beta} =\\beta_0+ \\beta_1x_i + \\beta_2x_i + \\ldots\\)\n\\(g(\\mu_i) = \\log \\left( \\frac{p_i}{1-p_i} \\right)\\)"
  },
  {
    "objectID": "index.html#required-r-packages",
    "href": "index.html#required-r-packages",
    "title": "Week 7: Generalised Linear Models",
    "section": "",
    "text": "Before we proceed, load all the packages needed for this week:\n\nCodelibrary(tidyr)\nlibrary(ggplot2)\nlibrary(moderndive)\nlibrary(sjPlot)\nlibrary(tidymodels)\nlibrary(broom)\nlibrary(performance)\nlibrary(janitor)"
  },
  {
    "objectID": "index.html#teaching-evaluation-scores",
    "href": "index.html#teaching-evaluation-scores",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n2.1 Teaching evaluation scores",
    "text": "2.1 Teaching evaluation scores\nStudent feedback in higher education is extremely important when it comes to the evaluation of teaching techniques, materials, and improvements in teaching methods and technologies. However, there have been studies into potential bias factors when feedback is provided, such as the physical appearance of the teacher; see Economics of Education Review for details. Here, we shall look at a study from student evaluations of \\(n=463\\) professors from The University of Texas at Austin.\nPreviously, we looked at teaching score as our continuous response variable and beauty score as our explanatory variable. Now we shall consider gender as our response variable, and hence shall have a binary response variable (female/male). We will examine if there is any difference in gender by age of the teaching instructors within the evals data set.\nFirst, let’s start by selecting the variables of interest from the evals data set:\n\nCodeevals.gender &lt;- evals %&gt;%\n                  select(gender, age)\n\n\nNow, let’s look at a boxplot of age by gender to get an initial impression of the data:\n\n\nR plot\nR code\n\n\n\n\n\n\n\nTeaching instructor age by gender.\n\n\n\n\n\n\nggplot(data = evals.gender,\n       aes(x = gender, y = age, fill = gender)) +\n  geom_boxplot() +\n  labs(x = \"Gender\", y = \"Age\") +\n  theme(legend.position = \"none\")\n\n\n\n\nHere we can see that the age of male teaching instructors tends to be higher than that of their female counterparts. Now, let’s fit a logistic regression model to see whether age is a significant predictor of the odds of a teaching instructor being male or female."
  },
  {
    "objectID": "index.html#log-odds",
    "href": "index.html#log-odds",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n2.2 Log-odds",
    "text": "2.2 Log-odds\nTo fit a logistic regression model we will use the generalised linear model function glm and set the argument family = binomial. The logistic regression model with gender as the response and age as the explanatory variable is given by:\n\nmodel &lt;- glm(gender ~ age, data = evals.gender,family = binomial)\n\nThis model uses the logit link function by default.\n\n\n\n\n\n\nNote\n\n\n\nTo use a non-default or link, pass in as an argument to binomial(). For example if we wanted to use the probit link function we could specify the following argument:\n\nglm(gender ~ age,\n    data = evals.gender,\n    family = binomial(link = \"probit\"))\n\n\n\nNow, let’s take a look at the summary produced from our logistic regression model using the tidy function from the broom library:\n\nmodel %&gt;% broom::tidy()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n-2.6979460\n0.5119379\n-5.270066\n1e-07\n\n\nage\n0.0629647\n0.0105852\n5.948386\n0e+00\n\n\n\n\n\nFirstly, the baseline category for our binary response is female. This is due to the default baseline in R being taken as the one which comes first alphabetically, which can be seen from the levels function:\n\nlevels(evals.gender$gender)\n\n[1] \"female\" \"male\"  \n\n\nThis means that estimates from the logistic regression model are for a change on the log-odds scale for males in comparison to the response baseline females. That is\n\\[\\begin{align}\n\\ln\\left(\\frac{p}{1-p}\\right) &= \\alpha + \\beta \\times \\textrm{age} = -2.7 + 0.06 \\times \\textrm{age}, \\nonumber\n\\end{align}\\]\nwhere \\(p = \\textrm{Prob}\\left(\\textrm{Male}\\right)\\) and \\(1 - p = \\textrm{Prob}\\left(\\textrm{Female}\\right)\\).\nHence, the log-odds of the instructor being male increase by 0.06 for every one unit increase in age.\n\n\n\n\n\n\n Task\n\n\n\nSuppose now that we are interested in the log-odds of the instructor being a female (i.e. treat male as our reference category), how can we calculate the log-odds of this alternative outcome?\n\n\nTake hint\n\nRecall that the logistic regression model we fitted previously predicts the log-odds of being a male (since female is treated as our reference category), i.e.\n\\[\\begin{align}\n\\mathrm{log}\\left(\\dfrac{P(male=1)}{P(male=0)}\\right) &= \\alpha + \\beta \\cdot \\textrm{age} = -2.7 + 0.06 \\cdot \\textrm{age}\n\\end{align}\\]\nNow we are interested in :\n\\[\\begin{align}\n\\mathrm{log}\\left(\\dfrac{P(male=0)}{P(male=1)}\\right) &= -\\mathrm{log}\\left(\\dfrac{P(male=1)}{P(male=0)}\\right) \\\\\n&= -[ \\alpha + \\beta \\cdot \\textrm{age}] = -[-2.7 + 0.06] \\cdot \\textrm{age}\n\\end{align}\\]\nNote: The reference category in your data can be changed by using the relevel() function. see ?relevel for further details.\n\n\n\n\nClick here to see the solution\n\nevals.gender %&gt;% \n  mutate(gender = relevel(gender,ref = \"male\")) %&gt;%\n  glm(gender ~ age, data = . ,family = binomial) %&gt;% \n  broom::tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)   2.70      0.512       5.27 0.000000136  \n2 age          -0.0630    0.0106     -5.95 0.00000000271\n\n\n\n\n\nThis provides us with a point estimate of how the log-odds changes with age, however, we are also interested in producing a 95% confidence interval for these log-odds. This can be done by setting the option conf.int = TRUE within the tidy() function:\n\nmodel %&gt;% tidy(conf.int = TRUE, conf.level = 0.95)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n-2.6979460\n0.5119379\n-5.270066\n1e-07\n-3.7196499\n-1.7097067\n\n\nage\n0.0629647\n0.0105852\n5.948386\n0e+00\n0.0425879\n0.0841436\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that we could have omitted the argument conf.level = 0.95 since this is the default confidence level used in the tidy() function. Thus, unless you need a confidence level other than the 95% CI, you can leave this argument with the default setting as will do in the following exercises.\n\n\nThe point estimate for the log-odds is 0.06, which has a corresponding 95% confidence interval of (0.04, 0.08). This can be displayed graphically using the plot_model function from the sjPlot package by simply passing our model as an argument:\n\nCodeplot_model(model, show.values = TRUE, transform = NULL,\n           title = \"Log-Odds (Male instructor)\", show.p = FALSE)\n\n\n\nThe log-odds of age for male instructors.\n\n\n\nSome of the interesting arguments that can be passed to the plot_model function are:\n\n\nshow.values = TRUE/FALSE: Whether the log-odds/odds values should be displayed;\n\nshow.p = TRUE/FALSE: Adds asterisks that indicate the significance level of estimates to the value labels;\n\ntransform: A character vector naming the function that will be applied to the estimates. The default transformation uses exp to display the odds ratios, while transform = NULL displays the log-odds; and\n\nvline.color: colour of the vertical “zero effect” line.\n\nFurther details on using plot_model can be found here.\n\n\n\n\n\n\n Task\n\n\n\nProduce a 95% Wald confidence interval (i.e., based on asymptotic normality) for the age coefficient by “hand”, i.e. using only the coefficient’s estimate and std. error. Then compare this result with the confidence interval obtained in R.\nNotice that the tidy function will compute a confidence interval using the profile-likelihood by default. Thus, if you want the results to be comparable, you will need to use the confint.default() function.\n\n\nI need a hint\n\nRecall that a 95% Wald CI for \\(\\beta\\) can be computed as \\(\\hat{\\beta} \\pm z_{1-\\alpha/2} \\times SE(\\hat{\\beta})\\), where \\(SE(\\cdot)\\) denotes the standard error of the estimate and \\(z_{1-\\alpha/2}\\) the quantile of the Normal density.\n\n\n\n\nSee the solution\n\nmod.coef.logodds &lt;- model %&gt;% tidy() %&gt;% filter(term==\"age\")\n\n# Compute Wald 95% CI by hand\nmod.coef.logodds$estimate + (c(-1,1)*\nqnorm(0.975)) * mod.coef.logodds$std.error \n\n[1] 0.04221815 0.08371129\n\n# Compare with Wald 95% CI in R\nconfint.default(model)[2,]\n\n     2.5 %     97.5 % \n0.04221815 0.08371129"
  },
  {
    "objectID": "index.html#odds",
    "href": "index.html#odds",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n2.3 Odds",
    "text": "2.3 Odds\nTypically we would like to work on the odds scale as it is easier to interpret an odds-ratio as opposed to the log-odds-ratio. To obtain the odds we simply exponentiate the log-odds, that is\n\\[\\begin{align}\n\\frac{p}{1-p} &= \\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right), \\nonumber\n\\end{align}\\]\n\nmodel %&gt;% tidy(exponentiate = T)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n0.07\n0.51\n-5.27\n0\n\n\nage\n1.06\n0.01\n5.95\n0\n\n\n\n\n\nOn the odds scale, the value of the intercept (0.07) gives the odds of a teaching instructor being male given their age = 0, which is obviously not a viable age for a teaching instructor, and hence why this value is very close to zero. For age we have an odds of 1.06, which indicates that for every 1 unit increase in age, the odds of the teaching instructor being male increase by a factor of 1.06.\nSo how is this calculated? Let’s look at the odds-ratio obtained from instructors aged 51 and 52 years old, that is, a one unit difference:\n\\[\\begin{align}\n\\frac{\\mbox{Odds}_{\\mbox{age=52}}}{\\mbox{Odds}_{\\mbox{age=51}}} &= \\left(\\frac{\\frac{p_{\\mbox{age=52}}}{1 - p_{\\mbox{age=52}}}}{\\frac{p_{\\mbox{age=51}}}{1 - p_{\\mbox{age=51}}}}\\right) \\\\\n&= \\frac{\\exp\\left(\\alpha + \\beta \\cdot 52\\right)}{\\exp\\left(\\alpha + \\beta \\cdot 51\\right)} = \\exp\\left(\\beta \\cdot (52 - 51)\\right) \\\\\n&= \\exp\\left(0.06\\right) = 1.06. \\nonumber\n\\end{align}\\]\nFor example, the odds of a teaching instructor who is 45 years old being male is given by\n\\[\\begin{align}\n\\frac{p}{1-p} &= \\exp\\left(\\alpha + \\beta \\cdot \\textrm{age}\\right) = \\exp\\left(-2.7 + 0.06 \\cdot 45\\right) = 1.15. \\nonumber\n\\end{align}\\]\nThis can be interpreted as the chances of an instructor who is 45 being male are 15% greater than them being female. We can obtain a 95% confidence interval for the odds by simply exponentiating the lower and upper bounds of our log-odds interval:\n\nmodel %&gt;% tidy(exponentiate = T,conf.int = T) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n0.07\n0.51\n-5.27\n0\n0.02\n0.18\n\n\nage\n1.06\n0.01\n5.95\n0\n1.04\n1.09\n\n\n\n\n\nHence the point estimate for the odds is 1.06, which has a corresponding 95% confidence interval of (1.04, 1.09). This can be displayed graphically using the plot_model function from the sjPlot package by simply passing our model as an argument as well as removing transform = NULL (the default transformation is exponential):\n\nCodeplot_model(model, show.values = TRUE,\n           title = \"Odds (Male instructor)\", show.p = FALSE, axis.lim = c(1, 1.5))\n\n\n\nThe odds of age for male instructors.\n\n\n\nNote: axis.lim is used to zoom in on the 95% confidence interval."
  },
  {
    "objectID": "index.html#probabilities",
    "href": "index.html#probabilities",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n2.4 Probabilities",
    "text": "2.4 Probabilities\nSince we have used the logit link function to link the linear predictor \\(\\alpha + \\beta \\cdot \\textrm{age}\\) to the probabilities of being a male, we can then obtain back the probability \\(p = \\textrm{Prob}(\\textrm{Male})\\) by using the inverse-logit transformation:\n\\[\\begin{align}\np &= \\frac{\\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right)}{1 + \\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right)} . \\nonumber\n\\end{align}\\]\nFor example, the probability of a teaching instructor who is 52 years old being male is\n\\[\\begin{align}\np &= \\frac{\\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right)}{1 + \\exp\\left(\\alpha + \\beta \\cdot \\textrm{age} \\right)}\n=\\frac{\\exp\\left(-2.7 + 0.06\\cdot 52 \\right)}{1 + \\exp\\left(-2.7 + 0.06\\cdot 52 \\right)}\n= 0.64, \\nonumber\n\\end{align}\\]\nwhich can be computed in R using the inverse logit plogis() function from the stats library:\n\nplogis(model$coefficients[1] \n              + model$coefficients[2] *  52)\n\n(Intercept) \n  0.6401971 \n\n\nFinally, we can plot the probability of being male using sjPlot by specifying the following:\n\nThe model we have fitted\ntype = \"pred\" to plot predicted values (marginal effects) for specific model terms.\nThe model terms of interest. Here, you can also plot the marginal effects at specific values ,e.g. selecting age[30:60] will plot the predictions based on age-values from 30 to 60, while age[all] will plot the predictions across all of the range of our independent variable (You can also provide a custom grid to evaluate you predictions, we will this cover next week).\n\n\nCodeplot_model(model, \n           type = \"pred\", \n           title = \"\", \n           terms=\"age [all]\", \n           axis.title = c(\"Age\", \"Prob. of instructor being male\"))\n\n\n\nProbability of teaching instructor being male by age.\n\n\n\nTable 1 summarises the relationship between between Odds and Probabilities:\n\n\nTable 1: Relationship between Odds and Probabilities\n\n\n\n\n\n\n\nScale\nEquivalence\n\n\n\nOdds\n\\[                                                                                                                                                                                                                                                                                                                                                                                                                                 \n                                                                                                                                                                                                                                                                                             Odds = \\mathrm{exp}(log Odds) = \\dfrac{P(event)}{1-P(event)}                                                                                          \n                                                                                                                                                                                                                                                                                             \\]\n\n\nProbability\n\\[                                                                                                                                                                                                                                                                                                                                                                                                                                 \n                                                                                                                                                                                                                                                                                             P(event) =\\dfrac{\\mathrm{exp}(logOdds)}{1+\\mathrm{exp}(logOdds)}  = \\dfrac{Odds}{1+Odds}                                                              \n                                                                                                                                                                                                                                                                                             \\]"
  },
  {
    "objectID": "index.html#diagnostic-plots",
    "href": "index.html#diagnostic-plots",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n3.1 Diagnostic plots",
    "text": "3.1 Diagnostic plots\nAs usual, now that we have fitted the model we need to assess how well the model fits the data and check whether our assumptions are met. Our assumptions can be checked as usual by using the plot() function in base R. However, we can visualize this with a much nicer and appropiate layout using the check_model() function within the performance library.\n\nCodelibrary(performance)\ncheck_model(model, panel = TRUE)\n\n\n\nGLM diagnostics for the gender of teaching instructors\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nBe aware that the performance library has quite a few dependencies so check whether any package needs to be updated when you install the package for the first time.\n\n\nBy specifying panel = TRUE the output shows us 4 different diagnostic plots. These are:\n\nPosterior predictive check (Top-left):Compares the observed data (in this case the number of females and males) against the predicted number of males and females using simulated data. We can look for systematic discrepancies between real and simulated data to assess the goodness of fit (see. (Gabry et al. 2019) and check_predictions() for further details)\nBinned residuals (Top-right): Bin the observations based on their fitted values, and average the residual value in each bin. The plots shows the average residual values versus the average fitted value for each bin” (Gelman and Hill 2006). If the model were true, one would expect about 95% of the residuals to fall inside the error bounds (see ?binned_residuals for more details.)\nInfluential observations (Bottom-left): This plot is used to identify influential observations. If any points in this plot fall outside of Cook’s distance (the dashed lines) then it is considered an influential observation. See check_outliers() for further details.\nUniformity of residuals (Bottom-right): Since our response is not normally distributed, residuals are not expected to be normally distributed either. Thus, we can use QQ plots to check the uniformity of residuals, i.e. the extent to which the observed values deviate from the model expectations using simulated residuals instead of the usual Pearson residuals"
  },
  {
    "objectID": "index.html#predictive-performance-metrics",
    "href": "index.html#predictive-performance-metrics",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n3.2 Predictive performance metrics",
    "text": "3.2 Predictive performance metrics\nSince our outcome of interest is a binary categorical variable, we can compute the predicted classes and evaluate how accurate our predictions are by comparing them against the observed values. To do so, we can add the predicted (fitted) values to our data with broom::augment() and classify them based on a decision threshold.\n\n\n\n\n\n\nNote\n\n\n\nNote that we can append either the logit-scaled fitted values (by setting type.predict = c(\"link\") or the predicted probabilities (type.predict = c(\"response\")).\n\n\nIn the case of logistic regression, you typically classify these probabilities into discrete classes based on a cutoff (commonly 0.5 for binary classification). Here is an example where predicted probabilities outcomes of \\(\\hat{p} &gt; 0.5\\) are classified as male and as female if \\(\\hat{p} \\leq 0.5\\) .\n\nCodepred_results = model %&gt;% \n  augment(type.predict = c(\"response\")) %&gt;%\n  mutate(predicted_class = \n           factor(ifelse(.fitted &gt; 0.5, \"male\", \"female\")))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngender\nage\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\npredicted_class\n\n\n\nfemale\n36\n0.3938360\n-1.0006045\n0.0058192\n1.132909\n0.0019126\n-1.003529\nfemale\n\n\nfemale\n36\n0.3938360\n-1.0006045\n0.0058192\n1.132909\n0.0019126\n-1.003529\nfemale\n\n\nfemale\n36\n0.3938360\n-1.0006045\n0.0058192\n1.132909\n0.0019126\n-1.003529\nfemale\n\n\nfemale\n36\n0.3938360\n-1.0006045\n0.0058192\n1.132909\n0.0019126\n-1.003529\nfemale\n\n\nmale\n59\n0.7343825\n0.7857803\n0.0047899\n1.133280\n0.0008746\n0.787669\nmale\n\n\n\n\n\nWe can use these predicted classes to compute different predictive performance/evaluation metrics. To do so we can compute a confusion matrix according to the true and predicted classes:\n\n\n\n\nThe table can be interpreted as follows:\n\nThe correct classification rate (CCR) or accuracy describes the overall proportion of teaching instructors (males or females) that were classified correctly among all the \\(S = 463\\) individuals.\nThe true positive rate (TPR) or sensitivity (a.k.a. recall), denotes the proportion of actual male instructors that are correctly classified as males by the model.\nThe true negative rate (TNR) or specificity, denotes the proportion of actual females that have been classified correctly as females by the model. Note: the false positive rate (FPR) computed as (1- specificity) is another popular metric that measures the proportion of actual negatives that are incorrectly predicted as positive.\nThe model’s precision or positive predictive value (PPV) represents the proportion of predicted male instructors that were actually male, i.e. how many of the predicted positive cases were actually positive.\nThe model’s negative predictive value (NPV) represents the proportion of predicted female instructors that were actually females, i.e. how many of the predicted negative cases were actually negative.\n\nTo compute the confusion matrix we can use the conf_mat() function and use as input the data frame with the predicted and observed classes as follows:\n\nconf_mat(pred_results,truth = gender,estimate = predicted_class)\n\n          Truth\nPrediction female male\n    female     76   60\n    male      119  208\n\n\nRather than computing these predictive performance metrics by hand, we can take advantage of the metric_set() function from the yardstick library (loaded as part of the tidymodels ecosystem) to combine multiple metric functions together into a new function that calculates all of them at once. Then we just simply need to supply the same arguments we used for computing the confusion matrix.\n\n# Step (1) create a set with classification metrics we want to compute\neval_metric &lt;- metric_set(accuracy,sensitivity,specificity,ppv,npv)\n# Step (2) call out the metric set and input the data containing the observed and predicted classes\neval_metric(pred_results,\n            truth = gender,\n            estimate = predicted_class,  \n            event_level = \"second\")\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\naccuracy\nbinary\n0.61\n\n\nsensitivity\nbinary\n0.39\n\n\nspecificity\nbinary\n0.78\n\n\nppv\nbinary\n0.56\n\n\nnpv\nbinary\n0.64\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNotice that we passed on the argument event_level = \"second\" since the default behavior of the eval_metric() function created via metric_set() is to consider the first class of the variable genderas the event. If you recall, female appears as first due alphabetical order, and hence is treated as our reference category in our model and thus we are modelling the event \\(\\mathbb{Pr}(male)\\)\n\nlevels(pred_results$gender)\n\n[1] \"female\" \"male\"  \n\n\nHowever since female appear first, we need to tell our function that we are interested in the second class of the variables gender , i.e. male, as the event and not the other way around.\n\n\nWe can see that our model is not doing a great job in predicting the outcome (accuracy of \\(\\approx\\) 60%).This is due to a particularly poor performance in terms of specificity, i.e. roughly only 40% of truly male instructors are correctly classified as such by our mode. Despite this, our model is not doing a terrible job in terms of sensitivity since almost 80% of the females are actually being predicted as such.\nNote that the threshold of 0.5 is common but may not always be optimal. You can adjust it based on your specific application and the desired balance between sensitivity and specificity (we will see an example of such in the next task).\n\n\n\n\n\n\nNote\n\n\n\nFor this course we have been using only our data to assess the predicative performance of our models. In reality, if we were to properly assess the classification performance of our models, we should look at out-of-sample prediction by first splitting the data into a training and a test set, then fitting the model to the training data and finally predicting the class of the each observation in the test data that has been held back. Otherwise we run the risk of overstating the classification accuracy of the model."
  },
  {
    "objectID": "index.html#roc-curve",
    "href": "index.html#roc-curve",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n3.3 ROC Curve",
    "text": "3.3 ROC Curve\nAnother way to assess how good the model is at separating the 2 classes of the outcome is through the Receiver Operating Characteristic (ROC) curve. The ROC curve is a graphical representation used to evaluate the performance of a binary classification model. It shows the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) at various threshold settings.\nThe threshold determines the cutoff probability for classifying a sample as positive or negative (male or female in our case).\n\nAt a cutoff \\(=0\\) , all the observations will be classified as positive (i.e. all the instructors are going to be classified as males since we are modelling the event \\(P(male)\\)).\nAt a cutoff \\(=1\\) , all the observations will be classified as negative (i.e. all the instructors are going to be classified as females corresponding to \\(1 - P(male)\\)).\n\nAs the threshold moves between 0 and 1, the ROC curve traces out points that show the model’s performance for various balances between TPR and FPR.\nWe can combine the roc_curve() function from the performance library and autoplot() function from ggplot to calculate and visualize the ROC curve. We just need to supply the true classes and the predicted probabilities (NOT the predicted classes!).\n\nroc_curve(pred_results,\n          truth = gender,\n          .fitted,\n          event_level = \"second\") %&gt;%\n  autoplot()\n\n\n\nROC curve for GLM fitted to the evaluation scores data set.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nSimilarly to the the evaluation metric function we had before, we set event_level = \"second\" since the default behavior of the roc_auc() function is also to consider the first class of the variable genderas the event. Thus, we need to tell the roc_auc() that we are interested in the male level as our event, which is the second class of the variable gender.\n\n\nThe closer the ROC curve is to the top-left corner, the better the model is at distinguishing between the positive and negative classes. This means high true positive rates (sensitivity) and low false positive rates. The closer the curve is to the diagonal line (i.e. when TPR = FPR) then the performance is no better than random guessing. We can see here that our model is doing a little bit better than just random guessing.\nWe can also calculate the area under the ROC curve (AUC) as a single value to summarize the model’s performance:\n\nroc_auc(pred_results,\n        truth = gender,\n        .fitted,\n        event_level = \"second\")\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\nroc_auc\nbinary\n0.66\n\n\n\n\nThe AUC ranges from 0 to 1:\n\nAUC = 1: Perfect classifier.\nAUC = 0.5: No better than random guessing.\nAUC \\(\\leq\\) 0.5: Worse than random guessing, meaning the model is consistently misclassifying.\n\nAgain, the AUC of 0.66 indicates a moderate-poor fit to the data (mainly due to miss-classifications of male instructors as we saw previously).\n\n\n\n\n\n\n Task\n\n\n\nThe Youden index (Youden’s J statistics) is a summary metric that helps to identify the optimal threshold that maximizes both sensitivity (true positive rate) and specificity (true negative rate) simultaneously. It is computed as follows:\n\\[ J = \\mathrm{sensitivity} + \\mathrm{specificity} -1 \\]\nCompute the Youden index and find the optimal threshold that maximizes both sensitivity and specificity in the logistic regression model fitted to the evaluation scores data set.\n\n\nTake hint\n\nYou can use the roc_curve() function to compute the sensitivity and specificity values at different thresholds. Then use these values to compute Youden’s index and find the threshold value associated with the maximum value of Youden’s index.\n\n\n\n\nClick here to see the solution\n\nroc_curve(pred_results,\n          truth = gender,\n          .fitted,\n          event_level = \"second\") %&gt;% \n  mutate(youden_j = sensitivity + specificity - 1) %&gt;% \n  filter(youden_j == max(youden_j)) %&gt;% \n  select(.threshold)\n\n# A tibble: 1 × 1\n  .threshold\n       &lt;dbl&gt;\n1      0.709"
  },
  {
    "objectID": "index.html#log-odds-1",
    "href": "index.html#log-odds-1",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n4.1 Log-odds",
    "text": "4.1 Log-odds\nThe logistic regression model is given by:\n\\[\\begin{align}\ny_i &\\sim \\mathrm{Bernoulli}(p_i)\\\\\n\\mathrm{logit}(p_i) &= \\alpha +  \\beta_{\\mbox{ethnicity}}  \\times \\mathbb{I}_{\\mbox{ethnicity}}(\\mathrm{not~  minority}).\n\\end{align}\\]\nWhich can be fitted in R as follows:\n\nmodel.ethnic &lt;- glm(gender ~ ethnicity,\n                    data = evals.ethnic,\n                    family = binomial) \n\nHere, \\(y_i\\) denotes the \\(i\\)th instructor’s gender, again, the baseline category for our binary response is female. Thus, \\(p_i = \\mathrm{Prob}(\\mathrm{Male})\\) is linked to the linear predictor through the logit link function. This means that estimates we get from fitting the logistic regression model are for a change on the log-odds scale for males (\\(p_i = \\textrm{Prob}(\\textrm{Males})\\)) in comparison to the response baseline females.\nAlso, the baseline category for our explanatory variable is minority, which, like gender, is done alphabetically by default by R:\n\nlevels(evals.ethnic$ethnicity)\n\n[1] \"minority\"     \"not minority\"\n\n\nThus, \\(\\alpha\\) correspond to the log-odds of the instructors being a males given that they are on the minority baseline category.\nThen, \\(\\mathbb{I}_{\\mbox{ethnicity}}(\\mbox{not minority})\\) is an indicator function for those instructors in the not minority group and \\(\\beta_{\\mbox{ethnicity}}\\) represent the change in the log-odds of a male instructor that is not on the minority group.\nLets break this down. The model we have fitted is:\n\\[\n\\mathrm{log}\\left(\\dfrac{p_i}{1-p_i}\\right) = \\alpha + \\beta_{\\mbox{ethnicity}}  \\times \\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~  minority})\n\\]\n\n\\(\\alpha\\) is the intercept, representing the log-odds when \\(\\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~ minority}) = 0\\) (i.e., when the instructor is in the minority group). When the instructor belongs to the reference category minority the models simplifies to: \\[\\mathrm{log}\\left(\\frac{p_i}{1-p_i}\\right) = \\alpha \\]\n\\(\\beta_{\\mathrm{ethnicity}}\\) is the coefficient for the predictor \\(\\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~ minority})\\), which shows how the log-odds change when moving from the reference category (minority) to the other level (not minority). When the instructor does not belong to reference category, i.e. \\(\\mathbb{I}_{\\mathrm{ethnicity}}(\\mathrm{not~ minority}) = 1\\), the model becomes: \\[\\mathrm{log}\\left(\\dfrac{p_i}{1-p_i}\\right) = \\alpha + \\beta_{\\mbox{ethnicity}}\\]\n\nSo, the log-odds of the instructors being male in the not minority group are \\(\\alpha +\\beta_{\\mbox{ethnicity}}\\). Lets compute the model estimates and 95 % confidence intervals for the log-odds:\n\nmodel.ethnic %&gt;%\n  tidy(conf.int = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n-0.25\n0.25\n-1.00\n0.32\n-0.75\n0.24\n\n\nethnicitynot minority\n0.66\n0.27\n2.44\n0.01\n0.13\n1.20\n\n\n\n\n\nThe fitted model is:\n\\[\\begin{align}\n\\ln\\left(\\frac{\\hat{p_i}}{1-\\hat{p_i}}\\right) &= -0.25 + 0.66 \\cdot \\mathbb{I}_{\\mbox{ethnicity}}(\\mbox{not minority}), \\nonumber\n\\end{align}\\]\nHence, the log-odds of an instructor being male increase by 0.66 if they are in the ethnicity group not minority, which has a corresponding 95% confidence interval of (0.13, 1.2).\nThis can be displayed graphically using the plot_model function from the sjPlot package by simply passing our model as an argument:\n\nCodeplot_model(model.ethnic, show.values = TRUE, transform = NULL, \n           title = \"Log-Odds (Male instructor)\", show.p = FALSE)\n\n\n\nThe log-odds for male instructors by ethnicity (not a minority)."
  },
  {
    "objectID": "index.html#odds-1",
    "href": "index.html#odds-1",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n4.2 Odds",
    "text": "4.2 Odds\nOn the odds scale the regression coefficients are given by\n\nmodel.ethnic %&gt;%\n  tidy(conf.int = T,\n       exponentiate = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n0.78\n0.25\n-1.00\n0.32\n0.47\n1.27\n\n\nethnicitynot minority\n1.94\n0.27\n2.44\n0.01\n1.14\n3.33\n\n\n\n\n\nThe (Intercept) gives us the odds of the instructor being male given that they are in the minority ethnic group, that is, 0.78 (the indicator function is zero in that case).\nThe odds of the instructor being male given they are in the not minority ethnic group are 1.94 times greater than the odds if they were in the minority ethnic group.\nBefore moving on, let’s break down how these values are computed. First, the odds of the instructor being male given that they are in the minority ethnic group (reference category) can be obtained as follows:\n\\[\\begin{align}\n\\mathrm{Odds}(\\mathrm{male = 1} | \\mathrm{minority} =1) &= \\dfrac{p_{(minority=1)}}{1 - p_{(minority=1)}}\\\\\n&= \\exp\\left(\\alpha\\right) \\\\\n&= \\exp\\left(-0.25\\right) \\\\\n&= 0.78. \\nonumber\n\\end{align}\\]\nHere, \\(p_{(minority=1)} = \\mathrm{Prob}\\left(\\mathrm{Male}=1 | \\mathrm{minority}=1\\right)\\). Thus, the odds of the instructor in the minority group being a male are 0.78 the odds of an instructor being a female in the same group.\nSlightly confusing right? Maybe it makes more sense to interpret this result in terms of the female group, i.e. \\(\\mathrm{Odds}(\\mathrm{female} = 1 | \\mathrm{minority} =1)\\). However you must be aware that this is not a probability! so you can not simply compute \\(1- \\mathrm{exp}(\\alpha)\\). To compute this correctly, we should do as follows:\n\\[\\begin{align}\n\\mathrm{Odds}(\\mathrm{female} = 1 | \\mathrm{minority} =1) &= \\dfrac{P(\\mathrm{female}=1 |\\mathrm{minority}=1)}{P(\\mathrm{male}= 1 |\\mathrm{minority}=1)}\\\\\n&= \\left[\\dfrac{P(\\mathrm{male}=1 |\\mathrm{minority}=1)}{P(\\mathrm{female}= 1 |\\mathrm{minority}=1)}\\right]^{-1}\\\\\n&= \\left[\\mathrm{Odds}(\\mathrm{male}=1|\\mathrm{minority}=1)\\right]^{-1}\\\\\n&= \\mathrm{exp}(\\alpha)^{-1} \\\\ &= \\exp\\left(-0.25\\right)^{-1} = 1.28\n\\end{align}\\]\nThus, the odds of the instructor in the minority group being a female are 28% higher than the odds of an instructor being a male in the same group. However, by looking at the confidence intervals and p-value we can see that this difference is not statistically significant.\n\n\n\n\n\n\n Task\n\n\n\nUse the relevel function to fit a logistic regression that estimates the odds of the instructor being female given that they are in the minority ethnic group directly.\n\n\nTake hint\n\nThe reference category in your data can be changed by using the relevel() function. see ?relevel for further details.\n\n\n\n\nClick here to see the solution\n\nevals.ethnic %&gt;%\n  mutate(gender = relevel(gender,ref = \"male\")) %&gt;% \n  glm(gender ~ ethnicity, data = . ,family = binomial) %&gt;%  \n  broom::tidy(exponentiate = T) %&gt;% \n  filter(term==\"(Intercept)\") %&gt;%\n  pull(estimate) \n\n[1] 1.285714\n\n\n\n\n\nNow, let’s look a the odds-ratio of an instructor being male in the not minority group compared to the minority ethnic group.\n\n\nFirst, the odds of an instructor being male given that they are in the minority ethnic group were:\n\\[\\begin{align}\n\\mathrm{Odds}(\\mathrm{male} = 1 | \\mathrm{minority} = 1) =&\n\\dfrac{p_{(minority=1)}}{1 - p_{(minority=1)}} \\\\\n&= \\exp\\left(\\alpha\\right) \\\\ &= 0.78. \\nonumber\n\\end{align}\\]\n\n\nRecall that the log-odds of an instructor not belonging to the minority group (i.e. not minority) being a male are \\(\\alpha +\\beta_{\\mbox{ethnicity}}\\). On the odds scale this is:\n\\[\\begin{align}\n\\mathrm{Odds}(\\mathrm{male} = 1 | \\mathrm{minority} = 0) =& \\dfrac{p_{(\\mathrm{minority} = 0)}}{1- p_{(\\mathrm{minority} = 0)}}\\\\\n&= \\mathrm{exp}( \\alpha + \\beta_{\\mbox{ethnicity}}) \\\\\n&= 1.51\n\\end{align}\\]\n\n\nNow, the odds of an instructor that is not in the minority group being male against the odds of an instructor that comes from the minority group is given by the ratio of the odds we just calculated:\n\\[\\begin{align}\n\\frac{\\mathrm{Odds}(\\mathrm{male} = 1| \\mathrm{minority} = 0)}{\\mathrm{Odds}(\\mathrm{male} = 1| \\mathrm{minority} = 1)} &= \\dfrac{\\frac{p_{(\\mathrm{minority} = 0)}}{1- p_{(\\mathrm{minority} = 0)}}}{\\frac{p_{(\\mathrm{minority}=1)}}{1- p_{(\\mathrm{minority}=1)}}} \\\\\n&= \\frac{\\mathrm{exp}( \\alpha + \\beta_{\\mbox{ethnicity}})}{\\exp\\left(\\alpha\\right)}\\\\\n&= \\exp\\left(\\alpha + \\beta_{\\mbox{ethnicity}} - \\alpha\\right) \\\\\n&= \\exp\\left(\\beta_{\\mbox{ethnicity}}\\right) = \\exp\\left(0.66 \\right) \\\\\n&= 1.93. \\nonumber\n\\end{align}\\]\n\n\nWhich is the coefficient estimate we got from the tidy() summaries. This means that instructors that are not in the minority groups are significantly 1.93 times more likely to be males compared to instructors in the minority group.\nThe corresponding 95% confidence interval of (1.14, 3.33). Again, we can display this graphically using the plot_model function from the sjPlot package:\n\nCodeplot_model(model.ethnic, show.values = TRUE,\n           title = \"Odds (Male instructor)\", show.p = FALSE)\n\n\n\nThe odds-ratio of a male instructor given they are in the not minority group."
  },
  {
    "objectID": "index.html#probabilities-1",
    "href": "index.html#probabilities-1",
    "title": "Week 7: Generalised Linear Models",
    "section": "\n4.3 Probabilities",
    "text": "4.3 Probabilities\nYou can also use the model to predict the probability of having an instructor being male for each ethnicity group.\nThe probabilities of an instructor being male given they are in the minority and not minority groups are:\n\nplogis(coef(model.ethnic)[1])\n\n(Intercept) \n     0.4375 \n\nplogis(coef(model.ethnic)[1]+coef(model.ethnic)[2])\n\n(Intercept) \n  0.6015038 \n\n\nHence, the probabilities of an instructor being male given they are in the minority and not minority ethnic groups are 0.437 and 0.602, respectively.\nFinally, we can use the plot_model() function from the sjPlot package to produce the estimated probabilities by ethnicity as follows:\n\nCodeplot_model(model.ethnic,\n           type = \"pred\",\n           terms = \"ethnicity\",\n           axis.title = c(\"Ethnicity\", \n                          \"Prob. of instructor being male\"),\n           title = \" \")\n\n\n\nProbability of teaching instructor being male by ethnicity."
  }
]