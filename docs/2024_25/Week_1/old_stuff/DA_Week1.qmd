---
title: "Week 1: Visualising and data tidying using R"
format:
  html:    
    code-link: true
    code-fold: true
    code-tools:
      source: false
      toggle: true
    toc: true
    toc-location: left
    toc-title: Contents
    number-sections: true
  pdf:
    latex-auto-install: true
editor: visual
editor_options: 
  chunk_output_type: console
---

# Getting started

This week we will demonstrate various techniques for data **tidying**, **wrangling** and **visualization** in R. This will also include the correct interpretation and understanding of the different plotting techniques.

::: callout-note
A lot of the content within this course is based on the open-source book [Statistical Inference via Data Science](https://moderndive.com/index.html) and thus is a useful source for additional examples and questions.
:::

First, start by opening **RStudio** by going to `Desktop -> Maths-Stats -> RStudio`. Once RStudio has opened create a new R script by going to `File -> New File -> R Script`. Next go to `File -> Save As...` and save the script into your personal drive, either `M:` or `K:` (do not save it to the `H:` drive). We shall now load into R all of the libraries we will need for this session. This can be done by typing the following into your R script:

```{r}
#| message: false
#| warning: false
library(ggplot2)
library(tidyverse)
library(nycflights13)
library(fivethirtyeight)
```

The libraries can be loaded into R by highlighting them in your script and then clicking on the `Run` button located in the top right of the script window. The first library `ggplot2` allows us to use functions within that package in order to create nice data visualisations. The `tidyverse` library is actually a collection of different R packages for manipulating data. The final two libraries (`nycflights13` and `fivethirtyeight`) contain interesting data sets that we shall examine in this session.

Notice that when loading the `tidyverse` package you get a message that tells you about conflicting functions of certain packages. This means that there is at least one or more functions with the same name loaded from different packages (and thus one the function will mask the other). You can use the function `tidyverse_conflicts()` for getting a list of the conflicted packages:

```{r}
#| warning: false
#| code-fold: true
tidyverse_conflicts()
```

In here, we can see for example that the `filter` function from the `dplyr` package has a conflict with the `filter` function in base R `stats` library. A way of sorting that out is to load the `dplyr` library after base R so that R will only consider the version of the function that was last loaded. We can be more rigorous about this and load the `conflicted` library. This will prohibit us to us any functions that have some conflict with previously defined functions.

```{r}
#| eval: false
library(conflicted)
```

By doing this, we would need to be more specific about the source package from which the desired function should be loaded. There are two ways of doing this:

1.  Using `::` after calling the package name every time we use the function from that package. E.g., `dplyr::filter(â€¦)` will tell R to explicitly use the function `filter` from the `dplyr` library.

2.  Using the `conflicts_prefer("function","package")` function to explicitly declare which version of the function you want to use in the remaining R session (i.e. after `conflicts_prefer()` is called, e.g., `conflict_prefer("filter","dplyr")` .

::: {.callout-tip icon="false"}
## {{< bi question-octagon color=#6dc83c >}} Question

What do you think is the advantage of using the `conflicts_prefer` as opposed to the first approach?
:::

# Viewing the data

Before visualising any data set, we first need to know its contents. For example, the contents of the `flights` data within the `nycflights13` library can be observed using the following command:

```{r}
glimpse(flights)
```

This function provides a concise overview of a data frame's structure. For each column/variable, it displays the name, data type, and a brief preview of the actual values along with dimensions of the data set (i.e, the number of columns `r ncol(flights)` and rows `r nrow(flights)`).

Another useful function that can be used to quickly explore your data is the `slice()`. It allows you to extract specific rows from a data frame based on their positions. For example, `slice(flights, 1:5)` retrieves the first 5 rows of the `flights` data frame. Additionally, the `.by` argument in `slice()` enables grouped slicing. For example, `slice(flights, 1:3, .by = carrier)` retrieves the first three rows within each group defined by the carrier variable. This function is useful for obtaining subsets of data for inspection or further analysis while preserving the structure within subgroups.

::: {.callout-warning icon="false"}
## Task

Use the `slice` function to print the first row of the `flights` data frame grouped by origin.

`r hide("Take hint")`

See the documentation for `slice()` (`?slice`).

`r unhide()`

```{r, echo = TRUE, eval = TRUE}
#| webex.hide: "Click here to see the solution"
#| code-fold: show

slice(flights, 1, .by = origin)
```
:::

# Tidy data

```{r}
#| echo: false
#| warning: false
#| message: false
library(kableExtra)
```

What does it mean for your data to be **tidy**? Beyond just being organised, having **tidy** data means that your data follows a standardised format. This makes it easier for you and others to visualise your data, to wrangle/transform your data, and to model your data. We will follow Hadley Wickham's definition of **tidy data** here:

> A data set is a collection of values, usually either numbers (if quantitative) or strings AKA text data (if qualitative). Values are organised in two ways. Every value belongs to a variable and an observation. A variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units. An observation contains all values measured on the same unit (like a person, or a day, or a city) across attributes.

> Tidy data is a standard way of mapping the meaning of a data set to its structure. A data set is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data:

> 1.  Each variable forms a column.
> 2.  Each observation forms a row.
> 3.  Each type of observational unit forms a table.

```{r tidyfig, echo=FALSE, fig.cap="Tidy data graphic from http://r4ds.had.co.nz/tidy-data.html"}
knitr::include_graphics("tidy-1.png",dpi = 300)
```

For example, say the following table consists of stock prices:

```{r echo=FALSE, warning = FALSE}
 stocks <- data_frame(
   Date = as.Date('2009-01-01') + 0:4,
   `Boeing Stock Price` = paste("$", c("173.55", "172.61", "173.86", "170.77", "174.29"), sep = ""),
   `Amazon Stock Price` = paste("$", c("174.90", "171.42", "171.58", "173.89", "170.16"), sep = ""),
   `Google Stock Price` = paste("$", c("174.34", "170.04", "173.65", "174.87", "172.19") ,sep = "")
 ) %>%
   slice(1:2)
  stocks %>%
    kable(
      digits = 2,
      caption = "Stock Prices (Non-Tidy Format)",
      booktabs = TRUE
    ) %>%
    kable_styling(font_size = 9, latex_options = "hold_position")
```

Although the data are neatly organised in a spreadsheet-type format, they are not in tidy format since there are three variables corresponding to three unique pieces of information (Date, Stock Name, and Stock Price), but there are not three columns. In tidy data format each variable should be its own column, as shown below. Notice that both tables present the same information, but in different formats.

```{r echo=FALSE, warning = FALSE}
 stocks_tidy <- stocks %>% 
   rename(
     Boeing = `Boeing Stock Price`,
     Amazon = `Amazon Stock Price`,
     Google = `Google Stock Price`
   ) %>% 
   gather(`Stock Name`, `Stock Price`, -Date)
 stocks_tidy %>%
   kable(
     digits = 2,
     caption = "Stock Prices (Tidy Format)",
     booktabs = TRUE
   ) %>%
    kable_styling(font_size = 9, latex_options = "hold_position")
```

However, consider the following table:

```{r echo=FALSE, warning = FALSE}
 stocks <- data_frame(
   Date = as.Date('2009-01-01') + 0:4,
   `Boeing Price` = paste("$", c("173.55", "172.61", "173.86", "170.77", "174.29"), sep = ""),
   `Weather` = c("Sunny", "Overcast", "Rain", "Rain", "Sunny")
 ) %>%
   slice(1:2)
 stocks %>%
   kable(
     digits = 2,
     caption = "Date, Boeing Price, Weather Data",
     booktabs = TRUE
   ) %>%
    kable_styling(font_size = 9, latex_options = "hold_position")
```

In this case, even though the variable **Boeing Price** occurs again, the data *is* tidy since there are three variables corresponding to three unique pieces of information (Date, Boeing stock price, and the weather on that particular day).

The non-tidy data format in the original table is also known as [wide](https://en.wikipedia.org/wiki/Wide_and_narrow_data) format whereas the tidy data format in the second table is also known as [long/narrow](https://en.wikipedia.org/wiki/Wide_and_narrow_data#Narrow) data format. In this course, we will work mostly with data sets that are already in the tidy format.

::: {.callout-tip icon="false"}
## {{< bi question-octagon color=#6dc83c >}} Question

Consider the following data frame of average number of servings of beer, spirits, and wine consumption in three countries as reported in the FiveThirtyEight article [Dear Mona Followup: Where Do People Drink The Most Beer, Wine And Spirits?](https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/)

```{r echo=FALSE}
drinks_sub <- drinks %>%
  select(-total_litres_of_pure_alcohol) %>% 
  filter(country %in% c("USA", "Canada", "South Korea"))
drinks_sub_tidy <- drinks_sub %>%
  gather(type, servings, -c(country)) %>%
  mutate(
    type = str_sub(type, start=1, end=-10)
  ) %>%
  arrange(country, type) %>% 
  rename(`alcohol type` = type)
drinks_sub |> kable()
```

This data frame is not in tidy format. What would it look like if it were?

`r hide("I need a hint")`

Think of these data as being in a wide format. What variables in this data set could be placed in different columns?

`r unhide()`

<!-- note: you could also just set webex.hide to TRUE -->

```{r, echo = FALSE, eval = TRUE}
#| webex.hide: "See the solution"
#| code-fold: true

drinks_sub |> gather("beverages type","number of servings",-country) |> kable()
```
:::

# Converting to **tidy** data format {#tidying}

In this section, we will see how to convert a data set that is not in the **tidy** format i.e. [wide](https://en.wikipedia.org/wiki/Wide_and_narrow_data) format, to a data set that is in the **tidy** format i.e. [long/narrow](https://en.wikipedia.org/wiki/Wide_and_narrow_data#Narrow) format.

First, let's download a **Comma Separated Values** (CSV) file of ratings of the level of democracy in different countries spanning 1952 to 1992: <https://moderndive.com/data/dem_score.csv>. We use the `read_csv()` function from the `readr` package to read it off the web:

```{r readcsv, message=FALSE, eval=TRUE, echo = 1}
dem_score <- read_csv("https://moderndive.com/data/dem_score.csv")
dem_score
```

In this `dem_score` data frame, the minimum value of -10 corresponds to a highly autocratic nation whereas a value of 10 corresponds to a highly democratic nation. Let's use the `dem_score` data frame but focus on only data corresponding to the country of Guatemala.

```{r echo = 1, eval = TRUE}
guat_dem <-  dplyr::filter(dem_score,country == "Guatemala")  
guat_dem
```

::: callout-note
Here we have used the `filter` function from `dplyr` package to subset the data set. We will revisit this code for subsetting data later in the session.
:::

In order for this data set to be on a **tidy** format, we need to take the values of the current column names in `guat_dem` (aside from `country`) and convert them into a new variable that will act as a key called `year`. Then, we'd like to take the numbers on the inside of the table and turn them into a column that will act as values called `democracy_score`. Our resulting data frame will have three columns: `country`, `year`, and `democracy_score`.

The `pivot_longer` function in the `tidyr` package can complete this task for us. The first argument to `pivot_longer`, is the `data` argument where we specify which data frame we would like to tidy. The next argument to `pivot_longer` is `cols` which specifies which columns we want to pivot into the longer format. Note that there are helper functions which help us declaring which variable (or variables) we want to pivotÂ `!`,`start_with`,Â `last_col`,Â `everything`,Â `contains`, etc. E.g. `!country` will the tell the function that all the variables except for `country` should be included in the pivoting process.

The next two arguments `names_to` and `values_to`, specify what we would like to call the new columns that convert our wide data into tidy/long format.

```{r}
#| echo: true
#| code-fold: show
guat_dem_long = pivot_longer(guat_dem,cols = !country, names_to = "year", values_to = "democracy_score")
slice(guat_dem_long,1:5)
```

The inverse transformation of `pivot_longer()` is of course, `pivot_wider()` and allows us to pivot from a long to a wide format. As arguments we need to provide which column (or columns) to get the name of the output column (`names_from`), and which column (or columns) to get the cell values from (`values_from`). For instance, if want the data from the previous example to go back to a wide-format, we can use the following code:

```{r}
#| code-fold: false
#| 
guat_dem_wide = pivot_wider(guat_dem_long, names_from = year, values_from = democracy_score)
guat_dem_wide
```

::: {.callout-warning icon="false"}
## Task

The information about drink consumption across countries is available on the `drinks` data set in the `fivethirtyeight` library:

```{r}
#| code-fold: false
slice(drinks,1:3)
```

Convert data frame to tidy data (long) format by pivoting the variables related to the servings of beer, spirits and wine. Name the new type of beverage column as `beverages type` and the servings as `number of servings`.

`r hide("Take hint")`

Your new data frame should contain 4 columns: `country`, `total_litres_of_pure_alcohol`, `beverages type` and `number of servings`. You can use the `ends_with()` function to match variables according to a given pattern.

`r unhide()`

```{r, echo = TRUE, eval = FALSE}
#| webex.hide: "Click here to see the solution"
#| code-fold: show

# We can explicitly declare which variable we don't want to pivot

drinks %>%
  pivot_longer(cols = !c(country,total_litres_of_pure_alcohol), 
               names_to = "beverages type",
               values_to = "number of servings")

# Or much concise 

drinks %>%
  pivot_longer(cols = ends_with("servings"),
               names_to = "beverages type",
               values_to = "number of servings")

```
:::

# Reminder of ggplot

Now that we have our data on a tidy format we can use `ggplot2` to produce a plot showing how the democracy scores have changed over the 40 years from 1952 to 1992 for Guatemala. Lets have a reminder of how we can do this using the ggplot.

As the first step you would pass the data to the `ggplot()` function and then add layers that can combines data, aesthetic mapping, a geom (geometric object), a stat (statistical transformation), and a position adjustment.

Let's start by laying out how we would map our aesthetics to variables in the data frame:

-   The `data` frame is `guat_dem_long` so we use `data = guat_dem_long`.
-   The mapping of the coordinates for the axes using `aes(x = year, y = democracy_score)`, where `aes()` relates to the plots aesthetics. That is,
    -   `year` maps to the `x` coordinate; and
    -   `democracy_score` maps to the `y` coordinate.

In order to create a line graph we now need to add an additional layer using the `+` command. We can include a points layer first:

```{r}
#| fig-width: 5
#| fig-height: 5
#| fig-align: center

ggplot(data = guat_dem_long, mapping =aes(x = year, y = democracy_score)) + 
  geom_point()
```

When adding layers using `ggplot` it should be noted that:

-   the `+` command should come at the end of lines, otherwise R will produce an error.
-   when adding additional layers it is a good idea to take a new line after each `+` command. This is so your code will be nice and clear with each layer given its own line of code. This is handy for code debugging.

Now we add a line connecting each point:

```{r}
#| fig-width: 5
#| fig-height: 5
#| fig-align: center
#| error: true
#| 
ggplot(data = guat_dem_long, mapping =aes(x = year, y = democracy_score)) + 
  geom_point()+
  geom_line()
```

**What happened?** Note that the `year` variable in `guat_dem_long` is stored as a character vector since we had to circumvent the naming rules in R by adding backticks around the different year columns in `guat_dem_long`. This is leading to `ggplot` not knowing exactly how to plot a line using a categorical variable. We can fix this by using the `parse_number` function in the `readr` package:

```{r }
#| fig-width: 5
#| fig-height: 5
#| fig-align: center
#| 
ggplot(data = guat_dem_long, mapping = aes(x = parse_number(year), y = democracy_score)) +
   geom_point()+
   geom_line()

```

We'll see later how we could use the `mutate` function to change `year` to be a numeric variable during the tidying process (alternatively we could have added the argument `names_transform = list(year = as.integer)` in the `pivot_longer()` function to declare the `year` column values as an integers; see `?pivot_longer` for more details).

As a final step we can change the axes labels and include a title on our plot by adding another layer as follows:

```{r}
#| fig-width: 5
#| fig-height: 5
#| fig-align: center
#|
ggplot(data = guat_dem_long, mapping = aes(x = parse_number(year), y = democracy_score)) +
   geom_point()+
   geom_line()+
  labs(x = "year", y = "Democracy score",
       title = "Guatemala's democracy score ratings from 1952 to 1992") 
```

# Data wrangling {#wrangling}

We are now able to import data and perform basic operations on the data to get it into the **tidy** format. In this and subsequent sections we will use tools from the `dplyr` package to perform data **wrangling** which includes transforming, mapping and summarising variables.

## The pipe %\>% {#piping}

Before we dig into data wrangling, let's first introduce the pipe operator (`%>%`). Just as the `+` sign was used to add layers to a plot created using `ggplot`, the pipe operator allows us to chain together data wrangling functions. The pipe operator can be read as **then**.

The piping syntax will be our major focus throughout the rest of this course and you'll find that you'll quickly be addicted to the chaining with some practice.

## Data wrangling verbs {#verbs}

The `d` in `dplyr` stands for data frames, so the functions in `dplyr` are built for working with objects of the data frame type. For now, we focus on the most commonly used functions that help wrangle and summarise data. A description of these verbs follows, with each subsequent section devoted to an example of that verb, or a combination of a few verbs, in action.

1.  `select`: Select variables in a data frame

2.  `filter`: Pick rows based on conditions about their values

3.  `summarize`: Compute summary measures known as "summary statistics" of variables

4.  `group_by`: Group rows of observations together

5.  `mutate`: Create a new variable in the data frame by mutating existing ones

6.  `join`: Join/merge two data frames by matching along a "key" variable. There are many different `join`s available. Here, we will focus on the `inner_join` function.

All of the verbs are used similarly where you: take a data frame, pipe it using the `%>%` syntax into one of the verbs above followed by other arguments specifying which criteria you would like the verb to work with in parentheses.

## Select and rename columns

```{r selectfig, echo=FALSE, fig.cap="Select diagram from Data Wrangling with dplyr and tidyr cheatsheet.", purl=FALSE}
knitr::include_graphics("select.png")
```

We've seen that the `flights` data frame in the `nycflights13` package contains many different variables. The `names` function gives a listing of all the columns in a data frame; in our case you would run `names(flights)`. However, say you only want to consider two of these variables, `carrier` and `flight`. You can `select` these:

```{r, eval=TRUE}
flights %>% 
  select(carrier, flight)
```

The `select` function allows a subset of columns to be extracted, making navigation data sets with a very large number of variables easier. Reversely, one can exclude specific columns via negative selection (using -). For instance, in the `flights` data set, the `year` variable isn't really a variable here in that it doesn't vary (the `flights` data set actually comes from a larger data set that covers many years). Thus, we may want to remove the `year` variable from our data set since it won't be helpful for analysis in this case. We can deselect `year` by using the `-` sign:

```{r, eval=TRUE}
flights_no_year <- flights  %>% select(-year)
```

The `select` function can also be used to reorder columns in combination with the `everything` helper function. Let's suppose we would like the `hour`, `minute`, and `time_hour` variables, which appear at the end of the `flights` data set, to actually appear immediately after the `day` variable:

```{r, eval=TRUE}
flights_reorder <- flights %>%
  select(month:day, hour:time_hour, everything())
names(flights_reorder)
```

in this case `everything()` picks up all remaining variables. Lastly, the helper functions `starts_with`, `ends_with`, and `contains` can be used to choose **variables / column names** that match those conditions:

```{r, eval=TRUE}
flights_begin_a <- flights %>% select(starts_with("a"))
flights_begin_a
```

```{r, eval=TRUE, echo = c(1)}
flights_delays <- flights %>%    select(ends_with("delay")) 
flights_delays
```

```{r, eval=TRUE, echo = c(1)}
flights_time <- flights %>%   select(contains("time")) 
flights_time
```

::: {.callout-warning icon="false"}
## Task

How could one use `starts_with`, `ends_with`, and `contains` to select columns from the `flights` data frame? Provide three different examples in total: one for `starts_with`, one for `ends_with`, and one for `contains`.

```{r}
#| webex.hide: "Click here to see the solution"
#| code-fold: show
# Select arrival time and arrival delay columns
flights %>%
  select(starts_with("arr"))
# Select departure and arrival delay columns
flights %>%
  select(ends_with("delay"))
# Select departure times, schedule  departure and departure delay columns
flights %>%
  select(contains("dep"))

```
:::

Finally, if we want to rename a column while preserving the other columns we can use the `rename` function. Suppose we wanted `dep_time` and `arr_time` to be `departure_time` and `arrival_time` instead in the `flights_time` data frame:

```{r, eval=TRUE, echo = c(1)}
flights_time <- flights %>% 
  select(contains("time")) %>%
  rename(departure_time = dep_time, arrival_time = arr_time) 
names(flights_time)
```

Note that in this case we used a single `=` sign with `rename`. e.g,. `departure_time = dep_time`. This is because we are not testing for equality like we would using `==`, but instead we want to assign a new variable `departure_time` to have the same values as `dep_time` and then delete the variable `dep_time`.

## Filter observations using filter {#filter}

```{r filter, echo=FALSE, purl=FALSE}
knitr::include_graphics("filter.png")
```

The `filter` function allows you to specify criteria about values of a variable in your data set and then chooses only those rows that match that criteria.

::: callout-important
Recall that the base R has already a filter function defined. So make sure to avoid any conflicts either by calling `dplyr::filter()` every time you use the function (specially if you have loaded the `conflicts` library) or alternatively run the`conflict_prefer()` function to let R know that it should use `dplyr`'s `filter` function as default.

```{r}
#| message: false
#| echo: false
library(conflicted)
```

```{r}
#| code-fold: show
conflict_prefer("filter", "dplyr")
```
:::

Let's begin by focusing only at *Alaska Airlines* flights leaving from New York City in 2013. We can combine the data wrangling output with ggplot plotting techniques. Run the following code and look at the resulting scatterplot.

```{r}
flights %>% 
  filter(carrier ==  "AS") %>%
  ggplot(aes(x = dep_delay, y = arr_delay)) + 
  geom_point()+
   labs(x = "Departure delay (minutes)", y = "Arrival delay (minutes)",
       title = "Alaska Airlines flights leaving NYC in 2013") 
```

Note the following:

-   The ordering of the commands:
    -   Take the data frame `flights` **then**
    -   `filter` the data frame so that only those where the `carrier` equals `AS` are included. (The double equals sign `==` tests equality, and not a single equals sign `=`).
    -   pass the filtered data to the ggplot function and add a point layer and then modify axis labels.

You can combine multiple criteria together using operators that make comparisons:

-   `|` corresponds to **or**
-   `&` corresponds to **and**

We can often skip the use of `&` and just separate our conditions with a comma. You'll see this in the example below.

In addition, you can use other mathematical checks (similar to `==`):

-   `>` corresponds to **greater than**
-   `<` corresponds to **less than**
-   `>=` corresponds to **greater than or equal to**
-   `<=` corresponds to **less than or equal to**
-   `!=` corresponds to **not equal to**

To see many of these in action, let's select all flights that left JFK airport heading to Burlington, Vermont (`BTV`) or Seattle, Washington (`SEA`) in the months of October, November, or December. Run the following:

```{r filterex2, exercise=TRUE, echo=TRUE}
btv_sea_flights_fall <- flights %>% 
  filter(origin == "JFK", (dest == "BTV" | dest == "SEA"), month >= 10) %>%
  relocate(dest,.before = dep_time )

btv_sea_flights_fall %>%
  slice(1:3)
```

::: callout-note
Even though colloquially speaking one might say "all flights leaving Burlington, Vermont *and* Seattle, Washington," in terms of computer logical operations, we really mean "all flights leaving Burlington, Vermont *or* Seattle, Washington." For a given row in the data, `dest` can be `BTV`, `SEA`, or something else, but not `BTV` **and** `SEA` at the same time. Also note that we have used the `relocate` function to change the `dest` column position to just before the `dep_time`. See `?relocate` for further details.
:::

Another example uses `!` to pick rows that *do not* match a condition. The `!` can be read as **not**. Here, we are selecting rows corresponding to flights that **did not** go to Burlington, VT or Seattle, WA.

```{r filterex3, exercise=TRUE, echo=TRUE}
not_BTV_SEA <- flights %>% 
  filter(!(dest == "BTV" | dest == "SEA")) %>%
  relocate(dest,.before = dep_time )
not_BTV_SEA %>%
  slice(1:3)
```

As a final note we point out that `filter` should often be the first verb you'll apply to your data. This narrows down the data to just the observations your are interested in.

::: {.callout-warning icon="false"}
## Task

What is another way of using the **not** operator `!` to filter only the rows that are not going to Burlington, VT nor Seattle, WA in the `flights` data frame?

`r hide("Take a hint")`

Try using the `%in%` operator

`r unhide()`

```{r}
#| webex.hide: "Click here to see the solution"
#| code-fold: show
flights %>%
  filter( !dest %in% c("BTV","SEA")) %>%
  head()
```
:::

## Create new variables/change old variables using mutate {#mutate}

```{r select, echo=FALSE,  purl=FALSE}
knitr::include_graphics("mutate.png")
```

When looking at the `flights` data set, there are some clear additional variables that could be calculated based on the values of variables already in the data set. Passengers are often frustrated when their flights depart late, but change their mood a bit if pilots can make up some time during the flight to get them to their destination close to when they expected to land. This is commonly referred to as "gain" and we will create this variable using the `mutate` function. Note that we will be overwriting the `flights` data frame with one including the additional variable `gain` here, or put differently, the `mutate` command outputs a new data frame which then gets saved over the original `flights` data frame.

```{r}
flights <- flights %>% 
  mutate(gain = dep_delay - arr_delay)
```

Let's take a look at `dep_delay`, `arr_delay`, and the resulting `gain` variables in our new `flights` data frame:

```{r, echo=FALSE}
flights %>%
  select(dep_delay, arr_delay, gain) %>% kable()
```

The flight in the first row departed 2 minutes late but arrived 11 minutes late, so its "gained time in the air" is actually a loss of 9 minutes, hence its `gain` is `-9`. Contrast this to the flight in the fourth row which departed a minute early (`dep_delay` of `-1`) but arrived 18 minutes early (`arr_delay` of `-18`), so its "gained time in the air" is 17 minutes, hence its `gain` is `+17`.

Why did we overwrite `flights` instead of assigning the resulting data frame to a new object, like `flights_with_gain`? As a rough rule of thumb, as long as you are not losing information that you might need later, it's acceptable practice to overwrite data frames. However, if you overwrite existing variables and/or change the observational units, recovering the original information might prove difficult. In this case, it might make sense to create a new data object.

Let's look at summary measures of this `gain` variable and plot it in the form of a histogram:

```{r, eval=FALSE, echo = c(1,2,3,4,5,6,7,8,9,10)}
gain_summary <- flights  %>% 
  summarize(
    min = min(gain, na.rm = TRUE),
    q1 = quantile(gain, 0.25, na.rm = TRUE),
    median = quantile(gain, 0.5, na.rm = TRUE),
    q3 = quantile(gain, 0.75, na.rm = TRUE),
    max = max(gain, na.rm = TRUE),
    mean = mean(gain, na.rm = TRUE),
    sd = sd(gain, na.rm = TRUE),
    missing = sum(is.na(gain))
  )


gain_summary
```

```{r,echo=FALSE}
gain_summary <- flights %>% 
  summarize(
    min = min(gain, na.rm = TRUE),
    q1 = quantile(gain, 0.25, na.rm = TRUE),
    median = quantile(gain, 0.5, na.rm = TRUE),
    q3 = quantile(gain, 0.75, na.rm = TRUE),
    max = max(gain, na.rm = TRUE),
    mean = mean(gain, na.rm = TRUE),
    sd = sd(gain, na.rm = TRUE),
    missing = sum(is.na(gain))
  )
kable(gain_summary, booktabs = TRUE)  %>%
    kable_styling(font_size = 9, latex_options = "hold_position")
```

We have recreated the `summary` function using the `summarize` function in `dplyr`. Lets make a histogram for the new created `gain` variable by adding a `geom_histogram` layer.

```{r message=FALSE, fig.cap="Histogram of gain variable.", fig.align = "center", warning=FALSE}
ggplot(data = flights, mapping = aes(x = gain)) +
  geom_histogram(color = "white", bins = 20)
```

We can also create multiple columns at once and even refer to columns that were just created in a new column.

```{r}
flights <- flights %>%
  mutate(
    gain = dep_delay - arr_delay,
    hours = air_time / 60,
    gain_per_hour = gain / hours
  )
```

::: {.callout-tip icon="false"}
## {{< bi question-octagon color=#6dc83c >}} Question

What do positive values of the `gain` variable in `flights` correspond to?

`r mcq(c(answer ="Departure delays are greater than arrivals delays", "Departure delays are lower than arrivals delays", "Departures and arrivals delays are the same"))`

What about negative values?

`r mcq(c(  "Departure delays are greater than arrivals delays", answer =  "Departure delays are lower than arrivals delays", "Departures and arrivals delays are the same"))`

And what about a zero value?

`r mcq(c("Departure delays are greater than arrivals delays", "Departuredelays are lower than arrivals delays", answer =  "Departures and arrivals delays are the same"))`
:::

::: {.callout-tip icon="false"}
## {{< bi question-octagon color=#6dc83c >}} Question

Could we create the `dep_delay` and `arr_delay` columns by simply subtracting `dep_time` from `sched_dep_time` and similarly for arrivals? Try the code out and explain any differences between the result and what actually appears in `flights`.

```{r}
#| eval: false
#| code-fold: show

flights %>%
  mutate(dep_delay  = sched_dep_time - dep_time ,
         arr_delay  = sched_arr_time  - arr_time)  
```

`r hide("Take a hint")`

See the description of the variables `arr_time`, `dep_time`, `sched_dep_time` and `sched_arr_time` in the flights data set `?flights`

`r unhide()`

`r hide("Answer")`

The differences are due to departure and arrival times have a HHMM or HMM format. E.g., if we compute the difference between a flight scheduled to arrive by 923 and its actual arrival time at 850, the result would be a difference of 73, while in reality there was only a 33 min difference if we consider the correct time format! We will see more detials on how to work with time-date variables later on in this session.

`r unhide()`
:::


# Summarise variables using summarize {#summarize}

The next common task is to be able to summarise data: take a large number of values and summarise them with a single value. While this may seem like a very abstract idea, something as simple as the sum, the smallest value, and the largest values are all summaries of a large number of values.

```{r sum2, echo=FALSE,  purl=FALSE}
knitr::include_graphics("summary.png")
```

We can calculate the standard deviation and mean of the temperature variable `temp` in the `weather` data frame of `nycflights13` in one step using the `summarize` (or equivalently using the UK spelling `summarise`) function in `dplyr`. Before compute the mean it is important to notice that there are some **missing values** in the data. Thus, by default any time you try to summarise a number of values (using `mean()` and `sd()` for example) that has one or more missing values, an `NA` will be returned.

You can summarise all non-missing values by setting the `na.rm` argument to TRUE (`rm` is short for remove). This will remove any `NA` missing values and only return the summary value for all non-missing values. So the code below computes the mean and standard deviation of all non-missing values. Notice how the `na.rm=TRUE` are set as arguments to the `mean` and `sd` functions, and not to the `summarize` function.

```{r, eval=FALSE, echo=c(1,2)}
summary_temp <- weather %>% 
  summarize(mean = mean(temp, na.rm = TRUE), std_dev = sd(temp, na.rm = TRUE))
summary_temp
```

Another very useful function that allows you to summarise multiple columns is the `summarise_at()` function. Here, we can supply a vector of variables we want to summarise and a list of functions that we want to apply to each of the variables. See the following example where we compute the minimum and maximum values of temperature and relative humidity (notice that we also specified the argument `na.rm=T` to remove missing values - this arguments gets passed on to all the functions in the list):

```{r}
weather %>%
  summarise_at(.vars = c("temp","humid"),
               .funs = list(min = min, max = max),
               na.rm = TRUE)
```

::: callout-important
It is **not** good practice to include `na.rm = TRUE` in your summary commands by default; you should attempt to run code first without this argument as this will alert you to the presence of missing data. Only after you have identified where missing values occur and have thought about the potential issues of these should you consider using `na.rm = TRUE`. In the upcoming Tasks we will consider the possible ramifications of blindly sweeping rows with missing values under the rug.
:::

What other summary functions can we use inside the `summarize` verb? Any function in R that takes a vector of values and returns just one. Here are just a few:

-   `mean`: the mean (or average)
-   `sd`: the standard deviation, which is a measure of spread
-   `min` and `max`: the minimum and maximum values, respectively
-   `IQR`: the interquartile range
-   `sum`: the sum
-   `n`: a count of the number of rows/observations in each group. This particular summary function will make more sense when **grouping** is used in the next section.

::: {.callout-tip icon="false"}
## {{< bi question-octagon color=#6dc83c >}} Question

Say a doctor is studying the effect of smoking on lung cancer for a large number of patients who have records measured at five year intervals. She notices that a large number of patients have missing data points because the patient has died, so she chooses to ignore these patients in her analysis. What is wrong with this doctor's approach?

```{r}
#| echo: false
opts_p <- c(
  answer = "Introduces a selection bias since patient who died due to lung cancer are excluded from the analysis, leading to an underestimation of the true impact of smoking on lung cancer risk",
    "There is no problem, smaller datasets with fewer missing values may require less computational resources, leading to faster processing times.",
  answer = "Removing patients with missing data reduces the sample size. Hence, conclusions may not be as easily generalizable to the broader population, as the excluded patients may represent a different subset with unique characteristics.", "Removing missing values can result in a dataset with fewer errors and inconsistencies, which can lead to more accurate analyses."
)
```

`r longmcq(opts_p)`
:::

::: {.callout-tip icon="false"}
## {{< bi question-octagon color=#6dc83c >}} Question

Modify `summary_temp` from above to also use the `n` summary function: `summarize(count = n())`. What does the returned value correspond to?

`r mcq(c("Number of weather stations", "Number of columns in the data", answer = "Sample size"))`
:::

::: {.callout-tip icon="false"}
## {{< bi question-octagon color=#6dc83c >}} Question

Why does the code below not work?

```{r}
#| error: true
#| code-fold: show

summary_temp <- weather %>%   
  summarize(mean = mean(temp, na.rm = TRUE)) %>% 
  summarize(std_dev = sd(temp, na.rm = TRUE))

```

`r hide("Take hint")`

Run the code line by line instead of all at once, and then look at the data. In other words, run `summary_temp <- weather %>% summarize(mean = mean(temp, na.rm = TRUE))` first.

`r unhide()`

`r hide("Answer")`

The first line of code computes the temperature mean for the weather data set. Then, the output gets passed on to `weather %>% summarize(std_dev= sd(temp, na.rm = TRUE))`. However, the temperature value is no longer present in the first result, hence the error: `! object 'temp' not found`

`r unhide()`
:::

## Using grouping structures {#groupby}

```{r groupsummarize, echo=FALSE, purl=FALSE}
knitr::include_graphics("group_summary.png")
```

It is often more useful to summarise a variable based on the groupings of another variable. Let's say we are interested in the mean and standard deviation of temperatures but *grouped by month*. Run the following code:

```{r, eval=FALSE, echo = c(1,2,3,4)}
summary_monthly_temp <- weather %>% 
  summarize(mean = mean(temp, na.rm = TRUE), 
            std_dev = sd(temp, na.rm = TRUE),
            .by = month)
```

This code is identical to the previous code that created `summary_temp`, with an extra `.by = month` added. This kind per-operation grouping allow us to do the grouping within the operation where the summarisation takes place without changing the structure of the data .

<!-- ::: {#note_grouping .callout-note} -->

<!-- Previous versions of `dplyr` relied on the specification of a `group_by` function within the pipeline to do the grouping. For example, in the next line of code the `weather` data set is initially grouped by `month`and then passed as a new grouped data frame into `summarize`. Yielding to the same data frame that shows the mean and standard deviation of temperature for each month in New York City: -->

<!-- ```{r} -->

<!-- #| eval: false -->

<!-- #| code-fold: true -->

<!-- summary_monthly_temp <- weather |>  -->

<!--   group_by(month) |>  -->

<!--   summarize(mean = mean(temp, na.rm = TRUE),  -->

<!--             std_dev = sd(temp, na.rm = TRUE)) -->

<!-- ``` -->

<!-- While`group_by` doesn't change the data frame, it sets *meta-data* (data about the data), specifically the group structure of the data. If we wanted to remove this group structure meta-data, we could add the `.groups = "drop"` option. -->

<!-- ```{r} -->

<!-- #| eval: false -->

<!-- #| code-fold: show -->

<!-- summary_monthly_temp <- weather |>  -->

<!--   group_by(month) |>  -->

<!--   summarize(mean = mean(temp, na.rm = TRUE),  -->

<!--             std_dev = sd(temp, na.rm = TRUE), -->

<!--             .groups = "drop") -->

<!-- ``` -->

<!-- The advantage of using the `.by` argument is that the grouping occurs within the `summarise` function and thus the resulting data frame is no longer grouped. -->

<!-- ::: -->

::: {.callout-tip icon="false"}
## {{< bi question-octagon color=#6dc83c >}} Question

The `drop_na()` function can be used in the pipeline to remove missing observations from a data set. Try running the following code to compute the mean and standard deviation of the temperature in the `weather` data set and comment on the output. Why is this different from one one we had before?

```{r}
#| eval: false
summary_monthly_temp <- weather %>% 
  drop_na() %>%
  summarize(mean = mean(temp), 
            std_dev = sd(temp),
            .by = month)
```

`r hide("Answer")`

The `drop_na()` function remove all missing observation from the data set while specifying `na.rm =T` in each summarizing function only removes the missing values for the specific variable to which the function is applied.

`r unhide()`
:::

We now revisit the `n` counting summary function we introduced in the previous section. For example, suppose we would like to get a sense for how many flights departed from each of the three airports in New York City:

```{r, eval=FALSE, echo = c(1,2,3)}
by_origin <- flights %>%
  summarize(count = n(),
            .by =origin)
by_origin
```

```{r, echo=FALSE}
by_origin <- flights %>% 
  group_by(origin) %>% 
  summarize(count = n())
kable(by_origin, booktabs = TRUE)  %>%
    kable_styling(font_size = 9, latex_options = "hold_position")
```

We see that Newark (`EWR`) had the most flights departing in 2013 followed by `JFK` and lastly by LaGuardia (`LGA`). Note, there is a subtle but important difference between `sum` and `n`. While `sum` simply adds up a large set of numbers, the latter counts the number of times each of many different values occur.

::: {.callout-warning icon="false"}
## Task

With the `wether` data set, write code to produce the mean and standard deviation temperature for each day in 2013 for NYC.

`r hide("Take a hint")`

See the documentation for `summarize()` (`?summarize`)

`r unhide()`

```{r}
#| code-fold: show
#| webex.hide: "Click here to see the solution"

 weather %>% 
  summarize(mean = mean(temp, na.rm = TRUE), 
            std_dev = sd(temp, na.rm = TRUE),
            .by = day)

```
:::

### Grouping by more than one variable

You are not limited to grouping by one variable. Say you wanted to know the number of flights leaving each of the three New York City airports *for each month*, we can also group by a second variable `month`:

```{r}
#| code-fold: show
by_origin_monthly <- flights %>%  
  summarize(count = n(), 
            .by = c(origin, month))
by_origin_monthly
```

We see there are 36 rows for `by_origin_monthly` because there are 12 months times 3 airports (`EWR`, `JFK`, and `LGA`). How can we visualize this information? Lets look now into different techniques for manipulation and visualizing categorical data.


# Working with categorical data


Recall that barplots, or barcharts, are used to visualise the distributions of categorical variables. This essentially provides us with the frequencies of categories within a categorical variable. You can use either the raw data (e.g. the original `flights` data set) or the summarised data set (e.g. the `by_origin_monthly` data set we just created) to create barplots in `ggplot.`

::: panel-tabset
# Raw data and geom_bar()

Here we can use a data set with variable(s) representing the categories. We can add a `geom_bar()` layer to create a barplot layer by counting the number of cases for each level of a categorical variable and use the `fill=origin` option to assign a different color to the counts based on the origin. Including the `fill` argument lets `ggplot` plot know that we want to split the barplot according to an additional categorical variable, which is `origin` in this case, Adding proper labels to the barplot produces:

```{r}
#| fig-width: 5
#| fig-align: center
#| fig-height:5

flights %>%
  ggplot(aes(x=factor(month),fill=origin))+
  geom_bar()+ 
  scale_x_discrete(labels = month.abb) +
  labs(x= "Months",y="Number of flights")
```

Note that the month variable in our data set is an integer. Thus, we convert this into a factor using the `factor()` function directly in the aesthetic mapping. Then we provide appropriate labels for each month (`labels = month.abb)` by adding one more `scale_x_discrete`layer.

# Summarized data set and geom_col()

Here we can use a data set with variables representing the categories and the counts of each category (e.g. the `by_origin_monthly` data set we just created). Here, to produce the bar plot we add a `geom_col()` layer which expects a data set that already contains the counts for each group. We use the `fill=origin` option to assign a different color to the counts based on the origin. Including the `fill` argument lets `ggplot` plot know that we want to split the barplot according to an additional categorical variable, which is `origin` in this case. Adding proper labels to the barplot produces:

```{r}
#| fig-width: 5
#| fig-align: center
#| fig-height:5

by_origin_monthly %>%
ggplot(aes(x = factor(month), y = count, fill= origin )) +  
  geom_col() +
  scale_x_discrete(labels = month.abb)+
    labs(x= "Months",y="Number of flights")
```

Note that the month variable in our data set is an integer. Thus, we convert this into a factor using the `factor()` function directly in the aesthetic mapping. Then we provide appropriate labels for each month (`labels = month.abb)` by adding one more `scale_x_discrete`layer.
:::

This is what is referred to as a *Stacked barplot* since the bars for each `origin` are simply stacked on top of one another for each of the carriers. This provides us with a visually nice barplot to present the monthly number of flights by airport of origin. However, there are also alternative barplots to the stacked barplot. One alternative to a stacked barplot is the **side-by-side** (or **dodged**) **barplot**, which, as suggested by its name, places the bars next to each other instead of on top of one another. This can be produced by including `position = 'dodge'` within the `geom_col` or `geom_bar` layer. A second alternative is to use a **faceted barplot**. This can be produced by adding a `facet_wrap()` layer to ggplot. E.g. try adding `facet_wrap(~ origin, ncol = 1)` to any of the previous barplots you have produced. The `facet_wrap` function tells ggplot that we want to separate out barplots by `origin`, and hence we use `~ origin`. We also choose to have them plotted in one column via `ncol = 1`. This makes it easier to compare their distributions now that they are not stacked on top or beside one another. Lets put this into practice:

::: {.callout-warning icon="false"}
## Task

Boxplots are useful visualisations when comparing the distribution of a numerical variable split across groups (or a categorical variable). Taking the `weather` data set, use ggplot to create a boxplot showing how the hourly temperature changes by month for each of the three different Weather stations (`origin` variable). Use a different color for each station.

`r hide("Take a hint")`

To create boxplots using ggplot you can use the `geom_boxplot` function. If we want to look at boxplots of a variable separately for a categorical variable then you need to declare that variable as a factor using the `factor` function.

`r unhide()`

```{r}
#| code-fold: show
#| webex.hide: "Click here to see the solution"
#| warning: false

ggplot(data = weather, mapping = aes(x = factor(month), y = temp, fill = origin)) +
  geom_boxplot() +
  facet_wrap(~origin)+
  labs(x = "Month", y = "Temperature (Hourly)",
        title = "Hourly temperatures from NYC in 2013 by month")  +
   scale_x_discrete(labels = month.abb)

```
:::

::: {.callout-warning icon="false"}
## Task

By using the `summarise()` function, how could we identify how many flights left each of the three airports for each `carrier`? Can you create a barplot showing these results?

`r hide("Take a hint")`

You can count how many flights left each of the three airports by summarising the data using the `n()` function while grouping by the origin and carrier. Then, you can pass the resulting data frame to `ggplot` using the pipeline command `%>%` and use a `geom_col` layer as in the previous example.

`r unhide()`

```{r}
#| code-fold: show
#| webex.hide: "Click here to see the solution"

flights %>% 
  summarise(count = n(), 
            .by = c(origin,carrier)) %>%
  ggplot(aes(x = carrier, y = count, fill = origin)) + geom_col()

```
:::


## Vectorised if-else thru `case_when`

`case_when` serves as a method to streamline multiple if-else statements by vectorizing them. It allows us to assess a condition expression and make decisions accordingly. In this session, we will consider a scenario where we need to categorize weather conditions according to the meteorological data contained in the `weather` data set.

Let suppose that we want to categorize the temperature variable into three categories:

-   **low** for temperatures $<39.9$

-   **medium** for temperature values $\geq 39.9$ and $\leq 70$

-   **high** for temperature values $> 70$

We can achieve this with the following code:

```{r}
#| code-fold: show
weather |>
  mutate(
    temp_cat = case_when(
      is.na(temp) ~ NA,
      temp < 39.9 ~ "low",
      between(temp,39.9 ,70)~ "medium",
      .default = "large"
    )
  ) |>
  relocate(temp,temp_cat)

```

Here we use the `mutate` command to create new variable named `temp_cat`. The `case_when` will then set to `NA` those values in the original `temp` variable that are missing. Then if the values of `temp` are $< 30.9$ it will assign them the label of `low`. If they lie between $39.9$ and $70$ it will assign them the label of `medium` and finally set to `large` any of the values that do not meet any of the aforementioned conditions. We can also use the function `relocate` to change the columns position so that the `temp` and `temp_cat` appears first on the data frame.

::: {.callout-warning icon="false"}
## Task

Create a new variable called `extreme_weather` that takes the value of `extreme` if the wind speed exceeds 64 mph and the temperature is less than 40 Â°F and `not extreme` otherwise. Then, relocate this new variable along with the variables used to create it at the first columns of the data frame, and sort them out based on `wind_speed`.

`r hide("Take a hint")`

Use the conditional operators `|` and `&` to add multiple conditions.

`r unhide()`

```{r}
#| code-fold: show
#| webex.hide: "Click here to see the solution"
weather |>
  mutate(
    extreme_weather  = case_when(
      is.na(temp)|is.na(wind_speed) ~ NA,
      temp < 40 & wind_speed  > 64~ "extreme",
      .default = "not extreme"
    )
  ) |>
  relocate(extreme_weather,temp,wind_speed) |>
  arrange(desc(wind_speed))
```
:::


# Working with dates and times

Working with date-time data in R can be challenging due to the unintuitive and inconsistent commands across different date-time objects. Additionally, managing things like time zones, leap days, and daylight saving time can be tricky since R doesn't always handle these well. The `lubridate` and `hms` packages (loaded as part of `tideverse`) simplify date-time operations in R, making it easier to perform common tasks and enabling functionalities that R's base capabilities do not support. Unfortunately, we don't have enough time to cover all the details in this session. Instead, we will only give short introduction on how to work and manipulate date and time variables in R using the `lubridate` and `hms` packages. But if you want to learn more please have a look at the [R for Data Science](https://r4ds.hadley.nz/datetimes) ebook.

First, what do we mean by Date/Time data? well, when we speak of Date/Time data we are mainly referring to three data types:

1.  **Date**  - a variable containing only the date when an observation was made (e.g. 2024-07-12). More formally, it is a day stored as the number of days since 1970-01-01

2.  **Time** - a variable containing only the time when an observation was made (e.g. 18:15:00). Formally , the number of seconds since 00:00:00

3.  **Date & Time** - combination of both the date and time (e.g. 2024-07-12 18:15:00). Formally, is a point on the timeline, stored as the number of seconds since 1970-01-01 00:00:00 UTC

There are several ways in which Date-time variables can be created. Here are some examples:

```{r}
# Example 1: string input with date Y/M/D format
ymd("2017-01-31")
# Example 2: string input with date M/D/Y format
mdy("January 31st, 2017")
# Example 3: string input with date D/M/Y format
dmy("31-Jan-2017")
# Example 4: numeric input with date M/D/Y format
mdy(07082016)
# Examples 5: string input with time H:M formate
hm("20:11")
# Example 6:  string input with date-time D/M/Y H:M:S
ymd_hms("2017-01-31 20:11:59")

```

In this session, instead of creating data-time variables by ourselves, we will focus on already existing Date/Time Data. Let's look at some of the date-time variables in the `flights` data set, namely the scheduled departure dates and times:

::: panel-tabset
# Output

```{r}
#| echo: false

flights_dep <- flights %>% 
  select(year, month, day, hour, minute)
flights_dep

```

# R-Code

```{r}
#| eval: false
#| code-fold: false
flights_dep <- flights %>% 
  select(year, month, day, hour, minute)
flights_dep
```
:::

Instead of having separate date-time variables spread across different columns, we can use the `make_date()` or `make_datetime()`functions to create new date and date-time variables respectively:

```{r}
flights_dep <- flights_dep %>%
    mutate(departure_time = make_datetime(year, month, day, hour, minute),
           departure_date = make_date(year, month, day))
flights_dep
```

We then can visualize the distribution of the scheduled departure times across the year with ggplot by adding a `geom_freqpoly()` layer (which is similar to an histogram where the counts are displayed with lines instead of bars). Note that when you use date-times in a numeric context (like in a histogram), a binwidth of 1 is equivalent to 1 second, so a binwidth of 86400 is equivalent to one day.

```{r}
flights_dep %>%  
  ggplot(aes(x = departure_time)) + 
  geom_freqpoly(binwidth = 86400) # 86400 seconds = 1 day
```

Likewise, if were interested in the distribution of the scheduled departures for a given day:

```{r}
flights_dep %>% 
  filter(departure_date == ymd(20130102)) %>% 
  ggplot(aes(x = departure_time)) + 
  geom_freqpoly(binwidth = 600) # 600 s = 10 minutes
```

In here, `binwidth = 600` means we are clumping all flights within each 10 minutes (600 s) together into one single data point in our frequency polygon.

Now, notice that in the original `flights` data set, the hour and minute of the actual departure (`dep_time`) and arrival times (`arr_time`)  are encoded together into a single integer. Let make a function that sets the actual times in a sensible format:



```{r}
make_datetime_flights <- function(year, month, day, time) {
  hour <- case_when(
      nchar(time)== 1 ~ time  %/%1,
      nchar(time)== 2 ~ time  %/%10,
      .default =  time  %/%100
    )
  
  min <- case_when(
      nchar(time)== 1 ~ time  %%1,
      nchar(time)== 2 ~ time  %%10,
      .default =  time  %%100
    )
  make_datetime(year, month, day, hour, min)
}

```


The new `make_datetime_flights()` function we just created separates the hour and minute of a given HM input and pass it on to `make_datetime` function. This is achieved by using a vectorized `case_when` argument based on the number of characters in the integer that uses the `%/%` or`%%`  operator to find (or discards accordingly) the remainder of an integer division to obtain the hour and minute components (e.g.  `951 %/% 100`  and `951 %% 100` splits the entry `951` into 9  and 51  (9:15 am once converted to time-date data) while  `15 %/% 10` and `15 %% 10` and  gives 1  and 5  (equivalent to 1:05 am in date-time format) .

```{r}
flights_dt <- flights %>% 
  filter(!is.na(dep_time), !is.na(arr_time)) %>% 
  mutate(
    dep_time = make_datetime_flights(year, month, day, dep_time),
    arr_time = make_datetime_flights(year, month, day, arr_time),
    sched_dep_time = make_datetime_flights(year, month, day, sched_dep_time),
    sched_arr_time = make_datetime_flights(year, month, day, sched_arr_time)
  ) 

flights_dt %>% 
  select(dep_time,arr_time,sched_dep_time,sched_arr_time) %>%
  slice(1:3)
```



## Extracting individual date-time components 

The `lubridate` package also provide us with different tools for extracting specific components from date-time objects (e.g. year, month, hours, minutes, etc). Suppose we are interested in finding out which day of the week each flight took place. The `wday()` functions allow us to extract the numeric entry of the day of the week, by including the argument `label =TRUE`, we can also print the name of the weekday as the output

```{r}
flights_dt %>%
   select(dep_time,arr_time,sched_dep_time,sched_arr_time) %>%
  mutate(weekday = wday(dep_time,label=TRUE)) %>%
  slice_sample(n=5)
```

Here are some more few examples of helper functions that allow you to extract individual date-time components:

```{r}
datetime <- ymd_hms("2026-07-08 12:34:56")

year(datetime)
month(datetime,label = TRUE)
day(datetime)
hour(datetime)
minute(datetime)
```


::: {.callout-warning icon="false"}
## Task

Can you make a plot showing how does the distribution of flight times within a day change over the course of the year? i.e., how many flights have taken off by each hour. Comment on the patterns


`r hide("Take a hint")`

Within a day, we want to observe how the flight times differ. This means we should look at how flight times differ by the hour (i.e how many flights are taking off at every hour of the day). You can use the `hour()` function to extract the hours for every departure time and then count (using `summarize()`) how many flights have taken off by each hour. You can visualize the trend using `geom_line` in ggplot.

`r unhide()`

```{r}
#| code-fold: show
#| webex.hide: "Click here to see the solution"

flights_dt %>% 
  mutate(hour = hour(dep_time)) %>%
  summarize(numflights_per_hour = n(),.by= hour)%>%
  ggplot(aes(x = hour, y = numflights_per_hour)) +
    geom_line() +
  labs(y="number of flights per hour",x = "hour") 

```

We can see there is a peak of flights around 8am, a dip in flights from 10am-12pm, and then a drop off in number of flights past 7pm.
:::




::: {.callout-tip icon="false"}
## {{< bi question-octagon color=#6dc83c >}} Question

Modify `summary_temp` from above to also use the `n` summary function: `summarize(count = n())`. What does the returned value correspond to?

`r mcq(c("Number of weather stations", "Number of columns in the data", answer = "Sample size"))`
:::

::: {.callout-tip icon="false"}
## {{< bi question-octagon color=#6dc83c >}} Question

If we want to measure how the average delay time change over the course of a day, should we use the actual (`dep_time`) or the scheduled (`sched_dep_time`) departure times? Why?


`r hide("Take hint")`
See `?flights` for details on how `dep_delay` is calculated

`r unhide()`

`r hide("Answer")`

The delay is calculated as the difference between the actual departure time (`dep_time`) and the scheduled departure time (`sched_dep_time`). Using `sched_dep_time` ensures that you are comparing the delay against a consistent reference point, which is the planned schedule. Moreover, if you use `dep_time`, it reflects when the flight actually departed, which already includes the delay. Using `sched_dep_time` allows you to analyze delays in the context of the planned schedule, helping to identify patterns or trends that occur at specific scheduled times of the day (e.g., early morning flights versus evening flights).

`r unhide()`
:::



::: {.callout-warning icon="false"}
## Task

Find out on what day of the week should you leave if you want to minimise the chance of a delay?

`r hide("Take a hint")`

To find the days of the week that have the lowest average delay, first you need  to assign a day to each observation using `wday()`. You can then use `summarize()` and group by the the day of the week to find the average delay time for each day of the week.

`r unhide()`

```{r}
#| code-fold: show
#| webex.hide: "Click here to see the solution"

flights_dt %>% 
  mutate(wday = wday(sched_dep_time, label = TRUE)) %>% 
  group_by(wday) %>%
  summarize ( avg_dep_delay_week = mean(dep_delay, na.rm = TRUE),
              avg_arr_delay_week = mean(arr_delay, na.rm = TRUE)) %>%
  slice_min(avg_dep_delay_week,n=1)

# Saturday has the lowest average delay at 7.61, and on average the flights even arrive earlier than expected!
```

We can see there is a peak of flights around 8am, a dip in flights from 10am-12pm, and then a drop off in number of flights past 7pm.
:::



## Time intervals, durations and periods


Now that we have seen a few examples of R's date-time data structures, lets look into some of the time span classes.

- Duration: exact number of seconds.
- Periods: human units like weeks and months.
- Intervals: a time span defined by a start and end point.

Duration is simply defined by the exact amount of time between two time events. It does not consider what these two events are in terms of,e.g. calendar years or time zone (so things like leap years would be ignored), and the output is shown in seconds.
For example, say we want to manually compute the departure delays in the `flights` data set (we will use the `flights_dt` data frame we created previously which has the `dep_time` and `sched_dep_times` in the correct date-time format).

```{r}

 flights_dt %>%
  mutate(
    dep_delay_manual =  dep_time - sched_dep_time) %>% 
    select(dep_time,sched_dep_time,dep_delay_manual,dep_delay)  %>%
  slice(1:5)

```

At first glance, we can see that the manually computed departure delays `dep_delay_manual` and the original delays `dep_delay` are not on the same format. By default, when you  subtract two dates (e.g. `dep_time - sched_dep_time`), you get a `difftime` object which records a time span of seconds, minutes, hours, days, or weeks. This variability can make `difftime` objects difficult to work with. To address this, we can use convert a `difftime` object to a `duration` class using the `as.duration()` function. Additionally, the original delays `dep_delay`, which are measured in minutes but have no default date-time format, can also be transformed into a duration class using the `duration(units ="")` function.


```{r}
 flights_dt %>%
  mutate(
      dep_delay = duration(minute  = dep_delay),
      dep_delay_manual =  as.duration(dep_time - sched_dep_time))%>%
  select(dep_time,sched_dep_time,dep_delay_manual,dep_delay)  %>%
  slice(1:5)
```

`Durations` always record the time span in seconds. Instead, `periods` represent time spans without a fixed length in seconds; they work with "human" times, such as days and months. This allows them to operate in a more intuitive manner.  periods can be created with different functions, here are some examples:

```{r}
hours(c(12, 24))
days(7) 
months(1:3)
```


Lets see how the output changes when we use `periods` instead of `durations`:


```{r}
flights_dt %>%
  mutate(
      dep_delay = period(minute  = dep_delay),
      dep_delay_manual =  as.period(dep_time - sched_dep_time))%>%
  select(dep_time,sched_dep_time,dep_delay_manual,dep_delay)  %>%
  slice(1:5)

```


The last type of time-span defined in `lubridate` are *intervals*. As with *durations*,  intervals are expressed in physical time spans defined by a start and end points that are real date-times, i.e. intervals are *durations* defined by a calendar time. Lets suppose we are only given the scheduled departure times and the departure delay. We can create an interval time-span to compute the actual departure time as follows:

```{r}

flights_dt %>% 
  select(sched_dep_time,dep_delay) %>%
  mutate(
      dep_delay_duration = duration(minute  = dep_delay),
      dep_delay_interval= as.interval(x = dep_delay_duration, start= sched_dep_time))%>%
  slice(1:5)
```


# Joining data frames {#joins}

Another common task is joining (merging) two different data sets. For example, in the `flights` data, the variable `carrier` lists the carrier code for the different flights. While `UA` and `AA` might be somewhat easy to guess for some (United and American Airlines), what are VX, HA, and B6? This information is provided in a separate data frame `airlines`.

```{r eval=TRUE}
airlines
```

We see that in `airports`, `carrier` is the carrier code while `name` is the full name of the airline. Using this table, we can see that VX, HA, and B6 correspond to Virgin America, Hawaiian Airlines Inc., and JetBlue Airways, respectively. However, will we have to continually look up the carrier's name for each flight in the `airlines` data set? No! Instead of having to do this manually, we can have R automatically do the "looking up" for us.

Note that the values in the variable `carrier` in `flights` match the values in the variable `carrier` in `airlines`. In this case, we can use the variable `carrier` as a *key variable* to join/merge/match the two data frames by. Key variables are almost always identification variables that uniquely identify the observational units as we saw back in the **Identification vs Measurement Variable** section. This ensures that rows in both data frames are appropriately matched during the join. This diagram helps us understand how the different data sets are linked by various key variables:

```{r reldiagram, echo=FALSE, fig.cap="Data relationships in nycflights13 from R for Data Science, Hadley and Garrett (2016).", purl=FALSE}
knitr::include_graphics("relational-nycflights.png")
```

## Joining by "key" variables

In both `flights` and `airlines`, the key variable we want to join/merge/match the two data frames with has the same name in both data sets: `carriers`. We make use of the `inner_join` function to join by the variable `carrier`.

```{r eval=TRUE}
flights_joined <- flights |> 
  inner_join(airlines, 
             by = join_by(carrier))
             
flights
flights_joined
```

We observe that the `flights` and `flights_joined` are identical except that `flights_joined` has an additional variable `name` whose values were drawn from `airlines`.

A visual representation of the `inner_join` is given below:

```{r ijdiagram, echo=FALSE, fig.cap="Diagram of inner join from R for Data Science.", purl=FALSE}
knitr::include_graphics("join-inner.png")
```

There are more complex joins available, but the `inner_join` will solve nearly all of the problems you will face here.

## Joining by "key" variables with different names

Say instead, you are interested in all the destinations of flights from NYC in 2013 and ask yourself:

-   "What cities are these airports in?"
-   "Is `ORD` Orlando?"
-   "Where is `FLL`?"

The `airports` data frame contains airport codes:

```{r eval=TRUE}
airports
```

However, looking at both the `airports` and `flights` and the visual representation of the relations between the data frames in the figure above, we see that in:

-   `airports` the airport code is in the variable `faa`
-   `flights` the airport code is in the variable `origin`

So to join these two data sets, our `inner_join` operation involves a logical operator `==` argument that accounts for the different names.

```{r, eval=FALSE}
flights |> 
  inner_join(airports,
             by = join_by(dest == faa))
```

We can read the code out loud as:

```{=tex}
\begin{center}
"*Take the flights data frame and inner join it to the airports data frame by the entries where the variable* `dest` *is equal to* `faa`"
\end{center}
```
Let's construct the sequence of commands that computes the number of flights from NYC to each destination, but also includes information about each destination airport:

```{r, echo = c(1)}
named_dests <- flights|>
  summarize(num_flights = n(),
            .by = dest) |>
  arrange(desc(num_flights)) |>
  inner_join(airports, by = join_by(dest == faa)) %>%
  rename(airport_name = name)
named_dests
```

In case you didn't know, `ORD` is the airport code of Chicago O'Hare airport and `FLL` is the main airport in Fort Lauderdale, Florida, which we can now see in our `named_dests` data frame.

## Joining by multiple "key" variables

Say instead we are in a situation where we need to join by multiple variables. For example, in the first figure in this section we see that in order to join the `flights` and `weather` data frames, we need more than one key variable: `year`, `month`, `day`, `hour`, and `origin`. This is because the combination of these 5 variables act to uniquely identify each observational unit in the `weather` data frame: hourly weather recordings at each of the 3 NYC airports.

We achieve this by specifying a vector of key variables to join by.

```{r, echo = c(1)}
flights_weather_joined <- flights |>
  inner_join(weather, 
             by = join_by(year,month,day,hour,origin))
             
flights_weather_joined
```

::: {.callout-tip icon="false"}
## {{< bi question-octagon color=#6dc83c >}} Question

Looking at the first figure in this section, when joining `flights` and `weather` (or, in other words, matching the hourly weather values with each flight), why do we need to join by all of `year`, `month`, `day`, `hour`, and `origin`, and not just `hour`?

`r hide("Answer")`

`year`,`month`,`day`,`hour`,`origin` are the key variables that allow us to uniquely identify the observational units.

`r unhide()`
:::

::: {.callout-warning icon="false"}
## Task

Create a new data frame that shows the top 5 airports with the largest average arrival delays from NYC in 2013.

`r hide("Take a hint")`

Compute the mean arrival delay from each destination. You can then join the resulting data set with the `airports` data which contains the airports names and search for the top 5 entries.

`r unhide()`

```{r}
#| webex.hide: "Click here to see the solution"
#| eval: false
#| code-fold: show


  flights|>
  summarize(mean_arr_delay = mean(arr_delay,na.rm=T),
            .by = dest) |>
  inner_join(airports, by = join_by(dest == faa)) |>
  rename(airport_name = name) |>
    slice_max(mean_arr_delay,n=5)

```
:::


